var relearn_search_index = [
  {
    "breadcrumb": "",
    "content": " Kursbeschreibung Der Compiler ist das wichtigste Werkzeug in der Informatik. In der Königsdisziplin der Informatik schließt sich der Kreis, hier kommen die unterschiedlichen Algorithmen und Datenstrukturen und Programmiersprachenkonzepte zur Anwendung.\nIn diesem Modul geht es um ein fortgeschrittenes Verständnis für interessante Konzepte im Compilerbau. Wir schauen uns dazu relevante aktuelle Tools und Frameworks an und setzen diese bei der Erstellung eines Byte-Code-Compilers samt Virtual Machine für Java ein.\nÜberblick Modulinhalte Lexikalische Analyse: Scanner/Lexer\nReguläre Sprachen Klassisches Vorgehen: RegExp nach NFA (Thompson's Construction), NFA nach DFA (Subset Construction), DFA nach Minimal DFA (Hopcroft's Algorithm) Manuelle Implementierung, Generierung mit ANTLR und Flex Syntaxanalyse: Parser\nKontextfreie Grammatiken (CFG), Chomsky LL-Parser (Top-Down-Parser) FIRST, FOLLOW Tabellenbasierte Verfahren, rekursiver Abstieg LL(1), LL(k), LL(*) Umgang mit Vorrang-Regeln, Assoziativität und linksrekursiven Grammatiken Backtracking, Memoizing, Predicated Parsers; ANTLR4: ALL(*) LR-Parser (Bottom-Up-Parser) Shift-Reduce LR(0), SLR(1), LR(1), LALR(1) Generierung mit ANTLR und Bison Symboltabellen\nBerücksichtigung unterschiedlicher Sprachparadigmen Namen und Scopes Typen, Klassen, Polymorphie Semantische Analyse und Optimierungen\nAttributierte Grammatiken: L-attributed vs. R-attributed grammars Typen, Typ-Inferenz, Type Checking Datenfluss-Analyse Optimierungen: Peephole u.a. Zwischencode: Intermediate Representation (IR), LLVM\nInterpreter\nAST-Traversierung Read-Eval-Schleife Resolver: Beschleunigung der Interpretation Code-Generierung, Byte-Code\nSpeicherlayout Erzeugen von Byte-Code Ausführen in einer Virtuellen Maschine Team BC George Carsten Gips Kursformat ​ Compilerbau Concepts of Programming Languages Vorlesung (2 SWS) Praktikum (2 SWS) Di, 13:30 - 15:00 Uhr Di, 15:15 - 16:45 Uhr online/C1 online/C1 Vorlesung (2 SWS) Praktikum (3 SWS) Di, 13:30 - 15:00 Uhr Di, 15:15 - 17:30 Uhr online/C1 online/C1 Durchführung als Flipped Classroom (Carsten) bzw. Vorlesung (BC): alle Sitzungen online/per Zoom (Zugangsdaten siehe ILIAS)\nPrüfungsform, Note und Credits ​ Compilerbau Concepts of Programming Languages Mündliche Prüfung plus Testat, 5 ECTS (PO18)\nTestat: Vergabe der Credit-Points Für die Vergabe der Credit-Points ist die regelmäßige und erfolgreiche Teilnahme am Praktikum erforderlich, welche am Ende des Semesters durch ein Testat bescheinigt wird.\nKriterien:\nAktive Mitarbeit im Praktikum und an den Meilensteinen Aktive Teilnahme an den drei gemeinsamen Terminen mit der University of Alberta (Edmonton) Vortrag I mit Edmonton zu Spezialisierung/Baustein in Projekt, ca. 10 Minuten pro Person (60 bis 90 Minuten gesamt) Vortrag II: Vorstellung der Projektergebnisse, ca. 10 Minuten pro Person (60 bis 90 Minuten gesamt) Gesamtnote: Mündliche Prüfung am Ende des Semesters, angeboten in beiden Prüfungszeiträumen.\nParcoursprüfung, 10 ECTS (PO23)\nunbenotet: Aktive Mitarbeit im Praktikum und an den Meilensteinen unbenotet: Aktive Teilnahme an den drei gemeinsamen Terminen mit der University of Alberta (Edmonton) unbenotet: Vortrag I mit Edmonton zu Spezialisierung/Baustein in Projekt, ca. 10 Minuten pro Person (60 bis 90 Minuten gesamt) unbenotet: Vortrag II: Vorstellung der Projektergebnisse, ca. 10 Minuten pro Person (60 bis 90 Minuten gesamt) benotet (30%): Vortrag III: Vortrag zu Paper-Analyse (extra-Leistung neue PO) plus Diskussion (Vorbereitung/Leitung), 30 Minuten Dauer; Zusammenfassung als Blog-Eintrag benotet (70%): Mündliche Prüfung am Ende des Semesters, angeboten in beiden Prüfungszeiträumen Gesamtnote: 30% Vortrag III plus 70% mdl. Prüfung\nMaterialien Literatur \"Compiler: Prinzipien, Techniken und Werkzeuge\". Aho, A. V. und Lam, M. S. und Sethi, R. und Ullman, J. D., Pearson Studium, 2008. ISBN 978-3-8273-7097-6.\n\"Crafting Interpreters\". Nystrom, R., Genever Benning, 2021. ISBN 978-0-9905829-3-9.\n\"Modern Compiler Design\". Grune, D. und van, Reeuwijk, K. und Bal, H. E. und Jacobs, C. J. H. und Langendoen, K., Springer, 2012. ISBN 978-1-4614-4698-9.\n\"Engineering a Compiler\". Torczon, L. und Cooper, K., Elsevier MK, 2012. ISBN 978-0-1208-8478-0.\n\"The Definitive ANTLR 4 Reference\". Parr, T., Pragmatic Bookshelf, 2014. ISBN 978-1-9343-5699-9.\nTools github.com/antlr Fahrplan Hier finden Sie einen abonierbaren Google Kalender mit allen Terminen der Veranstaltung zum Einbinden in Ihre Kalender-App.\nVorlesung Woche Datum Themen Lead Bemerkung 41 Di, 10.10. Orga (Zoom) || Überblick | Sprachen | Anwendungen BC, Carsten 42 Di, 17.10. Reguläre Sprachen BC 42 Di, 17.10. (Praktikum) Tabellenbasierte Lexer | Handcodierter Lexer | Lexer mit ANTLR Carsten 43 Di, 24.10. CFG BC 43 Di, 24.10. (Praktikum) LL-Parser (Theorie) BC 44 Mo, 30.10., 17:00-18:00 Uhr Edmonton I: ANTLR + Live-Coding (CA) 44 Di, 31.10. LL-Parser (Praxis) | LL: Fortgeschrittene Techniken | Parser mit ANTLR | Grenze Lexer und Parser Carsten 45 Di, 07.11. LR-Parser (Teil 1) BC Meilenstein 1 46 Di, 14.11. LR-Parser (Teil 2) BC Exposé Vortrag III 47 Di, 21.11. Überblick Symboltabellen | Symboltabellen: Scopes | Symboltabellen: Funktionen | Symboltabellen: Klassen Carsten 48 Di, 28.11. Attributierte Grammatiken BC 48 Di, 28.11., 18:00-19:00 Uhr Edmonton II: Vortrag Mindener Projekte (DE) (Vortrag I) Meilenstein 2 (Kein Praktikum - Termin mit Edmonton!) 49 Di, 05.12. Optimierung und Datenflussanalyse BC 49 Mi, 06.12., 18:00-19:00 Uhr Edmonton III: Vortrag Edmontoner Projekte (CA) 50 Di, 12.12. Syntaxgesteuerte Interpreter | AST-basierte Interpreter 1 | AST-basierte Interpreter 2 | Garbage Collection | Maschinencode Carsten 51 Di, 19.12. Freies Arbeiten Carsten Meilenstein 3 02 Di, 09.01. Freies Arbeiten Carsten 03 Di, 16.01. Vortrag II: Termine siehe Etherpad BC, Carsten Vortrag II: Termine siehe Etherpad 04 Di, 23.01. Vortrag III: Termine siehe Etherpad BC, Carsten Vortrag III: Termine siehe Etherpad Praktikum Sie bearbeiten im Praktikum eine gemeinsame Aufgabe in der Semestergruppe.\nWoche Rücksprache Vorstellung Praktikum Inhalt 45 Meilenstein 1 Di, 07.11. Konzepte Java-Bytecode, Arbeitsaufteilung, Arbeitsplanung 46 Exposé Di, 14.11. Diskussion Exposé Vortrag III 48 Meilenstein 2 Di, 28.11., 18:00-19:00 Uhr (Edmonton II) Projektstatus (Vortrag I) 51 Meilenstein 3 Di, 19.12. Projektstatus Förderungen und Kooperationen Kooperation mit University of Alberta, Edmonton (Kanada) Über das Projekt \"We CAN virtuOWL\" der Fachhochschule Bielefeld ist im Frühjahr 2021 eine Kooperation mit der University of Alberta (Edmonton/Alberta, Kanada) im Modul \"Compilerbau\" gestartet.\nWir freuen uns, auch in diesem Semester wieder drei gemeinsame Sitzungen für beide Hochschulen anbieten zu können. (Diese Termine werden in englischer Sprache durchgeführt.)\n",
    "description": "",
    "tags": null,
    "title": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24)",
    "uri": "/index.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24) \u003e Praktikum",
    "content": "Aufgabe Konzipieren und implementieren Sie im Laufe des Semesters gemeinsam in der gesamten Gruppe einen Compiler cmjavac sowie eine Virtual Machine cmjava für die Programmiersprache Java.\nIhr Java-Compiler cmjavac soll entsprechenden Java-Bytecode in .class-Dateien ausgeben, Ihre VM cmjava soll .class-Dateien einlesen und abarbeiten können. Im Rahmen der umgesetzten Features soll eine Interoperabilität mit den \"echten\" Java-Tools möglich sein: Eine passende Java-Quelldatei soll sich sowohl von Ihrem Compiler als auch einem javac in einem üblichen JDK übersetzen lassen. Die .class-Dateien sollen sich dann sowohl mit Ihrer VM als auch der aus einem üblichen JDK einlesen und verarbeiten lassen und zum selben Ergebnis kommen.\nSie können sich relativ frei aussuchen, welche Java-Konzepte Sie umsetzen wollen. Folgende Konzepte stellen einen minimalen Umfang dar:\nKontrollfluss (if, for, ...) Erzeugung von primitiven Datentypen und Objekten Einfache Ein- und Ausgabe von/auf der Konsole Klassen, Vererbung, Polymorphie Exceptions Semantische Analyse und Optimierungen Garbage Collection (in der VM) Da dies eine Implementierung für Lernzwecke ist, müssen Sie folgende Java-Konzepte nicht umsetzen:\nGenerics Threads Reflection Annotations I/O (Umgang mit Dateien und Ordnern) Laden von Klassen aus einem .jar Strukturell werden Sie vermutlich ein Frontend mit Lexer und Parser benötigen. Hier empfehlen wir Ihnen die Umsetzung mit ANTLR. Sie werden ebenfalls Symboltabellen für die semantische Analyse benötigen und für verschiedene Optimierungen. Ihr Compiler cmjavac soll Java Bytecode als \"Zwischencode\" ausgeben (.class-Dateien). Ihre Virtual Machine cmjava liest die .class-Dateien wieder ein und verarbeitet diese. Arbeiten Sie mit einer Stack-basierten VM und implementieren Sie diese in einer Sprache, die keine Garbage Collection eingebaut hat.\nOrganisieren Sie selbstständig in Ihrer Semestergruppe die Arbeitsaufteilung und -planung. Da der Java Bytecode eine zentrale Rolle spielt, sollten Sie sich dieses Thema gemeinsam in einem ersten Schritt anschauen.\nStimmen Sie alle Schritte und Ergebnisse mit Ihren Dozent:innen ab und holen Sie sich aktiv Feedback.\nHinweis: Wir werden in der Vorlesung nicht alle benötigten Techniken besprechen können (und auch möglicherweise nicht rechtzeitig). Es besteht die Erwartung, dass Sie sich selbstständig und rechtzeitig mit den jeweiligen Themen auseinander setzen.\nMeilensteine Es gibt drei vordefinierte Meilensteine:\nMeilenstein 1: Vorstellung der Recherche-Ergebnisse und Konzepte zum Thema Java Bytecode sowie zur Arbeitsaufteilung und -planung (im Praktikum) Meilenstein 2: Vorstellung der Arbeit im Rahmen des Edmonton-Meetings (Prüfungsleistung \"Vortrag I\", Termin \"Edmonton II\") Meilenstein 3: Vorstellung des Projektstatus (in Vorlesung und/oder Praktikum) Im Rahmen des Vortrags II sollen am Ende des Semesters die Projektergebnisse gemeinsam vorgestellt werden.\nHinweis zu Meilenstein 2: Stellen Sie gemeinsam als Gruppe das Projekt den Studierenden der University of Alberta vor: Was ist die Aufgabe, welche Konzepte und Strukturen haben Sie erarbeitet, wie sehen erste Arbeitsergebnisse aus? Jede(r) sollte ca. 10 Minuten vortragen. Dieser Vortrag findet in englischer Sprache statt und ist Teil der Prüfungsleistung.\n",
    "description": "",
    "tags": null,
    "title": "Aufgabe: Java-Compiler und -VM",
    "uri": "/homework/project.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24) \u003e Praktikum",
    "content": "Der Vortrag III ist Teil der Prüfungsleistung für Studierende, die nach der neuen Prüfungsordnung (PO23) am Modul \"Concepts of Programming Languages\" teilnehmen. Die Vorträge sollen 30 Minuten dauern und werden in der letzten Vorlesungswoche stattfinden. Planen Sie eine Diskussion von ca. 10 Minuten ein. Zusätzlich ist eine kurze Zusammenfassung des Vortrags als Blog im Discussions-Thread des jeweiligen Exposés zu erstellen.\nWir bieten Ihnen hier verschiedene Themen zur Auswahl an, Sie können aber auch gern eigene Vorschläge erarbeiten. Erstellen Sie in beiden Fällen ein kurzes Exposé (Thema, Kernthesen, Paper) als neuen Beitrag in den GitHub-Discussions und stimmen Sie dieses bis zum Meilenstein I mit Ihren Dozent:innen ab. Ein Thema (bezogen auf die genutzten Paper/Quellen) kann nur einmal vergeben werden - hier gilt das first-come-first-serve-Prinzip in den Discussions.\nLR-Parsergeneratoren im Vergleich: Flex und Bison vs. Tree-Sitter Advanced Parsing: Pratt-Parser PEG-Parser Parser-Kombinatoren flap: A Deterministic Parser with Fused Lexing Interval Parsing Grammars for File Format Parsing Resolvable Ambiguity: Principled Resolution of Syntactically Ambiguous Programs VM und Bytecode: AST vs. Bytecode: Interpreters in the Age of Meta-Compilation An Introduction to Interpreters and JIT Compilation Optimizing the Order of Bytecode Handlers in Interpreters using a Genetic Algorithm Garbage Collection: Unified Theory of Garbage Collection Fast Conservative Garbage Collection Ownership guided C to Rust translation Precise Garbage Collection for C Optimierung: Alias-Based Optimization Applying Optimizations for Dynamically-typed Languages to Java Provably Correct Peephole Optimizations with Alive Profiling: Efficient Path Profiling Efficiently counting program events with support for on-line queries Whole program paths Don’t Trust Your Profiler: An Empirical Study on the Precision and Accuracy of Java Profilers LL(*) und Adaptive LL(*) in ANTLR v4 T. Parr: \"LL(*): The Foundation of the ANTLR Parser Generator\" T. Parr: \"Adaptive LL(*) Parsing: The Power of Dynamic Analysis\" T. Parr: LL(*) grammar analysis Testing: Finding and Understanding Bugs in C Compilers Validating JIT Compilers via Compilation Space Exploration A Survey of Compiler Testing An empirical comparison of compiler testing techniques Compiler Testing: A Systematic Literature Analysis Snapshot Testing for Compilers Tiny Unified Runner N' Tester (Turnt) Testing Language Implementations Typen und Typinferenzsysteme: Hindley-Milner Typinferenzsystem On Understanding Types, Data Abstraction, and Polymorphism Propositions as Types DSL: A Modern Compiler for the French Tax Code Compiling ML models to C for fun Programming Language Concepts Erforschen Sie das actor model mit Elixir, einer neuen funktionalen Programmiersprache für das Web basierend auf der Erlang Virtual Machine. Erkunden Sie borrowing and lifetimes anhand von Rust, einer Systemsprache ohne Garbage Collector. Erforschen Sie dependent type systems mit Idris, einer neuen, von Haskell inspirierten Sprache mit beispielloser Unterstützung für typgesteuerte Entwicklung. Erforschen Sie Algebraische Typen mit Pattern Matching, deren praktischen Einsatz (etwa in Haskell, Scala und Java) sowie die interne Umsetzung im Compiler und der Laufzeitumgebung. Erkunden Sie Konzepte zur Behandlung von optionalen und/oder null-/nil-Werten sowie deren Umsetzung im Compiler und der Laufzeitumgebung. ",
    "description": "",
    "tags": null,
    "title": "Concepts of Programming Languages (PO23): Vortrag III",
    "uri": "/homework/talk.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24)",
    "content": "Was ist ein Compiler? Welche Bausteine lassen sich identifizieren, welche Aufgaben haben diese?\nStruktur eines Compilers Bandbreite der Programmiersprachen Anwendungen ",
    "description": "",
    "tags": null,
    "title": "Überblick",
    "uri": "/intro.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24) \u003e Überblick",
    "content": "Sprachen verstehen, Texte transformieren The cat runs quickly.\n=\u003e Struktur? Bedeutung?\nWir können hier (mit steigender Abstraktionsstufe) unterscheiden:\nSequenz von Zeichen\nWörter: Zeichenketten mit bestimmten Buchstaben, getrennt durch bestimmte andere Zeichen; Wörter könnten im Wörterbuch nachgeschlagen werden\nSätze: Anordnung von Wörtern nach einer bestimmten Grammatik, Grenze: Satzzeichen\nHier (vereinfacht): Ein Satz besteht aus Subjekt und Prädikat. Das Subjekt besteht aus einem oder keinen Artikel und einem Substantiv. Das Prädikat besteht aus einem Verb und einem oder keinem Adverb.\nSprache: Die Menge der in einer Grammatik erlaubten Sätze\nCompiler: Big Picture Quelle: A Map of the Territory (mountain.png) by Bob Nystrom on Github.com (MIT)\nBegriffe und Phasen\nDie obige Bergsteige-Metapher kann man in ein nüchternes Ablaufdiagramm mit verschiedenen Stufen und den zwischen den Stufen ausgetauschten Artefakten übersetzen:\nFrontend, Analyse Die ersten Stufen eines Compilers, die mit der Analyse des Inputs beschäftigt sind. Dies sind in der Regel der Scanner, der Parser und die semantische Analyse.\nScanner, Lexer, Tokenizer, Lexikalische Analyse\nZerteilt den Zeichenstrom in eine Folge von Wörtern. Mit regulären Ausdrücken kann definiert werden, was Klassen gültiger Wörter (\"Token\") sind. Ein Token hat i.d.R. einen Namen und einen Wert.\nParser, Syntaxanalyse\nDer Parser erhält als Eingabe die Folge der Token und versucht mit Hilfe einer Grammatik zu bestimmen, ob es sich bei der Tokensequenz um gültige Sätze im Sinne der Grammatik handelt. Hier gibt es viele Algorithmen, die im Wesentlichen in die Klassen \"top-down\" und \"bottom-up\" fallen.\nSemantische Analyse, Kontexthandling\nIn den vorigen Stufen wurde eher lokal gearbeitet. Hier wird über den gesamten Baum und die Symboltabelle hinweg geprüft, ob beispielsweise Typen korrekt verwendet wurden, in welchen Scope ein Name gehört etc. Mit diesen Informationen wird der AST angereichert.\nSymboltabellen\nDatenstrukturen, um Namen, Werte, Scopes und weitere Informationen zu speichern. Die Symboltabellen werden vor allem beim Parsen befüllt und bei der semantischen Analyse gelesen, aber auch der Lexer benötigt u.U. diese Informationen.\nBackend, Synthese Die hinteren Stufen eines Compilers, die mit der Synthese der Ausgabe beschäftigt sind. Dies sind in der Regel verschiedene Optimierungen und letztlich die Code-Generierung\nCodegenerierung\nErzeugung des Zielprogramms aus der (optimierten) Zwischendarstellung. Dies ist oft Maschinencode, kann aber auch C-Code oder eine andere Ziel-Sprache sein.\nOptimierung\nDiverse Maßnahmen, um den resultierenden Code kleiner und/oder schneller zu gestalten.\nSymboltabellen\nDatenstrukturen, um Namen, Werte, Scopes und weitere Informationen zu speichern. Die Symboltabellen werden vor allem beim Parsen befüllt und bei der semantischen Analyse gelesen, aber auch der Lexer benötigt u.U. diese Informationen.\nWeitere Begriffe Parse Tree, Concrete Syntax Tree\nRepräsentiert die Struktur eines Satzes, wobei jeder Knoten dem Namen einer Regel der Grammatik entspricht. Die Blätter bestehen aus den Token samt ihren Werten.\nAST, (Abstract) Syntax Tree\nVereinfachte Form des Parse Tree, wobei der Bezug auf die Element der Grammatik (mehr oder weniger) weggelassen wird.\nAnnotierter AST\nAnmerkungen am AST, die für spätere Verarbeitungsstufen interessant sein könnten: Typ-Informationen, Optimierungsinformationen, ...\nZwischen-Code, IC\nZwischensprache, die abstrakter ist als die dem AST zugrunde liegenden Konstrukte der Ausgangssprache. Beispielsweise könnten while-Schleifen durch entsprechende Label und Sprünge ersetzt werden. Wie genau dieser Zwischen-Code aussieht, muss der Compilerdesigner entscheiden. Oft findet man den Assembler-ähnlichen \"3-Adressen-Code\".\nSprache\nEine Sprache ist eine Menge gültiger Sätze. Die Sätze werden aus Wörtern gebildet, diese wiederum aus Zeichenfolgen.\nGrammatik\nEine Grammatik beschreibt formal die Syntaxregeln für eine Sprache. Jede Regel in der Grammatik beschreibt dabei die Struktur eines Satzes oder einer Phrase.\nLexikalische Analyse: Wörter (\"Token\") erkennen Die lexikalische Analyse (auch Scanner oder Lexer oder Tokenizer genannt) zerteilt den Zeichenstrom in eine Folge von Wörtern (\"Token\"). Die geschieht i.d.R. mit Hilfe von regulären Ausdrücken.\nDabei müssen unsinnige/nicht erlaubte Wörter erkannt werden.\nÜberflüssige Zeichen (etwa Leerzeichen) werden i.d.R. entfernt.\nsp = 100; \u003cID, sp\u003e, \u003cOP, =\u003e, \u003cINT, 100\u003e, \u003cSEM\u003e Anmerkung: In der obigen Darstellung werden die Werte der Token (\"Lexeme\") zusammen mit den Token \"gespeichert\". Alternativ können die Werte der Token auch direkt in der Symboltabelle gespeichert werden und in den Token nur der Verweis auf den jeweiligen Eintrag in der Tabelle.\nSyntaxanalyse: Sätze erkennen In der Syntaxanalyse (auch Parser genannt) wird die Tokensequenz in gültige Sätze unterteilt. Dazu werden in der Regel kontextfreie Grammatiken und unterschiedliche Parsing-Methoden (top-down, bottom-up) genutzt.\nDabei müssen nicht erlaubte Sätze erkannt werden.\n\u003cID, sp\u003e, \u003cOP, =\u003e, \u003cINT, 100\u003e, \u003cSEM\u003e statement : assign SEM ; assign : ID OP INT ; statement = / \\ / \\ assign SEM sp 100 / | \\ | ID OP INT ; | | | sp = 100 Mit Hilfe der Produktionsregeln der Grammatik wird versucht, die Tokensequenz zu erzeugen. Wenn dies gelingt, ist der Satz (also die Tokensequenz) ein gültiger Satz im Sinne der Grammatik. Dabei sind die Token aus der lexikalischen Analyse die hier betrachteten Wörter!\nDabei entsteht ein sogenannter Parse-Tree (oder auch \"Syntax Tree\"; in der obigen Darstellung der linke Baum). In diesen Bäumen spiegeln sich die Regeln der Grammatik wider, d.h. zu einem Satz kann es durchaus verschiedene Parse-Trees geben.\nBeim AST (\"Abstract Syntax Tree\") werden die Knoten um alle später nicht mehr benötigten Informationen bereinigt (in der obigen Darstellung der rechte Baum).\nAnmerkung: Die Begriffe werden oft nicht eindeutig verwendet. Je nach Anwendung ist das Ergebnis des Parsers ein AST oder ein Parse-Tree.\nAnmerkung: Man könnte statt OP auch etwa ein ASSIGN nutzen und müsste dann das \"=\" nicht extra als Inhalt speichern, d.h. man würde die Information im Token-Typ kodieren.\nVorschau: Parser implementieren stat : assign | ifstat | ... ; assign : ID '=' expr ';' ;void stat() { switch (\u003c\u003ccurrent token\u003e\u003e) { case ID : assign(); break; case IF : ifstat(); break; ... default : \u003c\u003craise exception\u003e\u003e } } void assign() { match(ID); match('='); expr(); match(';'); }Der gezeigte Parser ist ein sogenannter \"LL(1)\"-Parser und geht von oben nach unten vor, d.h. ist ein Top-Down-Parser.\nNach dem Betrachten des aktuellen Tokens wird entschieden, welche Alternative vorliegt und in die jeweilige Methode gesprungen.\nDie match()-Methode entspricht dabei dem Erzeugen von Blättern, d.h. hier werden letztlich die Token der Grammatik erkannt.\nSemantische Analyse: Bedeutung erkennen In der semantischen Analyse (auch Context Handling genannt) wird der AST zusammen mit der Symboltabelle geprüft. Dabei spielen Probleme wie Scopes, Namen und Typen eine wichtige Rolle.\nDie semantische Analyse ist direkt vom Programmierparadigma der zu übersetzenden Sprache abhängig, d.h. müssen wir beispielsweise das Konzept von Klassen verstehen?\nAls Ergebnis dieser Phase entsteht typischerweise ein annotierter AST.\n{ int x = 42; { int x = 7; x += 3; // ??? } } = {type: real, loc: tmp1} sp = 100; / \\ / \\ sp inttofloat {type: real, | loc: var b} 100 Zwischencode generieren Aus dem annotierten AST wird in der Regel ein Zwischencode (\"Intermediate Code\", auch \"IC\") generiert. oft findet man hier den Assembler-ähnlichen \"3-Adressen-Code\", in manchen Compilern wird als IC aber auch der AST selbst genutzt.\n= {type: real, loc: tmp1} / \\ / \\ sp inttofloat {type: real, | loc: var b} 100 =\u003e t1 = inttofloat(100)\nCode optimieren An dieser Stelle verlassen wir das Compiler-Frontend und begeben uns in das sogenannte Backend. Die Optimierung des Codes kann sehr unterschiedlich ausfallen, beispielsweise kann man den Zwischencode selbst optimieren, dann nach sogenanntem \"Targetcode\" übersetzen und diesen weiter optimieren, bevor das Ergebnis im letzten Schritt in Maschinencode übersetzt wird.\nDie Optimierungsphase ist sehr stark abhängig von der Zielhardware. Hier kommen fortgeschrittene Mengen- und Graphalgorithmen zur Anwendung. Die Optimierung stellt den wichtigsten Teil aktueller Compiler dar.\nAus zeitlichen und didaktischen Gründen werden wir in dieser Veranstaltung den Fokus auf die Frontend-Phasen legen und die Optimierung nur grob streifen.\nt1 = inttofloat(100) =\u003e t1 = 100.0\nx = y*0; =\u003e x = 0;\nCode generieren Maschinencode:\nSTD t1, 100.0 Andere Sprache:\nBytecode C ... Probleme 5*4+3 AST?\nProblem: Vorrang von Operatoren\nVariante 1: +(*(5, 4), 3) Variante 2: *(5, +(4, 3)) stat : expr ';' | ID '(' ')' ';' ; expr : ID '(' ')' | INT ;Unbedingt lesenswert Sie sollten diese beiden Paper unbedingt als Einstieg in das Modul lesen:\nProgramming Language Semantics An Incremental Approach to Compiler Construction Wrap-Up Compiler übersetzen Text in ein anderes Format\nTypische Phasen:\nLexikalische Analyse Syntaxanalyse Semantische Analyse Generierung von Zwischencode Optimierung des (Zwischen-) Codes Codegenerierung ",
    "description": "",
    "tags": null,
    "title": "Struktur eines Compilers",
    "uri": "/intro/overview.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24) \u003e Überblick",
    "content": "99 Bottles of Beer 99 bottles of beer on the wall, 99 bottles of beer. Take one down and pass it around, 98 bottles of beer on the wall.\n98 bottles of beer on the wall, 98 bottles of beer. Take one down and pass it around, 97 bottles of beer on the wall.\n[...]\n2 bottles of beer on the wall, 2 bottles of beer. Take one down and pass it around, 1 bottle of beer on the wall.\n1 bottle of beer on the wall, 1 bottle of beer. Take one down and pass it around, no more bottles of beer on the wall.\nNo more bottles of beer on the wall, no more bottles of beer. Go to the store and buy some more, 99 bottles of beer on the wall.\nQuelle: nach \"Lyrics of the song 99 Bottles of Beer\" on 99-bottles-of-beer.net\nImperativ, Hardwarenah: C #define MAXBEER (99) void chug(int beers); main() { register beers; for(beers = MAXBEER; beers; chug(beers--)) puts(\"\"); puts(\"\\nTime to buy more beer!\\n\"); } void chug(register beers) { char howmany[8], *s; s = beers != 1 ? \"s\" : \"\"; printf(\"%d bottle%s of beer on the wall,\\n\", beers, s); printf(\"%d bottle%s of beeeeer . . . ,\\n\", beers, s); printf(\"Take one down, pass it around,\\n\"); if(--beers) sprintf(howmany, \"%d\", beers); else strcpy(howmany, \"No more\"); s = beers != 1 ? \"s\" : \"\"; printf(\"%s bottle%s of beer on the wall.\\n\", howmany, s); }Quelle: \"Language C\" by Bill Wein on 99-bottles-of-beer.net\nImperativ\nProcedural\nStatisches Typsystem\nResourcenschonend, aber \"unsicher\": Programmierer muss wissen, was er tut\nRelativ hardwarenah\nEinsatz: Betriebssysteme, Systemprogrammierung\nImperativ, Objektorientiert: Java class bottles { public static void main(String args[]) { String s = \"s\"; for (int beers=99; beers\u003e-1;) { System.out.print(beers + \" bottle\" + s + \" of beer on the wall, \"); System.out.println(beers + \" bottle\" + s + \" of beer, \"); if (beers==0) { System.out.print(\"Go to the store, buy some more, \"); System.out.println(\"99 bottles of beer on the wall.\\n\"); System.exit(0); } else System.out.print(\"Take one down, pass it around, \"); s = (--beers == 1)?\"\":\"s\"; System.out.println(beers + \" bottle\" + s + \" of beer on the wall.\\n\"); } } }Quelle: \"Language Java\" by Sean Russell on 99-bottles-of-beer.net\nImperativ\nObjektorientiert\nMulti-Threading\nBasiert auf C/C++\nStatisches Typsystem\nAutomatische Garbage Collection\n\"Sichere\" Architektur: Laufzeitumgebung fängt viele Probleme ab\nArchitekturneutral: Nutzt Bytecode und eine JVM\nEinsatz: High-Level All-Purpose Language\nLogisch: Prolog bottles :- bottles(99). bottles(1) :- write('1 bottle of beer on the wall, 1 bottle of beer,'), nl, write('Take one down, and pass it around,'), nl, write('Now they are all gone.'), nl,!. bottles(X) :- write(X), write(' bottles of beer on the wall,'), nl, write(X), write(' bottles of beer,'), nl, write('Take one down and pass it around,'), nl, NX is X - 1, write(NX), write(' bottles of beer on the wall.'), nl, nl, bottles(NX).Quelle: \"Language Prolog\" by M@ on 99-bottles-of-beer.net\nDeklarativ\nLogisch: Definition von Fakten und Regeln; eingebautes Beweissystem\nEinsatz: Theorem-Beweisen, Natural Language Programming (NLP), Expertensysteme, ...\nFunktional: Haskell bottles 0 = \"no more bottles\" bottles 1 = \"1 bottle\" bottles n = show n ++ \" bottles\" verse 0 = \"No more bottles of beer on the wall, no more bottles of beer.\\n\" ++ \"Go to the store and buy some more, 99 bottles of beer on the wall.\" verse n = bottles n ++ \" of beer on the wall, \" ++ bottles n ++ \" of beer.\\n\" ++ \"Take one down and pass it around, \" ++ bottles (n-1) ++ \" of beer on the wall.\\n\" main = mapM (putStrLn . verse) [99,98..0]Quelle: \"Language Haskell\" by Iavor on 99-bottles-of-beer.net\nDeklarativ\nFunktional\nLazy, pure\nStatisches Typsystem\nTypinferenz\nAlgebraische Datentypen, Patternmatching\nEinsatz: Compiler, DSL, Forschung\nBrainfuck Quelle: Screenshot of \"Language Brainfuck\" by Michal Wojciech Tarnowski on 99-bottles-of-beer.net\nImperativ\nFeldbasiert (analog zum Band der Turingmaschine)\n8 Befehle: Zeiger und Zellen inkrementieren/dekrementieren, Aus- und Eingabe, Sprungbefehle\nProgrammiersprache Lox fun fib(x) { if (x == 0) { return 0; } else { if (x == 1) { return 1; } else { fib(x - 1) + fib(x - 2); } } } var wuppie = fib; wuppie(4); Die Sprache \"Lox\" finden Sie hier: craftinginterpreters.com/the-lox-language.html\nC-ähnliche Syntax\nImperativ, objektorientiert, Funktionen als First Class Citizens, Closures\nDynamisch typisiert\nGarbage Collector\nStatements und Expressions\n(Kleine) Standardbibliothek eingebaut\nDie Sprache ähnelt stark anderen modernen Sprachen und ist gut geeignet, um an ihrem Beispiel Themen wie Scanner/Parser/AST, Interpreter, Object Code und VM zu studieren :)\nWrap-Up Compiler übersetzen formalen Text in ein anderes Format\nBerücksichtigung von unterschiedlichen\nSprachkonzepten (Programmierparadigmen) Typ-Systemen Speicherverwaltungsstrategien Abarbeitungsstrategien ",
    "description": "",
    "tags": null,
    "title": "Bandbreite der Programmiersprachen",
    "uri": "/intro/languages.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24) \u003e Überblick",
    "content": "Anwendung: Compiler Wie oben diskutiert: Der Sourcecode durchläuft alle Phasen des Compilers, am Ende fällt ein ausführbares Programm heraus. Dieses kann man starten und ggf. mit Inputdaten versehen und erhält den entsprechenden Output. Das erzeugte Programm läuft i.d.R. nur auf einer bestimmten Plattform.\nBeispiele: gcc, clang, ...\nAnwendung: Interpreter Beim Interpreter durchläuft der Sourcecode nur das Frontend, also die Analyse. Es wird kein Code erzeugt, stattdessen führt der Interpreter die Anweisungen im AST bzw. IC aus. Dazu muss der Interpreter mit den Eingabedaten beschickt werden. Typischerweise hat man hier eine \"Read-Eval-Print-Loop\" (REPL).\nBeispiele: Python\nAnwendung: Virtuelle Maschinen Hier liegt eine Art Mischform aus Compiler und Interpreter vor: Der Compiler übersetzt den Quellcode in ein maschinenunabhängiges Zwischenformat (\"Byte-Code\"). Dieser wird von der virtuellen Maschine (\"VM\") gelesen und ausgeführt. Die VM kann also als Interpreter für Byte-Code betrachtet werden.\nBeispiel: Java mit seiner JVM\nAnwendung: C-Toolchain Erinnern Sie sich an die LV \"Systemprogrammierung\" im dritten Semester :-)\nAuch wenn es so aussieht, als würde der C-Compiler aus dem Quelltext direkt das ausführbare Programm erzeugen, finden hier dennoch verschiedene Stufen statt. Zuerst läuft ein Präprozessor über den Quelltext und ersetzt alle #include und #define etc., danach arbeitet der C-Compiler, dessen Ausgabe wiederum durch einen Assembler zu ausführbarem Maschinencode transformiert wird.\nBeispiele: gcc, clang, ...\nAnwendung: C++-Compiler C++ hat meist keinen eigenen (vollständigen) Compiler :-)\nIn der Regel werden die C++-Konstrukte durch cfront nach C übersetzt, so dass man anschließend auf die etablierten Tools zurückgreifen kann.\nDieses Vorgehen werden Sie relativ häufig finden. Vielleicht sogar in Ihrem Projekt ...\nBeispiel: g++\nAnwendung: Bugfinder Tools wie FindBugs analysieren den (Java-) Quellcode und suchen nach bekannten Fehlermustern. Dazu benötigen sie nur den Analyse-Teil eines Compilers!\nAuf dem AST kann dann nach vorab definierten Fehlermustern gesucht werden (Stichwort \"Graphmatching\"). Dazu fällt die semantische Analyse entsprechend umfangreicher aus als normal.\nZusätzlich wird noch eine Reporting-Komponente benötigt, da die normalen durch die Analysekette erzeugten Fehlermeldungen nicht helfen (bzw. sofern der Quellcode wohlgeformter Code ist, würden ja keine Fehlermeldungen durch die Analyseeinheit generiert).\nBeispiele: SpotBugs, Checkstyle, ESLint, ...\nAnwendung: Pandoc Pandoc ist ein universeller und modular aufgebauter Textkonverter, der mit Hilfe verschiedener Reader unterschiedliche Textformate einlesen und in ein Zwischenformat (hier JSON) transformieren kann. Über verschiedene Writer können aus dem Zwischenformat dann Dokumente in den gewünschten Zielformaten erzeugt werden.\nDie Reader entsprechen der Analyse-Phase und die Writer der Synthese-Phase eines Compilers. Anstelle eines ausführbaren Programms (Maschinencode) wird ein anderes Textformat erstellt/ausgegeben.\nBeispielsweise wird aus diesem Markdown-Schnipsel ...\nDies ist ein Satz mit * einem Stichpunkt, und * einem zweiten Stichpunkt. ... dieses Zwischenformat erzeugt, ...\n{\"blocks\":[{\"t\":\"Para\",\"c\":[{\"t\":\"Str\",\"c\":\"Dies\"},{\"t\":\"Space\"}, {\"t\":\"Str\",\"c\":\"ist\"},{\"t\":\"Space\"},{\"t\":\"Str\",\"c\":\"ein\"}, {\"t\":\"Space\"},{\"t\":\"Str\",\"c\":\"Satz\"},{\"t\":\"Space\"}, {\"t\":\"Str\",\"c\":\"mit\"}]}, {\"t\":\"BulletList\",\"c\":[[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"einem\"},{\"t\":\"Space\"},{\"t\":\"Str\",\"c\":\"Stichpunkt,\"},{\"t\":\"Space\"},{\"t\":\"Str\",\"c\":\"und\"}]}],[{\"t\":\"Plain\",\"c\":[{\"t\":\"Str\",\"c\":\"einem\"},{\"t\":\"Space\"},{\"t\":\"Str\",\"c\":\"zweiten\"},{\"t\":\"Space\"},{\"t\":\"Str\",\"c\":\"Stichpunkt.\"}]}]]}],\"pandoc-api-version\":[1,17,0,4],\"meta\":{}}... und daraus schließlich dieser TeX-Code.\nDies ist ein Satz mit \\begin{itemize} \\tightlist \\item einem Stichpunkt, und \\item einem zweiten Stichpunkt. \\end{itemize}Im Prinzip ist Pandoc damit ein Beispiel für Compiler, die aus einem formalen Text nicht ein ausführbares Programm erzeugen (Maschinencode), sondern einen anderen formalen Text. Dieser werden häufig auch \"Transpiler\" genannt.\nWeitere Beispiele:\nLexer-/Parser-Generatoren: ANTLR, Flex, Bison, ...: formale Grammatik nach Sourcecode CoffeeScript: CoffeeScript (eine Art \"JavaScript light\") nach JavaScript Emscripten: C/C++ nach LLVM nach WebAssembly (tatsächlich kann LLVM-IR auch direkt als Input verwendet werden) Fitnesse: Word/Wiki nach ausführbare Unit-Tests Was bringt mir das? Beschäftigung mit dem schönsten Thema in der Informatik ;-)\nAuswahl einiger Gründe für den Besuch des Moduls \"Compilerbau\" Erstellung eigener kleiner Interpreter/Compiler Einlesen von komplexen Daten DSL als Brücke zwischen Stakeholdern DSL zum schnelleren Programmieren (denken Sie etwa an CoffeeScript ...) Wie funktionieren FindBugs, Lint und ähnliche Tools? Statische Codeanalyse: Dead code elimination Language-theoretic Security: LangSec Verständnis für bestimmte Sprachkonstrukte und -konzepte (etwa virtual in C++) Vertiefung durch Besuch \"echter\" Compilerbau-Veranstaltungen an Uni möglich :-) Wie funktioniert: ein Python-Interpreter? das Syntaxhighlighting in einem Editor oder in Doxygen? ein Hardwarecompiler (etwa VHDL)? ein Text-Formatierer (TeX, LaTeX, ...)? CoffeeScript oder Emscripten? Wie kann man einen eigenen Compiler/Interpreter basteln, etwa für MiniJava (mit C-Backend) Brainfuck Übersetzung von JSON nach XML Um eine profundes Kenntnis von Programmiersprachen zu erlangen, ist eine Beschäftigung mit ihrer Implementierung unerlässlich. Viele Grundtechniken der Informatik und elementare Datenstrukturen wie Keller, Listen, Abbildungen, Bäume, Graphen, Automaten etc. finden im Compilerbau Anwendung. Dadurch schließt sich in gewisser Weise der Kreis in der Informatikausbildung ... Aufgrund seiner Reife gibt es hervorragende Beispiele von formaler Spezifikation im Compilerbau. Mit dem Gebiet der formalen Sprachen berührt der Compilerbau interessante Aspekte moderner Linguistik. Damit ergibt sich letztlich eine Verbindung zur KI ... Die Unterscheidung von Syntax und Semantik ist eine grundlegende Technik in fast allen formalen Systeme. Parser-Generatoren (Auswahl) Diese Tools könnte man beispielsweise nutzen, um seine eigene Sprache zu basteln.\nANTLR (ANother Tool for Language Recognition) is a powerful parser generator for reading, processing, executing, or translating structured text or binary files: github.com/antlr/antlr4 Grammars written for ANTLR v4; expectation that the grammars are free of actions: github.com/antlr/grammars-v4 An incremental parsing system for programmings tools: github.com/tree-sitter/tree-sitter Flex, the Fast Lexical Analyzer - scanner generator for lexing in C and C++: github.com/westes/flex Bison is a general-purpose parser generator that converts an annotated context-free grammar into a deterministic LR or generalized LR (GLR) parser employing LALR(1) parser tables: gnu.org/software/bison Parser combinators for binary formats, in C: github.com/UpstandingHackers/hammer Eclipse Xtext is a language development framework: github.com/eclipse/xtext Statische Analyse, Type-Checking und Linter Als Startpunkt für eigene Ideen. Oder Verbessern/Erweitern der Projekte ...\nPluggable type-checking for Java: github.com/typetools/checker-framework SpotBugs is FindBugs' successor. A tool for static analysis to look for bugs in Java code: github.com/spotbugs/spotbugs An extensible cross-language static code analyzer: github.com/pmd/pmd Checkstyle is a development tool to help programmers write Java code that adheres to a coding standard: github.com/checkstyle/checkstyle JaCoCo - Java Code Coverage Library: github.com/jacoco/jacoco Sanitizers: memory error detector: github.com/google/sanitizers JSHint is a tool that helps to detect errors and potential problems in your JavaScript code: github.com/jshint/jshint Haskell source code suggestions: github.com/ndmitchell/hlint Syntax checking hacks for vim: github.com/vim-syntastic/syntastic DSL (Domain Specific Language) NVIDIA Material Definition Language SDK: github.com/NVIDIA/MDL-SDK FitNesse -- The Acceptance Test Wiki: github.com/unclebob/fitnesse Hier noch ein Framework, welches auf das Erstellen von DSL spezialisiert ist:\nEclipse Xtext is a language development framework: github.com/eclipse/xtext Konverter von X nach Y Emscripten: An LLVM-to-JavaScript Compiler: github.com/kripken/emscripten \"Unfancy JavaScript\": github.com/jashkenas/coffeescript Universal markup converter: github.com/jgm/pandoc Übersetzung von JSON nach XML Odds and Ends How to write your own compiler: staff.polito.it/silvano.rivoira/HowToWriteYourOwnCompiler.htm Building a modern functional compiler from first principles: github.com/sdiehl/write-you-a-haskell Language-theoretic Security: LangSec Generierung von automatisierten Tests mit Esprima: heise.de/-4129726 Eigener kleiner Compiler/Interpreter, etwa für MiniJava mit C-Backend oder sogar LLVM-Backend Brainfuck Als weitere Anregung: Themen der Mini-Projekte im W17 Java2UMLet JavaDoc-to-Markdown Validierung und Übersetzung von Google Protocol Buffers v3 nach JSON svg2tikz SwaggerLang -- Schreiben wie im Tagebuch Markdown zu LaTeX JavaDocToLaTeX MySQL2REDIS-Parser Wrap-Up Compiler übersetzen formalen Text in ein anderes Format\nNicht alle Stufen kommen immer vor =\u003e unterschiedliche Anwendungen\n\"Echte\" Compiler: Sourcecode nach Maschinencode Interpreter: Interaktive Ausführung Virtuelle Maschinen als Zwischending zwischen Compiler und Interpreter Transpiler: formaler Text nach formalem Text Analysetools: Parsen den Sourcecode, werten die Strukturen aus ",
    "description": "",
    "tags": null,
    "title": "Anwendungen",
    "uri": "/intro/applications.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24) \u003e Frontend",
    "content": "Der Lexer (auch \"Scanner\") soll den Zeichenstrom in eine Folge von Token zerlegen. Zur Spezifikation der Token werden in der Regel reguläre Ausdrücke verwendet.\nReguläre Sprachen, Ausdrucksstärke Lexer: Tabellenbasierte Implementierung Lexer: Handcodierte Implementierung Lexer mit ANTLR generieren ",
    "description": "",
    "tags": null,
    "title": "Lexer",
    "uri": "/frontend/lexing.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24)",
    "content": "Unter dem \"Frontend\" versteht man die ersten Stufen eines Compilers, die mit der Analyse des Inputs beschäftigt sind. Dies sind in der Regel der Scanner, der Parser und die semantische Analyse.\nLexer Parser Semantische Analyse ",
    "description": "",
    "tags": null,
    "title": "Frontend",
    "uri": "/frontend.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24) \u003e Frontend \u003e Lexer",
    "content": "Motivation Was muss ein Compiler wohl als erstes tun? Hier entsteht ein Tafelbild.\nThemen für heute Endliche Automaten Reguläre Ausdrücke Endliche Automaten Alphabete Def.: Ein Alphabet $\\Sigma$ ist eine endliche, nicht-leere Menge von Symbolen. Die Symbole eines Alphabets heißen Buchstaben.\nDef.: Ein Wort $w$ über einem Alphabet $\\Sigma$ ist eine endliche Folge von Symbolen aus $\\Sigma$. $\\epsilon$ ist das leere Wort. Die Länge $\\vert w \\vert$ eines Wortes $w$ ist die Anzahl von Buchstaben, die es enthält (Kardinalität).\nDef.: $\\Sigma^k = \\lbrace w\\ \\text{über}\\ \\Sigma\\ \\vert\\ \\vert w \\vert = k \\rbrace$\n$\\Sigma^{\\ast} = \\bigcup\\limits_{i \\in \\mathbb{N}_0} \\Sigma^i$ (die Kleene-Hülle von $\\Sigma$)\n$\\Sigma^+ = \\bigcup\\limits_{i \\in \\mathbb{N}} \\Sigma^i$ Sprachen über Alphabete Def.: Seien $x = a_1 a_2 \\ \\ldots \\ a_n$ und $y = b_1b_2 \\ \\ldots \\ b_m$ Wörter. Wir nennen $xy = x \\circ y = a_1 \\ \\ldots \\ a_nb_1 \\ \\ldots \\ b_m$ die Konkatenation von $x$ und $y$.\nDef.: Eine Sprache $L$ über einem Alphabet $\\Sigma$ ist eine Teilmenge von $\\Sigma^{\\ast} :\\ L \\subseteq\\Sigma^{\\ast}$\nDeterministische endliche Automaten Def.: Ein deterministischer endlicher Automat (DFA) ist ein 5-Tupel $A = (Q, \\Sigma, \\delta, q_0, F)$ mit\n$Q$ : eine endliche Menge von Zuständen $\\Sigma$ : ein Alphabet von Eingabesymbolen $\\delta$ : die Übergangsfunktion $(Q \\times \\Sigma) \\rightarrow Q, \\delta$ kann partiell sein $q_0 \\in Q$ : der Startzustand $F \\subseteq Q$ : die Menge der Endzustände Die Übergangsfunktion Def.: Wir definieren $\\delta^{\\ast}: (Q \\times \\Sigma^{\\ast}) \\rightarrow Q$: induktiv wie folgt:\nBasis: $\\delta^{\\ast}(q, \\epsilon) = q\\ \\forall q \\in Q$ Induktion: $\\delta^{\\ast}(q, a_1, \\ldots, a_n) = \\delta(\\delta^{\\ast}(q, a_1, \\ldots , a_{n-1}), a_n)$ Def.: Ein DFA akzeptiert ein Wort $w \\in \\Sigma^{\\ast}$ genau dann, wenn $\\delta^{\\ast}(q_0, w) \\in F.$\nDef.: Die Sprache eines DFA $A\\ L(A)$ ist definiert durch:\n$L(A) =\\lbrace w\\ \\vert \\delta^{\\ast}(q_0, w) \\in F \\rbrace$ Beispiel Hier entsteht ein Tafelbild.\nNichtdeterministische endliche Automaten Def.: Ein nichtdeterministischer endlicher Automat (NFA) ist ein 5-Tupel $A = (Q, \\Sigma, \\delta, q_0, F)$ mit\n$Q$: eine endliche Menge von Zuständen $\\Sigma$: ein Alphabet von Eingabesymbolen $\\delta$: die Übergangsfunktion $(Q \\times \\Sigma) \\rightarrow \\mathcal{P}(Q)$ $q_0 \\in Q$: der Startzustand $F \\subseteq Q$: die Menge der Endzustände Die Übergangsfunktion eines NFAs Def.: Wir definieren $\\delta^{\\ast}: (Q \\times \\Sigma) \\rightarrow \\mathcal{P}(Q):$ induktiv wie folgt:\nBasis: $\\delta^{\\ast}(q, \\epsilon) = q\\ \\forall q \\in Q$\nInduktion: Sei $w \\in \\Sigma^{\\ast}, w = xa, x \\in \\Sigma^{\\ast}, a \\in \\Sigma$ mit\n$\\delta^{\\ast}(q, x) = \\lbrace p_1,\\ \\ldots,\\ p_k \\rbrace, p_i \\in Q$, sei\n$A = \\bigcup\\limits_{i = 1}^k \\delta(p_i, a) = \\lbrace r_1, \\ldots r_m \\rbrace, r_j \\in Q$.\nDann ist $\\delta^{\\ast}(q, w) = \\lbrace r_1,\\ \\ldots\\ , r_m\\rbrace$.\nWozu NFAs im Compilerbau? Pattern Matching geht mit NFAs.\nNFAs sind so nicht zu programmieren, aber:\nSatz: Eine Sprache $L$ wird von einem NFA akzeptiert $\\Leftrightarrow L$ wird von einem DFA akzeptiert.\nKonvertierung eines NFAs in einen DFA Gegeben: Ein NFA $A = (Q, \\Sigma, \\delta, q_0, F)$\nWir konstruieren einen DFA $A' = (Q', \\Sigma, \\delta ', q_0, F')$ wie folgt:\nKonvertierung NFA in DFA\nBeispiel $\\delta$ a b $\\rightarrow q_0$ $\\lbrace q_0\\rbrace$ $\\lbrace q_1, q_2\\rbrace$ $q_1$ $\\lbrace q_2\\rbrace$ $\\lbrace q_1\\rbrace$ * $q_2$ - $\\lbrace q_0, q_2\\rbrace$ $\\delta$' a b $\\rightarrow$ $\\lbrace q_0\\rbrace$ $\\lbrace q_0\\rbrace$ $\\lbrace q_1,q_2\\rbrace$ * $\\lbrace q_1 q_2\\rbrace$ $\\lbrace q_2\\rbrace$ $\\lbrace q_0, q_1, q_2\\rbrace$ * $\\lbrace q_2\\rbrace$ - $\\lbrace q_0,q_2\\rbrace$ * $\\lbrace q_0, q_2\\rbrace$ $\\lbrace q_0\\rbrace$ $\\lbrace q_0, q_1, q_2\\rbrace$ * $\\lbrace q_0, q_1, q_2\\rbrace$ $\\lbrace q_0, q_2\\rbrace$ $\\lbrace q_0, q_1, q_2\\rbrace$ Minimierung eines DFAs Ist ist der DFA $A$ nicht vollständig, wird ein Fehlerzustand $q_e$, der kein Endzustand ist, hinzugefügt und in alle leeren Tabellenfelder eingetragen.\nDann wird eine Matrix generiert, die für alle Zustandspaare sagt, ob die beiden Zustände zu einem verschmelzen können.\nDFA Minimierung\nReguläre Ausdrücke Operatoren auf Sprachen Def.: Seien L und M Sprachen.\n$L \\cup M = \\lbrace w \\mid w \\in L \\vee w \\in M \\rbrace$ $LM = L \\cdot M = L \\circ M = \\lbrace vw \\mid v \\in L \\land w \\in M\\rbrace$ Die Kleene-Hülle einer Sprache: Basis: $L^0 = \\lbrace \\epsilon\\rbrace$ Induktion: $L^i = \\lbrace xw\\mid x \\in L^{i-1}, w \\in L, i \u003e 0\\rbrace$, $L^{\\ast} = \\bigcup\\limits_{i \\ge 0}L^i$, $L^+ = \\bigcup\\limits_{i \u003e 0}L^i$ Reguläre Ausdrücke Def.: Induktive Definition von regulären Ausdrücken (regex) und der von ihnen repräsentierten Sprache:\nBasis: $\\epsilon$ und $\\emptyset$ sind reguläre Ausdrücke mit $L(\\epsilon) = \\lbrace \\epsilon\\rbrace$, $L(\\emptyset)=\\emptyset$ Sei $a$ ein Symbol $\\Rightarrow$ $a$ ist ein regex mit $L(a) = \\lbrace a\\rbrace$ Induktion: Seien $E,\\ F$ reguläre Ausdrücke. Dann gilt: $E+F$ ist ein regex und bezeichnet die Vereinigung $L(E + F) = L(E)\\cup L(F)$ $EF$ ist ein regex und bezeichnet die Konkatenation $L(EF) = L(E)L(F)$ $E^{\\ast}$ ist ein regex und bezeichnet die Kleene-Hülle $L(E^{\\ast})=(L(E))^{\\ast}$ $(E)$ ist ein regex mit $L((E)) = L(E)$ Vorrangregeln der Operatoren für reguläre Ausdrücke: *, Konkatenation, +\nWichtige Identitäten Satz: Sei $A$ ein DFA $\\Rightarrow \\exists$ regex $R$ mit $L(A) = L(R)$.\nSatz: Sei $E$ ein regex $\\Rightarrow \\exists$ DFA $A$ mit $L(E) = L(A)$.\nBeispiel: Umwandlung eines regex in einen NFA Hier entsteht ein Tafelbild.\nFormale Grammatiken Def.: Eine formale Grammatik ist ein 4-Tupel $G=(N,T,P,S)$ aus\n$N$: einer endlichen Menge von $Nichtterminalen$ T: einer endlichen Menge von Terminalen, $N \\cap T = \\emptyset$ $S \\in N$: dem Startsymbol P: einer endlichen Menge von Produktionen der Form: $X \\rightarrow Y$ mit $X \\in (N \\cup T)^{\\ast} N (N \\cup T)^{\\ast}, Y \\in (N \\cup T)^{\\ast}$ Ableitungen Def.: Sei $G = (N, T, P, S)$ eine Grammatik, sei $\\alpha A \\beta$ eine Zeichenkette über $(N \\cup T)^{\\ast}$ und sei $A$ $\\rightarrow \\gamma$ eine Produktion von $G$.\nWir sagen: $\\alpha A \\beta \\Rightarrow \\alpha \\gamma \\beta$ ( $\\alpha A \\beta$ leitet $\\alpha \\gamma \\beta$ ab).\nDef.: Wir definieren die Relation $\\overset{\\ast}{\\Rightarrow}$ induktiv wie folgt:\nBasis: $\\forall \\alpha \\in (N \\cup T)^{\\ast} \\alpha \\overset{\\ast}{\\Rightarrow} \\alpha$ (Jede Zeichenkette leitet sich selbst ab.) Induktion: Wenn $\\alpha \\overset{\\ast}{\\Rightarrow} \\beta$ und $\\beta\\Rightarrow \\gamma$ dann $\\alpha \\overset{\\ast}{\\Rightarrow} \\gamma$ Def.: Sei $G = (N, T ,P, S)$ eine formale Grammatik. Dann ist $L(G) = \\lbrace w \\in T^{\\ast} \\mid S \\overset{\\ast}{\\Rightarrow} w\\rbrace$ die von $G$ erzeugte Sprache.\nReguläre Grammatiken Def.: Eine reguläre (oder type-3-) Grammatik ist eine formale Grammatik mit den folgenden Einschränkungen:\nAlle Produktionen sind entweder von der Form\n$X \\to aY$ mit $X \\in N, a \\in T, Y \\in N$ (rechtsreguläre Grammatik) oder $X \\to Ya$ mit $X \\in N, a \\in T, Y \\in N$ (linksreguläre Grammatik) $X\\rightarrow\\epsilon$ ist in beiden Fällen erlaubt.\nReguläre Sprachen Satz: Die von rechtsregulären Grammatiken erzeugten Sprachen sind genau die von linksregulären Grammatiken erzeugten Sprachen. Beide werden reguläre Sprachen genannt.\nSatz: Die von regulären Ausdrücken beschriebenen Sprachen sind die regulären Sprachen.\nDas Pumping Lemma für reguläre Sprachen Satz: Das Pumping Lemma für reguläre Sprachen:\nSei $L$ eine reguläre Sprache.\n$\\Rightarrow \\exists$ Konstante $n \\in \\mathbb{N}$:\n$\\underset{\\underset{|w| \\geq n} {w \\in L}}\\forall \\exists x, y, z \\in \\Sigma^{*}$ mit $w = xyz, y \\neq \\epsilon, |xy| \\leq n:$\n$\\underset{k \\geq 0} \\forall xy^{k}z \\in L$ Abschlusseigenschaften regulärer Sprachen Die Klasse der regulären Sprachen ist abgeschlossen unter\nVereinigung Konkatenation Kleene-Stern Komplementbildung Durchschnitt Entscheidbarkeit für reguläre Sprachen Satz: Es ist entscheidbar,\nob eine gegebene reguläre Sprache leer ist ob $w \\in \\Sigma^{\\ast}$ in einer gegebenen regulären Sprache enthalten ist (Das \"Wort-Problem\") ob zwei reguläre Sprachen äquivalent sind Grenzen der regulären Sprachen Reguläre Sprachen sind von ihrer Struktur her einfach. Schon Sprachen, in denen etwas \"gematcht\" werden muss, lassen sich nicht mehr regulär beschreiben, weil z. B. die fixe Anzahl von Zuständen eines DFAs die Erkennung solcher Sprachen verhindert.\nWozu das Ganze? Im Compilerbau werden reguläre Ausdrücke benutzt, um die Schlüsselwörter und weitere Symbole der zu erkennenden Sprache anzugeben. Daraus wird mit Hilfe eines Generators, der aus den regulären Ausdrücken DFAs (oder einen großen DFA) macht, der sog. Scanner oder Lexer genannt, generiert. Seine Aufgabe ist es, die Folge von Zeichen in der Quelldatei in eine Folge von sog. Token umzuwandeln. Z. B. wird so aus den Zeichen des Schlüsselwortes while im Programmtext das Token für while gemacht, das in der Syntaxanalyse weiterverarbeitet wird. Die Tokenfolge eines Programms ist ein Wort einer Sprache, die der Parser erkennt. Jedes vom Lexer erkannte Token ist dort also ein terminales Symbol.\nEin Lexer ist mehr als ein DFA Was ist zu beachten:\nMan braucht mindestens eine Liste von Paaren aus regulären Ausdrücken und Tokennamen. Neben den Schlüsselwörtern und Symbolen wie (,), *, $\\ldots$ müssen auch Namen für Variablen, Funktionen, Klassen, Methoden, $\\ldots$ (sog. Identifier) erkannt werden Namen haben meist eine gewisse Struktur, die sich mit regulären Ausdrücken beschreiben lassen. Erlaubte Token sind in der Grammatik des Parsers beschrieben, d. h. für literale Namen, Strings, Zahlen liefert der Scanner zwei Werte: z. B. \u003cID, \"radius\"\u003e, \u003cIntegerzahl, 558\u003e Kommentare und Strings müssen richtig erkannt werden. (Schachtelungen) Man kann natürlich auch einen Lexer selbst programmieren, d. h. die DFAs für die regulären Ausdrücke implementieren.\nAutomatisch oder händisch Hier entsteht ein Tafelbild.\nWrap-Up Wrap-Up Definition und Aufgaben von Lexern DFAs und NFAs Reguläre Ausdrücke Reguläre Grammatiken Zusammenhänge zwischen diesen Mechanismen und Lexern, bzw. Lexergeneratoren ",
    "description": "",
    "tags": null,
    "title": "Reguläre Sprachen, Ausdrucksstärke",
    "uri": "/frontend/lexing/regular.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24) \u003e Frontend \u003e Lexer",
    "content": "Lexer: Erzeugen eines Token-Stroms aus einem Zeichenstrom Aus dem Eingabe(-quell-)text\n/* demo */ a= [5 , 6] ;erstellt der Lexer (oder auch Scanner genannt) eine Sequenz von Token:\n\u003cID, \"a\"\u003e \u003cASSIGN\u003e \u003cLBRACK\u003e \u003cNUM, 5\u003e \u003cCOMMA\u003e \u003cNUM, 6\u003e \u003cRBRACK\u003e \u003cSEMICOL\u003e Input: Zeichenstrom (Eingabedatei o.ä.) Verarbeitung: Finden sinnvoller Sequenzen im Zeichenstrom (\"Lexeme\"), Einteilung in Kategorien und Erzeugen von Token (Paare: Typ/Name, Wert) Ausgabe: Tokenstrom Normalerweise werden für spätere Phasen unwichtige Elemente wie White-Space oder Kommentare entfernt.\nDurch diese Vorverarbeitung wird eine höhere Abstraktionsstufe erreicht und es können erste grobe Fehler gefunden werden. Dadurch kann der Parser auf einer abstrakteren Stufe arbeiten und muss nicht mehr den gesamten ursprünglichen Zeichenstrom verarbeiten.\nAnmerkung: In dieser Phase steht die Geschwindigkeit stark im Vordergrund: Der Lexer \"sieht\" alle Zeichen im Input. Deshalb findet man häufig von Hand kodierte Lexer, obwohl die Erstellung der Lexer auch durch Generatoren erledigt werden könnte ...\nAnmerkung: Die Token sind die Terminalsymbole in den Parserregeln (Grammatik).\nDefinition wichtiger Begriffe Token: Tupel (Tokenname, optional: Wert)\nDer Tokenname ist ein abstraktes Symbol, welches eine lexikalische Einheit repräsentiert (Kategorie). Die Tokennamen sind die Eingabesymbole für den Parser.\nToken werden i.d.R. einfach über ihren Namen referenziert. Token werden häufig zur Unterscheidung von anderen Symbolen in der Grammatik in Fettschrift oder mit großen Anfangsbuchstaben geschrieben.\nEin Token kann einen Wert haben, etwa eine Zahl oder einen Bezeichner, der auf das zum Token gehörende Pattern gematcht hatte (also das Lexem). Wenn der Wert des Tokens eindeutig über den Namen bestimmt ist (im Beispiel oben beim Komma oder den Klammern), dann wird häufig auf den Wert verzichtet.\nLexeme: Sequenz von Zeichen im Eingabestrom, die auf ein Tokenpattern matcht und vom Lexer als Instanz dieses Tokens identifiziert wird.\nPattern: Beschreibung der Form eines Lexems\nBei Schlüsselwörtern oder Klammern etc. sind dies die Schlüsselwörter oder Klammern selbst. Bei Zahlen oder Bezeichnern (Namen) werden i.d.R. reguläre Ausdrücke zur Beschreibung der Form des Lexems formuliert.\nErkennung mit RE und DFA Die obige Skizze ist eine Kurzzusammenfassung der Theorie-Vorlesung in der letzten Woche und stellt die Verbindung zur heutigen Vorlesung her:\nDie Lexeme werden mit Hilfe von DFA bestimmt. Die Formulierung der DFA ist eher komplex (zumindest sehr umständlich), weshalb man die Pattern für die Lexeme ersatzweise mit Hilfe von Regulären Ausdrücken (\"RE\") formuliert.\nMit Hilfe der Thompson's Construction kann man diese in äquivalente NFA umformen. Über die Subset Construction kann man daraus DFA erzeugen, die wiederum mit Hilfe des Hopcroft's Algorithm minimiert werden.\nDiese DFA erkennen die selbe Sprache wie die ursprünglichen REs. Man könnte also durch Simulation der DFA die Lexeme erkennen und die Token bilden. Dabei würde pro Eingabezeichen ein Übergang im DFA stattfinden und bei Erreichen eines akzeptierenden Zustandes hätte man das durch diesen DFA (bzw. dessen ursprünglichen RE) beschriebene Lexem identifiziert.\nFalls mehrere REs matchen, muss man in geeigneter Weise entscheiden. I.d.R. nimmt man den längsten Match. Zusätzlich wird eine Reihenfolge unter den REs festgelegt, um bei mehreren gleich langen Matches ein Token bestimmen zu können.\nIn der Praxis werden die DFA als Ausgangspunkt für die Implementierung des Lexers genutzt (ob nun bei einer \"handgeklöppelten\" Implementierung oder beim Einsatz eines Lexer-Generators). Als typische Implementierungsansätze sollen nachfolgend die tabellenbasierte Implementierung sowie als etwas schnellere Variante die direkt codierte Implementierung betrachtet werden. Während diese beiden Varianten noch sehr nah an der Simulation eines DFA sind, ist die manuelle Implementierung (vgl. Handcodierte Implementierung) noch einfacher in bestehenden Code zu integrieren (zum Preis einer erschwerten Änderbarkeit).\nÜber die Kleene's Construction könnte man aus den DFA wieder RE erzeugen und damit den Kreis schließen :-)\nErkennen von Zeichenketten für Strickmuster: \"10LRL\" DFA zur Erkennung von Strickanweisungen: Das erste Zeichen muss ein Digit im Bereich 1..9 sein, gefolgt von weiteren Digits, gefolgt von einer Anweisung für linke Maschen (\"L\") oder rechte Maschen (\"R\").\nEin passender regulärer Ausdruck dafür wäre \"[1-9][0-9]*[LR]+\".\nDie Eingabezeichen werden in relevante Kategorien sortiert. Dabei werden nur die für die Aufgabe interessanten Zeichen (\"R\" bzw. \"L\" und die Ziffern) einer konkreten Kategorie zugewiesen, der Rest wird als \"*\" zusammengefasst.\nFür jeden Zustand wird in der Tabelle vermerkt, in welchen Folgezustand beim Auftreten eines Zeichens einer bestimmten Kategorie gewechselt werden soll. Dies ist eine alternative Darstellung des DFA in der obigen Darstellung.\nDie Zustände des DFA werden den Tokentypen zugeordnet. Alle Zustände außer \"s2\" entsprechen keinem gültigen Token, dies könnte man etwa als Token-Typ \"invalid\" realisieren.\nAnmerkung: \"se\" ist ein Fehlerzustand, der im Automaten oben nicht dargestellt ist und der dazu dient, falsche Zeichen zu erkennen und entsprechend zu antworten.\nTabellenbasierte Implementierung def nextToken(): state = s0; lexeme = \"\"; stack = Stack() while (state != se): consume() # hole nächstes Zeichen (peek) lexeme += peek stack.push(state) state = TransitionTable[state, peek] while (state != s2 and stack.notEmpty()): state = stack.pop(); putBack(lexeme.truncate()) if state == s2: return s2(lexeme) else: return invalid()Der dargestellte Code implementiert direkt den DFA zur Erkennung von Register-Namen unter Nutzung der Tabellen aus dem letzten Abschnitt.\nDie Funktion consume() \"verbraucht\" das aktuelle Zeichen \"peek\" und holt das nächste Zeichen aus dem Eingabestrom:\ndef consume(): peek = nextChar()Nach einer Initialisierung wird in der Hauptschleife nach dem nächsten Zeichen im Eingabestrom gefragt und das Lexem erweitert. Anschließend wird der aktuelle Zustand auf dem Stack gesichert und mit Hilfe der Transitionstabelle und des aktuellen Zustands sowie des aktuellen Zeichens peek der Folgezustand bestimmt. Sobald der Fehlerzustand \"se\" erreicht wird, bricht die Schleife ab.\nAnmerkung: Wenn wir in \"s2\" sind, wird so lange nach weiteren Buchstaben \"L\" oder \"R\" gesucht, bis im Strom irgendetwas anderes auftaucht und wir entsprechend in \"se\" landen.\nIn der zweiten Schleife wird der Stack aufgerollt, um zu schauen, ob wir früher bereits in \"s2\" waren oder nicht. Das erste Element wird vom Stack genommen, das Lexem wird um das letzte Zeichen gekürzt und dieses letzte Zeichen wird mit putBack() in den Eingabestrom zurückgelegt. Falls wir früher bereits in \"s2\" waren, wird dieser Zustand irgendwann vom Stack genommen. Anderenfalls ist der Stack irgendwann leer.\nFalls \"s2\" erreicht wurde, wird ein neues \"s2\"-Token generiert und das Lexem wird als Attribut direkt gesetzt. Anderenfalls lag ein Fehler vor.\nAnmerkung: Diese Implementierung ist generisch: Wenn man im Code die direkte Nennung des akzeptierenden Zustands \"s2\" durch einen Vergleich mit einer Menge aller akzeptierender Zustände ersetzt (\"state == s2\" =\u003e \"state in acceptedStates\"), bestimmen nur die Tabellen die konkrete Funktionsweise.\nDie Tabellen können allerdings schnell sehr groß werden, insbesondere die Zustandsübergangstabelle!\nDirekt codierte Implementierung Die Implementierung über die Tabellen ist sowohl generisch als auch effizient. Allerdings kostet jeder Zugriff auf die Tabelle konstanten Aufwand (Erinnerung: Zugriff auf Arrays, Pointerarithmetik), der sich in der Praxis deutlich summieren kann. Außerdem müssen der Stack gepflegt (erweitert und später wieder reduziert) werden und Objekte für die Zustände angelegt werden.\nDie Lösung: Aufrollen der while-Schleife und direkt Umsetzung der Tabelle im Code mit Sprungbefehlen (\"goto\"):\ndef nextToken(): lexeme = \"\"; stack = Stack() goto s0 s0: consume() # hole nächstes Zeichen (peek) lexeme += peek stack.push(s0) if peek == '1' || ... || peek == \"9\": goto s1 else: goto se ...Durch die direkte Kodierung der Tabellen in Form von Sprungzielen für goto-Befehle spart man sich die Formulierung der Tabellen und den Zugriff auf die Inhalte. Allerdings ist der Code deutlich schwerer lesbar und auch deutlich schwerer an eine andere Sprache anpassbar. Dies stellt aber keinen echten Nachteil dar, wenn er durch einen Generator aus einer Grammatik o.ä. erzeugt wird.\nWrap-Up Zusammenhang DFA, RE und Lexer\nImplementierungsansatz: Tabellenbasiert (DFA-Tabellen)\n",
    "description": "",
    "tags": null,
    "title": "Lexer: Tabellenbasierte Implementierung",
    "uri": "/frontend/lexing/table.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24) \u003e Frontend \u003e Lexer",
    "content": "Lexer: Erzeugen eines Token-Stroms aus einem Zeichenstrom Aus dem Eingabe(-quell-)text\n/* demo */ a= [5 , 6] ;erstellt der Lexer (oder auch Scanner genannt) eine Sequenz von Token:\n\u003cID, \"a\"\u003e \u003cASSIGN\u003e \u003cLBRACK\u003e \u003cNUM, 5\u003e \u003cCOMMA\u003e \u003cNUM, 6\u003e \u003cRBRACK\u003e \u003cSEMICOL\u003e Manuelle Implementierung: Rekursiver Abstieg def nextToken(): while (peek != EOF): # globale Variable, über consume() switch (peek): case ' ': case '\\t': case '\\n': WS(); continue case '[': consume(); return Token(LBRACK, '[') ... default: if isLetter(peek): return NAME() raise Error(\"invalid character: \"+peek) return Token(EOF_Type, \"\u003cEOF\u003e\") def WS(): while (peek == ' ' || peek == '\\t' || ...): consume() def NAME(): buf = StringBuilder() do { buf.append(peek); consume(); } while (isLetter(peek)) return Token(NAME, buf.toString())Die manuelle Implementierung \"denkt\" nicht in den Zuständen des DFA, sondern orientiert sich immer am aktuellen Zeichen \"peek\". Abhängig von dessen Ausprägung wird entweder direkt ein Token erzeugt und das Zeichen aus dem Eingabestrom entfernt sowie das nächste Zeichen eingelesen (mittels der Funktion consume(), nicht dargestellt im Beispiel), oder man ruft weitere Funktionen auf, die das Gewünschte erledigen, beispielsweise um White-Spaces zu entfernen oder um einen Namen einzulesen: Nach einem Buchstaben werden alle folgenden Buchstaben dem Namen (Bezeichner) hinzugefügt. Sobald ein anderes Zeichen im Eingabestrom erscheint, wird das Namen-Token erzeugt.\nDie Funktion consume() \"verbraucht\" das aktuelle Zeichen \"peek\" und holt das nächste Zeichen aus dem Eingabestrom.\nAnmerkung: Häufig findet man im Lexer keinen \"schönen\" objektorientierten Ansatz. Dies ist i.d.R. Geschwindigkeitsgründen geschuldet ...\nRead-Ahead: Unterscheiden von \"\u003c\" und \"\u003c=\" def nextToken(): while (peek != EOF): # globale Variable switch (peek): case '\u003c': if match('='): consume(); return Token(LE, \"\u003c=\") else: consume(); return Token(LESS, '\u003c') ... return Token(EOF_Type, \"\u003cEOF\u003e\") def match(c): # Lookahead: Ein Zeichen consume() if (peek == c): return True else: rollBack(); return FalseUm die Token \"\u003c\" und \"\u003c=\" unterscheiden zu können, müssen wir ein Zeichen vorausschauen: Wenn nach dem \"\u003c\" noch ein \"=\" kommt, ist es \"\u003c=\", sonst \"\u003c\".\nErinnerung: Die Funktion consume() liest das nächste Zeichen aus dem Eingabestrom und speichert den Wert in der globalen Variable peek.\nFür das Read-Ahead wird die Funktion match() definiert, die zunächst das bereits bekannte Zeichen, in diesem Fall das \"\u003c\" durch das nächste Zeichen im Eingabestrom ersetzt (Aufruf von consume()). Falls der Vergleich des Lookahead-Zeichens mit dem gesuchten Zeichen erfolgreich ist, liegt das \"größere\" Token vor, also \"\u003c=\". Dann wird noch das \"=\" durch das nächste Zeichen ersetzt und das Token LE gebildet. Anderenfalls muss das zuviel gelesene Zeichen wieder in den Eingabestrom zurückgelegt werden (rollBack()).\nPuffern des Input-Stroms: Double Buffering Das Einlesen einzelner Zeichen führt zwar zu eleganten algorithmischen Lösungen, ist aber zur Laufzeit deutlich \"teurer\" als das Einlesen mit gepufferten I/O-Operationen, die eine ganze Folge von Zeichen einlesen (typischerweise einen ganzen Disk-Block, beispielsweise 4096 Zeichen).\nDazu kann man einen Ringpuffer nutzen, den man mit Hilfe von zwei gleich großen char-Puffern mit jeweils der Länge $N$ simulieren kann. ( $N$ sollte dann der Länge eines Disk-Blocks entsprechen.)\nVergleiche auch Wikipedia: \"Circular Buffer\".\nstart = 0; end = 0; fill(buffer[0:n]) def consume(): peek = buffer[start] start = (start+1) mod 2n if (start mod n == 0): fill(buffer[start:start+n-1]) end = (start+n) mod 2n def rollBack(): if (start == end): raise Error(\"roll back error\") start = (start-1) mod 2nZunächst wird nur der vordere Pufferteil durch einen passenden Systemaufruf gefüllt.\nBeim Weiterschalten im simulierten DFA oder im manuell kodierten Lexer (Funktionsaufruf von consume()) wird das nächste Zeichen aus dem vorderen Pufferteil zurückgeliefert. Über die Modulo-Operation bleibt der Pointer start immer im Speicherbereich der beiden Puffer.\nWenn man das Ende des vorderen Puffers erreicht, wird der hintere Puffer mit einem Systemaufruf gefüllt. Gleichzeitig wird ein Hilfspointer end auf den Anfang des vorderen Puffers gesetzt, um Fehler beim Roll-Back zu erkennen.\nWenn man das Ende des hinteren Puffers erreicht, wird der vordere Puffer nachgeladen und der Hilfspointer auf den Anfang des hinteren Puffers gesetzt.\nIm Grunde ist also immer ein Puffer der \"Arbeitspuffer\" und der andere enthält die bereits gelesene (verarbeitete) Zeichenkette. Wenn beim Nachladen weniger als $N$ Zeichen gelesen werden, liefert der Systemaufruf als letztes \"Zeichen\" ein EOF. Beim Verarbeiten wird peek entsprechend diesen Wert bekommen und der Lexer muss diesen Wert abfragen und berücksichtigen.\nFür das Roll-Back wird der start-Pointer einfach dekrementiert (und mit einer Modulo-Operation auf den Speicherbereich der beiden Puffer begrenzt). Falls dabei der end-Pointer \"eingeholt\" wird, ist der start-Pointer durch beide Puffer zurückgelaufen und es gibt keinen früheren Input mehr. In diesem Fall wird entsprechend ein Fehler gemeldet.\nAnmerkung: In der Regel sind die Lexeme kurz und man muss man nur ein bis zwei Zeichen im Voraus lesen. Dann ist eine Puffergröße von 4096 Zeichen mehr als ausreichend groß und man sollte nicht in Probleme laufen. Wenn der nötige Look-Ahead aber beliebig groß werden kann, etwa bei Sprachen ohne reservierte Schlüsselwörtern oder bei Kontext-sensitiven Lexer-Grammatiken (denken Sie etwa an die Einrücktiefe bei Python), muss man andere Strategien verwenden. ANTLR beispielsweise vergrößert in diesem Fall den Puffer dynamisch, alternativ könnte man die Auflösung zwischen Schlüsselwörtern und Bezeichnern dem Parser überlassen.\nTypische Muster für Erstellung von Token Schlüsselwörter\nEin eigenes Token (RE/DFA) für jedes Schlüsselwort, oder Erkennung als Name und Vergleich mit Wörterbuch und nachträgliche Korrektur des Tokentyps Wenn Schlüsselwörter über je ein eigenes Token abgebildet werden, benötigt man für jedes Schlüsselwort einen eigenen RE bzw. DFA. Die Erkennung als Bezeichner und das Nachschlagen in einem Wörterbuch (geeignete Hashtabelle) sowie die entsprechende nachträgliche Korrektur des Tokentyps kann die Anzahl der Zustände im Lexer signifikant reduzieren!\nOperatoren\nEin eigenes Token für jeden Operator, oder Gemeinsames Token für jede Operatoren-Klasse Bezeichner: Ein gemeinsames Token für alle Namen\nZahlen: Ein gemeinsames Token für alle numerischen Konstante (ggf. Integer und Float unterscheiden)\nFür Zahlen führt man oft ein Token \"NUM\" ein. Als Attribut speichert man das Lexem i.d.R. als String. Alternativ kann man (zusätzlich) das Lexem in eine Zahl konvertieren und als (zusätzliches) Attribut speichern. Dies kann in späteren Stufen viel Arbeit sparen.\nString-Literale: Ein gemeinsames Token\nKomma, Semikolon, Klammern, ...: Je ein eigenes Token\nRegeln für White-Space und Kommentare etc. ...\nNormalerweise benötigt man Kommentare und White-Spaces in den folgenden Stufen nicht und entfernt diese deshalb aus dem Eingabestrom. Dabei könnte man etwa White-Spaces in den Pattern der restlichen Token berücksichtigen, was die Pattern aber sehr komplex macht. Die Alternative sind zusätzliche Pattern, die auf die White-Space und anderen nicht benötigten Inhalt matchen und diesen \"geräuschlos\" entfernen. Mit diesen Pattern werden keine Token erzeugt, d.h. der Parser und die anderen Stufen bemerken nichts von diesem Inhalt.\nGelegentlich benötigt man aber auch Informationen über White-Spaces, beispielsweise in Python. Dann müssen diese Token wie normale Token an den Parser weitergereicht werden.\nJedes Token hat i.d.R. ein Attribut, in dem das Lexem gespeichert wird. Bei eindeutigen Token (etwa bei eigenen Token je Schlüsselwort oder bei den Interpunktions-Token) kann man sich das Attribut auch sparen, da das Lexem durch den Tokennamen eindeutig rekonstruierbar ist.\nToken Beschreibung Beispiel-Lexeme if Zeichen i und f if relop \u003c oder \u003e oder \u003c= oder \u003e= oder == oder != \u003c, \u003c= id Buchstabe, gefolgt von Buchstaben oder Ziffern pi, count, x3 num Numerische Konstante 42, 3.14159, 0 literal Alle Zeichen außer \", in \" eingeschlossen \"core dumped\" Anmerkung: Wenn es mehrere matchende REs gibt, wird in der Regel das längste Lexem bevorzugt. Wenn es mehrere gleich lange Alternativen gibt, muss man mit Vorrangregeln bzgl. der Token arbeiten.\nFehler bei der Lexikalischen Analyse Problem: Eingabestrom sieht so aus: fi (a==42) { ... }\nDer Lexer kann nicht erkennen, ob es sich bei fi um ein vertipptes Schlüsselwort handelt oder um einen Bezeichner: Es könnte sich um einen Funktionsaufruf der Funktion fi() handeln ... Dieses Problem kann erst in der nächsten Stufe sinnvoll erkannt und behoben werden.\n=\u003e Was tun, wenn keines der Pattern auf den Anfang des Eingabestroms passt?\nOptionen:\nAufgeben ...\nEventuell vielleicht sogar die beste und einfachste Variante :-)\n\"Panic Mode\": Entferne so lange Zeichen, bis ein Pattern passt.\nDas verwirrt u.U. den Parser, kann aber insbesondere in interaktiven Umgebungen hilfreich sein. Ggf. kann man dem Parser auch signalisieren, dass hier ein Problem vorlag.\nEin-Schritt-Transformationen:\nFüge fehlendes Zeichen in Eingabestrom ein. Entferne ein Zeichen aus Eingabestrom. Vertausche ein Zeichen: Ersetze ein Zeichen durch ein anderes. Vertausche zwei benachbarte Zeichen. Diese Transformationen versuchen, den Input in einem Schritt zu reparieren. Das ist durchaus sinnvoll, da in der Praxis die meisten Fehler in dieser Stufe durch ein einzelnes Zeichen hervorgerufen werden: Es fehlt ein Zeichen oder es ist eines zuviel im Input. Es liegt ein falsches Zeichen vor (Tippfehler) oder zwei benachbarte Zeichen wurden verdreht ...\nIm Prinzip könnte man auch eine allgemeinere Strategie versuchen, indem man diejenige Transformation mit der kleinsten Anzahl von Schritten zur Fehlerbehebung bestimmt. Beispiele dafür finden sich im Bereich Natural Language Processing (NLP), etwa die Levenshtein-Distanz oder der SoundEx-Algorithmus oder sogar Hidden-Markov-Modelle. Allerdings muss man sich in Erinnerung rufen, dass gerade in dieser ersten Phase eines Compilers die Geschwindigkeit stark im Fokus steht und eine ausgefeilte Fehlerkorrekturstrategie die vielen kleinen Optimierungen schnell wieder zunichte machen kann.\nFehler-Regeln: Matche typische Typos\nGelegentlich findet man in den Grammatiken für den Lexer extra Regeln, die häufige bzw. typische Typos matchen und dann passend darauf reagieren.\nWrap-Up Zusammenhang DFA, RE und Lexer\nImplementierungsansatz: Manuell codiert (rekursiver Abstieg)\nRead-Ahead\nPuffern mit Doppel-Puffer-Strategie\nTypische Fehler beim Scannen\n",
    "description": "",
    "tags": null,
    "title": "Lexer: Handcodierte Implementierung",
    "uri": "/frontend/lexing/recursive.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24) \u003e Frontend \u003e Lexer",
    "content": "Hello World grammar Hello; start : 'hello' GREETING ; GREETING : [a-zA-Z]+ ; WHITESPACE : [ \\t\\n]+ -\u003e skip ; Konsole: Hello (Classpath, Aliase, grun, Main, Dateien, Ausgabe) Hinweis zur Grammatik (Regeln) start ist eine Parser-Regel =\u003e Eine Parser-Regel pro Grammatik wird benötigt, damit man den generierten Parser am Ende auch starten kann ... Die anderen beiden Regeln (mit großem Anfangsbuchstaben) aus der obigen Grammatik zählen zum Lexer ANTLR einrichten Aktuelle Version herunterladen: antlr.org, für Java als Zielsprache: \"Complete ANTLR 4.x Java binaries jar\" CLASSPATH setzen: export CLASSPATH=\".:/\u003cpathToJar\u003e/antlr-4.11.1-complete.jar:$CLASSPATH\" Aliase einrichten (.bashrc): alias antlr='java org.antlr.v4.Tool' alias grun='java org.antlr.v4.gui.TestRig' Alternativ über den Python-Installer: pip install antlr4-tools Im Web ohne lokale Installation: ANTLR Lab (vgl. github.com/antlr/antlr4/blob/master/doc/getting-started.md)\n\"Hello World\" übersetzen und ausführen Grammatik übersetzen und Code generieren: antlr Hello.g4 Java-Code kompilieren: javac *.java Lexer ausführen: grun Hello start -tokens (Grammatik \"Hello\", Startregel \"start\")\nAlternativ mit kleinem Java-Programm:\nimport org.antlr.v4.runtime.*; public class Main { public static void main(String[] args) throws Exception { Lexer l = new HelloLexer(CharStreams.fromStream(System.in)); Token t = l.nextToken(); while (t.getType() != Token.EOF) { System.out.println(t); t = l.nextToken(); } } } Generierte Dateien und Klassen Nach dem Übersetzen finden sich folgende Dateien und Klassen vor:\n. ├── bin │ ├── HelloBaseListener.class │ ├── HelloBaseVisitor.class │ ├── HelloLexer.class │ ├── HelloListener.class │ ├── HelloParser.class │ ├── HelloParser$RContext.class │ ├── HelloVisitor.class │ └── Main.class ├── Hello.g4 └── src ├── HelloBaseListener.java ├── HelloBaseVisitor.java ├── HelloLexer.java ├── HelloLexer.tokens ├── HelloListener.java ├── HelloParser.java ├── Hello.tokens ├── HelloVisitor.java └── Main.java Anmerkung: Die Ordnerstruktur wurde durch ein ANTLR-Plugin für Eclipse erzeugt. Bei Ausführung in der Konsole liegen alle Dateien in einem Ordner.\nAnmerkung: Per Default werden nur die Listener angelegt, für die Visitoren muss eine extra Option mitgegeben werden.\nDie Dateien Hello.tokens und HelloLexer.tokens enthalten die Token samt einer internen Nummer. (Der Inhalt beider Dateien ist identisch.)\nDie Datei HelloLexer.java enthält den generierten Lexer, der eine Spezialisierung der abstrakten Basisklasse Lexer darstellt. Über den Konstruktor wird der zu scannende CharStream gesetzt. Über die Methode Lexer#nextToken() kann man sich die erkannten Token der Reihe nach zurückgeben lassen. (Diese Methode wird letztlich vom Parser benutzt.)\nDie restlichen Dateien werden für den Parser und verschiedene Arten der Traversierung des AST generiert (vgl. AST-basierte Interpreter).\nBedeutung der Ausgabe Wenn man dem Hello-Lexer die Eingabe\nhello world \u003cEOF\u003e (das \u003cEOF\u003e wird durch die Tastenkombination STRG-D erreicht) gibt, dann lautet die Ausgabe\n$ grun Hello start -tokens hello world \u003cEOF\u003e [@0,0:4='hello',\u003c'hello'\u003e,1:0] [@1,6:10='world',\u003cGREETING\u003e,1:6] [@2,12:11='\u003cEOF\u003e',\u003cEOF\u003e,2:0] Die erkannten Token werden jeweils auf einer eigenen Zeile ausgegeben.\n@0: Das erste Token (fortlaufend nummeriert, beginnend mit 0) 0:4: Das Token umfasst die Zeichen 0 bis 4 im Eingabestrom ='hello': Das gefundene Lexem (Wert des Tokens) \u003c'hello'\u003e: Das Token (Name/Typ des Tokens) 1:0: Das Token wurde in Zeile 1 gefunden (Start der Nummerierung mit Zeile 1), und startet in dieser Zeile an Position 0 Entsprechend bekommt man mit\n$ grun Hello start -tokens hello world \u003cEOF\u003e [@0,0:4='hello',\u003c'hello'\u003e,1:0] [@1,8:12='world',\u003cGREETING\u003e,2:2] [@2,15:14='\u003cEOF\u003e',\u003cEOF\u003e,4:0] ANTLR-Grammatik für die Lexer-Generierung Start der Grammatik mit dem Namen \"XYZ\" mit\ngrammar XYZ; oder (nur Lexer)\nlexer grammar XYZ; Token und Lexer-Regeln starten mit großen Anfangsbuchstaben (Ausblick: Parser-Regeln starten mit kleinen Anfangsbuchstaben)\nFormat: TokenName : Alternative1 | ... | AlternativeN ;\nRekursive Lexer-Regeln sind erlaubt. Achtung: Es dürfen keine links-rekursiven Regeln genutzt werden, etwa wie ID : ID '*' ID ; ... (Eine genauere Definition und die Transformation in nicht-linksrekursive Regeln siehe LL-Parser).\nAlle Literale werden in einfache Anführungszeichen eingeschlossen (es erfolgt keine Unterscheidung zwischen einzelnen Zeichen und Strings wie in anderen Sprachen)\nZeichenmengen: [a-z\\n] umfasst alle Zeichen von 'a' bis 'z' sowie '\\n'\n'a'..'z' ist identisch zu [a-z]\nSchlüsselwörter: Die folgenden Strings stellen reservierte Schlüsselwörter dar und dürfen nicht als Token, Regel oder Label genutzt werden:\nimport, fragment, lexer, parser, grammar, returns, locals, throws, catch, finally, mode, options, tokens Anmerkung: rule ist zwar kein Schlüsselwort, wird aber als Methodenname bei der Codegenerierung verwendet. =\u003e Wie ein Schlüsselwort behandeln!\n(vgl. github.com/antlr/antlr4/blob/master/doc/lexicon.md)\nGreedy und Non-greedy Lexer-Regeln Die regulären Ausdrücke (...)?, (...)* und (...)+ sind greedy und versuchen soviel Input wie möglich zu matchen.\nFalls dies nicht sinnvoll sein sollte, kann man mit einem weiteren ? das Verhalten auf non-greedy umschalten. Allerdings können non-greedy Regeln das Verhalten des Lexers u.U. schwer vorhersehbar machen!\nDie Empfehlung ist, non-greedy Lexer-Regeln nur sparsam einzusetzen (vgl. github.com/antlr/antlr4/blob/master/doc/wildcard.md).\nVerhalten des Lexers: 1. Längster Match Primäres Ziel: Erkennen der längsten Zeichenkette\nCHARS : [a-z]+ ; DIGITS : [0-9]+ ; FOO : [a-z]+ [0-9]+ ;Die Regel, die den längsten Match für die aktuelle Eingabesequenz produziert, \"gewinnt\".\nIm Beispiel würde ein \"foo42\" als FOO erkannt und nicht als CHARS DIGITS.\nVerhalten des Lexers: 2. Reihenfolge Reihenfolge in Grammatik definiert Priorität\nFOO : 'f' .*? 'r' ; BAR : 'foo' .*? 'bar' ;Falls mehr als eine Lexer-Regel die selbe Inputsequenz matcht, dann hat die in der Grammatik zuerst genannte Regel Priorität.\nIm Beispiel würden für die Eingabe \"foo42bar\" beide Regeln den selben längsten Match liefern - die Regel FOO ist in der Grammatik früher definiert und \"gewinnt\".\nVerhalten des Lexers: 3. Non-greedy Regeln Non-greedy Regeln versuchen so wenig Zeichen wie möglich zu matchen\nFOO : 'foo' .*? 'bar' ; BAR : 'bar' ;Hier würde ein \"foo42barbar\" zu FOO gefolgt von BAR erkannt werden.\nAchtung: Nach dem Abarbeiten einer non-greedy Sub-Regel in einer Lexer-Regel gilt \"first match wins\"\n.*? ('4' | '42')\n=\u003e Der Teil '42' auf der rechten Seite ist \"toter Code\" (wegen der non-greedy Sub-Regel .*?)!\nDie Eingabe \"x4\" würde korrekt erkannt, währende \"x42\" nur als \"x4\" erkannt wird und für die verbleibende \"2\" würde ein token recognition error geworfen.\n(vgl. github.com/antlr/antlr4/blob/master/doc/wildcard.md)\nAttribute und Aktionen grammar Demo; @header { import java.util.*; } @members { String s = \"\"; } start : TYPE ID '=' INT ';' ; TYPE : ('int' | 'float') {s = getText();} ; INT : [0-9]+ {System.out.println(s+\":\"+Integer.valueOf(getText()));}; ID : [a-z]+ {setText(String.valueOf(getText().charAt(0)));} ; WS : [ \\t\\n]+ -\u003e skip ;Attribute bei Token (Auswahl) Token haben Attribute, die man abfragen kann. Dies umfasst u.a. folgende Felder:\ntext: Das gefundene Lexem als String type: Der Token-Typ als Integer index: Das wievielte Token (als Integer) (vgl. github.com/antlr/antlr4/blob/master/doc/actions.md)\nZur Auswertung in den Lexer-Regeln muss man anders vorgehen als in Parser-Regeln: Nach der Erstellung eines Tokens kann man die zum Attribut gehörenden getX() und setX()-Methoden aufrufen, um die Werte abzufragen oder zu ändern.\nDies passiert im obigen Beispiel für das Attribut text: Abfrage mit getText(), Ändern/Setzen mit setText().\nDie Methodenaufrufe wirken sich immer auf das gerade erstellte Token aus.\nAchtung: Bei Aktionen in Parser-Regeln gelten andere Spielregeln!\nAktionen mit den Lexer-Regeln Aktionen für Lexer-Regeln sind Code-Blöcke in der Zielsprache, eingeschlossen in geschweifte Klammern. Die Code-Blöcke werden direkt in die generierten Lexer-Methoden kopiert.\nZusätzlich:\n@header: Package-Deklarationen und/oder Importe (wird vor der Klassendefinition eingefügt) @members: zusätzliche Attribute für die generierten Lexer- (und Parser-) Klassen. Mit @lexer::header bzw. @lexer::members werden diese Codeblöcke nur in den generierten Lexer eingefügt.\nAnmerkung: Lexer-Aktionen müssen am Ende der äußersten Alternative erscheinen. Wenn eine Lexer-Regel mehr als eine Alternative hat, müssen diese in runde Klammern eingeschlossen werden.\n(vgl. github.com/antlr/antlr4/blob/master/doc/grammars.md)\nHilfsregeln mit Fragmenten Fragmente sind Lexer-Regeln, die keine Token darstellen/erzeugen, aber bei der Formulierung von Regeln für mehr Übersicht oder Wiederverwendung sorgen. Fragmente werden mit dem Schlüsselwort fragment eingeleitet.\nBeispiel:\nNUM : DIGIT+ ; fragment DIGIT : [0-9] ;=\u003e Keine Token (für den Parser)!\nHier würde der Parser nur NUM \"bekommen\", aber keine DIGIT-Token.\nLexer Kommandos (Auswahl) TokenName : Alternative -\u003e command-name skip\nVerwerfe den aktuellen Text: WS : [ \\t]+ -\u003e skip ; (liefert kein Token)\nmore\nLese weiter ...\nDie Regel matcht zwar, aber es wird kein Token erzeugt. Die nächste matchende Regel wird den hier gematchten Text mit in ihr Token einbauen. Der Token-Typ ist der der zuletzt matchenden Regel.\nAnmerkung: Wird typischerweise zusammen mit Modes verwendet.\nmode (siehe unten)\nchannel (siehe unten)\n(vgl. github.com/antlr/antlr4/blob/master/doc/lexer-rules.md)\nModes und Insel-Grammatiken Umschalten zwischen verschiedenen Lexer-Modes: Wie verschiedene Sub-Lexer - einen für jeden Kontext.\n=\u003e Parsen von \"Insel-Grammatiken\" (beispielsweise XML).\nAnmerkung: mode-Spezifikation sind nur im Lexer-Teil der Grammatik erlaubt.\nAllgemeines Schema rules in default mode ... mode MODE_1; rules in MODE_1 ... mode MODE_N; rules in MODE_N ... Beispiel lexer grammar ModeLexer; LCOMMENT : '/*' -\u003e more, mode(CMNT) ; WS : [ \\t\\n]+ -\u003e skip ; mode CMNT; COMMENT : '*/' -\u003e mode(DEFAULT_MODE) ; CHAR : . -\u003e more ;Nach dem Matchen des Tokens wird mit mode(X) in den Mode X umgeschaltet. Der Lexer beachtet dann nur die Lexer-Regeln unter Mode X.\nMit pushMode(X) erreicht man das selbe Verhalten wie mit mode(X), allerdings wird vor dem Umschalten der aktuelle Mode auf einem Stack abgelegt. Mit popMode kann der oberste Mode vom Stack wieder herunter genommen werden und als aktueller Lexer-Mode gesetzt werden.\n(vgl. github.com/antlr/antlr4/blob/master/doc/lexer-rules.md)\nChannels Man kann die Token in verschiedene Kanäle (\"Channels\") schicken. Beispielsweise werden beim Parsen von Python-Programmen die White-Spaces evtl. noch benötigt.\nAnstatt diese mit skip komplett zu verwerfen, kann man sie in einen anderen Channel schicken, wo man sie im Parser bei Bedarf wieder abfragen kann. Der Token-Index bleibt dabei erhalten, auch wenn die Token in verschiedene Kanäle verteilt werden.\nAnmerkung: Channel-Spezifikationen sind nur im Lexer-Teil der Grammatik erlaubt.\nchannels { WHITESPACE, COMMENTS } BLOCK_COMMENT : '/*' .*? '*/' -\u003e channel(COMMENTS) ; LINE_COMMENT : '//' ~[\\n]* -\u003e channel(COMMENTS) ; WS : [ \\t\\n]+ -\u003e channel(WHITESPACE) ;Grammatiken importieren Mit import XZY; bindet man eine andere Grammatik XYZ ein. Dabei werden nur Regeln eingebunden, die bisher noch nicht definiert wurden.\nAus einer anderen Perspektive kann man diesen Mechanismus mit dem Überschreiben von Methoden in einer abgeleiteten Klasse vergleichen: Dann bekommt man beim Aufruf einer überschriebenen Methode ebenfalls nur die \"neueste\" Implementierung ...\nWenn mehrere verschachtelte Grammatiken eingebunden werden (wie im Beispiel), dann wird per Tiefensuche der Einbindungsbaum durchlaufen.\n(vgl. github.com/antlr/antlr4/blob/master/doc/grammars.md)\nWrap-Up Lexer mit ANTLR generieren: Lexer-Regeln werden mit Großbuchstaben geschrieben\nLängster Match gewinnt, Gleichstand: zuerst definierte Regel non greedy-Regeln: versuche so wenig Zeichen zu matchen wie möglich Aktionen beim Matchen Hilfsregeln mit \"Fragments\" Lexer Kommandos: skip, more, ... Modes für Insel-Grammatiken Channels als parallele Tokenstreams (Vorsortieren) Teilgrammatiken importieren ",
    "description": "",
    "tags": null,
    "title": "Lexer mit ANTLR generieren",
    "uri": "/frontend/lexing/antlr-lexing.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24) \u003e Frontend",
    "content": "Der Parser arbeitet mit dem Tokenstrom, der vom Lexer kommt. Mit Hilfe einer Grammatik wird geprüft, ob hier gültige Sätze im Sinne der Sprache/Grammatik gebildet wurden. Der Parser erzeugt dabei den Parse-Tree.\nCFG LL-Parser (Theorie) LL-Parser selbst implementiert LL-Parser: Fortgeschrittene Techniken Parser mit ANTLR generieren Grenze Lexer und Parser Syntaxanalyse: LR-Parser (Teil 1) Syntaxanalyse: LR-Parser (Teil 2) Error-Recovery ",
    "description": "",
    "tags": null,
    "title": "Parser",
    "uri": "/frontend/parsing.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24) \u003e Frontend \u003e Parser",
    "content": "Wiederholung Endliche Automaten, reguläre Ausdrücke, reguläre Grammatiken, reguläre Sprachen Wie sind DFAs und NFAs definiert? Was sind reguläre Ausdrücke? Was sind formale und reguläre Grammatiken? In welchem Zusammenhang stehen all diese Begriffe? Wie werden DFAs und reguläre Ausdrücke im Compilerbau eingesetzt? Motivation Wofür reichen reguläre Sprachen nicht? Für z. B. alle Sprachen, in deren Wörtern Zeichen über eine Konstante hinaus gezählt werden müssen. Diese Sprachen lassen sich oft mit Variablen im Exponenten beschreiben, die unendlich viele Werte annehmen können.\n$a^ib^{2*i}$ ist nicht regulär\n$a^ib^{2*i}$ für $0 \\leq i \\leq 3$ ist regulär\nWo finden sich die oben genannten VAriablen bei einem DFA wieder?\nWarum ist die erste Sprache oben nicht regulär, die zweite aber?\nThemen für heute PDAs: mächtiger als DFAs, NFAs kontextfreie Grammatiken und Sprachen: mächtiger als reguläre Grammatiken und Sprachen DPDAs und deterministisch kontextfreie Grammatiken: die Grundlage der Syntaxanalyse im Compilerbau Einordnung: Erweiterung der Automatenklasse DFA, um komplexere Sprachen als die regulären akzeptieren zu können Wir spendieren den DFAs einen möglichst einfachen, aber beliebig großen, Speicher, um zählen und matchen zu können. Wir suchen dabei konzeptionell die \"kleinstmögliche\" Erweiterung, die die akzeptierte Sprachklasse gegenüber DFAs vergrößert.\nDer konzeptionell einfachste Speicher ist ein Stack. Wir haben keinen wahlfreien Zugriff auf die gespeicherten Werte. Es soll eine deterministische und eine indeterministische Variante der neuen Automatenklasse geben. In diesem Zusammenhang wird der Stack auch Keller genannt. Kellerautomaten (Push-Down-Automata, PDAs) Def.: Ein Kellerautomat (PDA) $P = (Q,\\ \\Sigma,\\ \\Gamma,\\ \\delta,\\ q_0,\\ \\perp,\\ F)$ ist ein Septupel mit:\nDefinition eines PDAs\nEin PDA ist per Definition nichtdeterministisch und kann spontane Zustandsübergänge durchführen.\nWas kann man damit akzeptieren? Strukturen mit paarweise zu matchenden Symbolen.\nBei jedem Zustandsübergang wird ein Zeichen (oder $\\epsilon$) aus der Eingabe gelesen, ein Symbol von Keller genommen. Diese und das Eingabezeichen bestimmen den Folgezustand und eine Zeichenfolge, die auf den Stack gepackt wird. Dabei wird ein Symbol, das später mit einem Eingabesymbol zu matchen ist, auf den Stack gepackt.\nSoll das automatisch vom Stack genommene Symbol auf dem Stack bleiben, muss es wieder gepusht werden.\nBeispiel Ein PDA für $L=\\lbrace ww^{R}\\mid w\\in \\lbrace a,b\\rbrace^{\\ast}\\rbrace$:\nKonfigurationen von PDAs Def.: Eine Konfiguration (ID) eines PDAs 3-Tupel $(q, w, \\gamma)$ mit\n$q$ ist ein Zustand $w$ ist der verbleibende Input, $w\\in\\Sigma^{\\ast}$ $\\gamma$ ist der Kellerinhalt $\\gamma\\in \\Gamma^{\\ast}$ eines PDAs zu einem gegebenen Zeitpunkt.\nDie Übergangsrelation eines PDAs Def.: Die Relation $\\vdash$ definiert Übergänge von einer Konfiguration zu einer anderen:\nSei $(p, \\alpha) \\in \\delta(q, a, X)$, dann gilt $\\forall w\\ \\epsilon \\ \\Sigma^{\\ast}$ und $\\beta \\in \\Gamma^{\\ast}$:\n$(q, aw, X\\beta)\\vdash(p, w, \\alpha\\beta)$.\nDef.: Wir definieren mit $\\overset{\\ast}{\\vdash}$ 0 oder endlich viele Schritte des PDAs induktiv wie folgt:\nBasis: $I\\overset{\\ast}{\\vdash} I$ für eine ID $I$. Induktion: $I\\overset{\\ast}{\\vdash}J$, wenn $\\exists$ ID $K$ mit $I\\vdash K$ und $K \\overset{\\ast}{\\vdash}J$. Eigenschaften der Konfigurationsübergänge Satz: Sei $P=(Q, \\Sigma, \\Gamma, \\delta, q_{0}, \\perp, F)$ ein PDA und $(q, x,\\alpha)\\overset{\\ast}{\\vdash} (p, y, \\beta)$. Dann gilt für beliebige Strings $w\\in\\Sigma^{\\ast}$, $\\gamma$ in $\\Gamma^{\\ast}$:\n$(q, xw, \\alpha \\gamma) \\overset{\\ast}{\\vdash}(p, yw, \\beta\\gamma)$ Satz: Sei $P = (Q, \\Sigma, \\Gamma, \\gamma, q_0, \\perp, F)$ ein PDA und $(q,xw,\\alpha) \\overset{\\ast}{\\vdash} (p,y w, \\beta)$.\nDann gilt: $(q, x, a) \\overset{\\ast}{\\vdash} (p, y, \\beta)$\nAkzeptierte Sprachen Def.: Sei $P=(Q, \\Sigma, \\Gamma, \\delta, q_0, \\perp, F)$ ein PDA. Dann ist die über einen Endzustand akzeptierte Sprache $L(P) = \\lbrace w \\mid (q_0, w, \\perp) \\overset{\\ast}{\\vdash} (q, \\epsilon, \\alpha)\\rbrace$ für einen Zustand $q \\in F, \\alpha \\in \\Gamma^{\\ast}$.\nDef.: Für einen PDA $P=(Q, \\Sigma, \\Gamma, \\delta, q_{0}, \\perp, F)$ definieren wir die über den leeren Keller akzeptierte Sprache $N(P) = \\lbrace (w \\mid (q_0, w, \\perp) \\overset{\\ast}{\\vdash} (q, \\epsilon, \\epsilon)\\rbrace$.\nAkzeptanzäquivalenzen Satz: Wenn $L = N(P_N)$ für einen PDA $P_N$, dann gibt es einen PDA $P_L$ mit $L = L(P_L)$.\nSatz: Für einen PDA $P$ mit $\\epsilon$-Transitionen existiert ein PDA $Q$ ohne $\\epsilon$-Transitionen mit $L(P) = N(P) = L(Q) = N(Q)$.\nDie Transitionsfunktion $\\delta$ ist dann von der Form $\\delta: Q \\times \\Sigma \\times \\Gamma \\to2^{Q \\times \\Gamma^{\\ast}}$.\nDeterministische PDAs Def. Ein PDA $P = (Q, \\Sigma, \\Gamma, \\delta, q_0, \\perp, F)$ ist deterministisch $: \\Leftrightarrow$\n$\\delta(q, a, X)$ hat höchstens ein Element für jedes $q \\in Q, a \\in\\Sigma$ oder $(a = \\epsilon$ und $X \\in \\Gamma)$. Wenn $\\delta (q, a, x)$ nicht leer ist für ein $a \\in \\Sigma$, dann muss $\\delta (q, \\epsilon, x)$ leer sein. Deterministische PDAs werden auch DPDAs genannt.\nDer kleine Unterschied Satz: Die von DPDAs akzeptierten Sprachen sind eine echte Teilmenge der von PDAs akzeptierten Sprachen.\nDie Sprachen, die von regex beschrieben werden, sind eine echte Teilmenge der von DPDAs akzeptierten Sprachen.\nKontextfreie Grammatiken und Sprachen Kontextfreie Grammatiken Def. Eine kontextfreie (cf-) Grammatik ist ein 4-Tupel $G = (N, T, P, S)$ mit N, T, S wie in (formalen) Grammatiken und P ist eine endliche Menge von Produktionen der Form:\n$X \\rightarrow Y$ mit $X \\in N, Y \\in {(N \\cup T)}^{\\ast}$.\n$\\Rightarrow, \\overset{\\ast}{\\Rightarrow}$ sind definiert wie bei regulären Sprachen. Bei cf-Grammatiken nennt man die Ableitungsbäume oft Parse trees.\nBeispiel $S \\rightarrow a \\mid S\\ +\\ S\\ |\\ S \\ast S$ Ableitungsbäume für $a + a \\ast a$:\nHier entsteht ein Tafelbild.\nMehrdeutige Grammatiken Def.: Gibt es in einer von einer kontextfreien Grammatik erzeugten Sprache ein Wort, für das mehr als ein Ableitungsbaum existiert, so heißt diese Grammatik mehrdeutig. Anderenfalls heißt sie eindeutig.\nSatz: Es gibt kontextfreie Sprachen, für die keine eindeutige Grammatik existiert.\nKontextfreie Grammatiken und PDAs Satz: Die kontextfreien Sprachen und die Sprachen, die von PDAs akzeptiert werden, sind dieselbe Sprachklasse.\nSatz: Sei $L = N(P)$ für einen DPDA P, dann hat L eine eindeutige Grammatik.\nDef.: Die Klasse der Sprachen, die von einem DPDA akzeptiert werden, heißt Klasse der deterministisch kontextfreien (oder LR(k)-) Sprachen.\nDas Pumping Lemma für kontextfreie Sprachen Wenn wir beweisen müssen, dass eine Sprache nicht cf ist, hilft das Pumping Lemma für cf-Sprachen:\nSatz: Sei L eine kontextfreie Sprache\n$\\Rightarrow \\exists$ eine Konstante $p \\in \\mathbb{N}$:\n$\\underset{\\underset{|z| \\geq p} {z \\in L}}\\forall \\exists$ $u, v, w, x, y \\in \\Sigma ^{\\ast}$ mit $z = uvwxy$ und\n$\\mid vwx\\mid \\leq p$ $vx \\neq \\epsilon$ $\\forall i \\geq 0 : uv^i wx^i y \\in L$ Abschlusseigenschaften von kontextfreien Sprachen Satz: Die kontextfreien Sprachen sind abgeschlossen unter:\nVereinigung Konkatenation Kleene-Hüllen $L^{\\ast}$ und $L^+$ Satz: Wenn L kontextfrei ist, dann ist $L^R$ kontextfrei.\nEntscheidbarkeit von kontextfreien Grammatiken und Sprachen Satz: Es ist entscheidbar für eine kontextfreie Grammatik G,\nob $L(G) = \\emptyset$ welche Symbole nach $\\epsilon$ abgeleitet werden können welche Symbole erreichbar sind ob $w \\in L(G)$ für ein gegebenes $w \\in {\\Sigma}^{\\ast}$ Satz: Es ist nicht entscheidbar,\nob eine gegebene kontextfreie Grammatik eindeutig ist ob der Durchschnitt zweier kontextfreier Sprachen leer ist ob zwei kontextfreie Sprachen identisch sind ob eine gegebene kontextfreie Sprache gleich $\\Sigma^{\\ast}$ ist Abschlusseigenschaften deterministisch kontextfreier Sprachen Satz: Deterministisch kontextfreie Sprachen sind abgeschlossen unter\nDurchschnitt mit regulären Sprachen Komplement Sie sind nicht abgeschlossen unter\nUmkehrung Vereinigung Konkatenation Wrap-Up Das sollen Sie mitnehmen Die Struktur von gängigen Programmiersprachen lässt sich nicht mit regulären Ausdrücken beschreiben und damit nicht mit DFAs akzeptieren. Das Automatenmodell der DFAs wird um einen endlosen Stack erweitert, das ergibt PDAs. Kontextfreie Grammatiken (CFGs) erweitern die regulären Grammatiken. Deterministisch parsebare Sprachen haben eine eindeutige kontextfreie Grammatik. Es ist nicht entscheidbar, ob eine gegebene kontextfreie Grammatik eindeutig ist. ",
    "description": "",
    "tags": null,
    "title": "CFG",
    "uri": "/frontend/parsing/cfg.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24) \u003e Frontend \u003e Parser",
    "content": "Wiederholung PDAs und kontextfreie Grammatiken Warum reichen uns DFAs nicht zum Matchen von Eingabezeichen? Wie könnnen wir sie minimal erweitern? Sind PDAs deterministisch? Wie sind kontextfreie Grammatiken definiert? Sind kontextfreie Grammatiken eindeutig? Motivation Was brauchen wir für die Syntaxanalyse von Programmen? einen Grammatiktypen, aus dem sich manuell oder automatisiert ein Programm zur deterministischen Syntaxanalyse erstellen lässt einen Algorithmus zum sog. Parsen von Programmen mit Hilfe einer solchen Grammatik Themen für heute Arten der Syntaxanlyse mehrdeutige Sprachen Top-down-Analyse LL(k)-Grammtiken Syntaxanalyse Syntax Wir verstehen unter Syntax eine Menge von Regeln, die die Struktur von Daten (z. B. Programmen) bestimmen.\nSyntaxanalyse ist die Bestimmung, ob Eingabedaten einer vorgegebenen Syntax entsprechen.\nDiese vorgegebene Syntax wird im Compilerbau mit einer Grammatik beschrieben.\nZiele der Syntaxanalyse aussagekräftige Fehlermeldungen, wenn ein Eingabeprogramm syntaktisch nicht korrekt ist evtl. Fehlerkorrektur Bestimmung der syntaktischen Struktur eines Programms Erstellung des AST (abstrakter Syntaxbaum): Der Parse Tree ohne Symbole, die nach der Syntaxanalyse inhaltlich irrelevant sind (z. B. Semikolons, manche Schlüsselwörter) die Symboltablelle(n) mit Informationen bzgl. Bezeichner (Variable, Funktionen und Methoden, Klassen, benutzerdefinierte Typen, Parameter, ...), aber auch die Gültigkeitsbereiche. Arten der Syntaxanalyse Die Syntax bezieht sich auf die Struktur der zu analysierenden Eingabe, z. B. einem Computerprogramm in einer Hochsprache. Diese Struktur wird mit formalen Grammatiken beschrieben. Einsetzbar sind Grammatiken, die deterministisch kontextfreie Sprachen erzeugen.\nTop-Down-Analyse: Aufbau des Parse trees von oben Parsen durch rekursiven Abstieg LL-Parsing Bottom-Up-Analyse: LR-Parsing Mehrdeutigkeiten Wir können nur mit eindeutigen Grammatiken arbeiten, aber:\nDef.: Eine formale Sprache L heißt inhärent mehrdeutige Sprache, wenn jede formale Grammatik G mit $L(G) = L$ mehrdeutig ist.\nDas heißt, solche Grammatiken existieren.\n$\\Rightarrow$ Es gibt keinen generellen Algorithmus, um Grammatiken eindeutig zu machen.\nBevor wir richtig anfangen... Def.: Ein Nichtterminal A einer kontextfreien Grammatik G heißt unerreichbar, falls es kein $a,b \\in {(N \\cup T)}^{\\ast}$ gibt mit $S \\overset{\\ast}{\\Rightarrow} aAb$. Ein Nichtterminal A einer Grammatik G heißt nutzlos, wenn es kein Wort $w \\in T^{\\ast}$ gibt mit $A \\overset{\\ast}{\\Rightarrow} w$.\nDef.: Eine kontextfreie Grammatik $G=(N, T, P, S)$ heißt reduziert, wenn es keine nutzlosen oder unerreichbaren Nichtterminale in N gibt.\nBevor mit einer Grammatik weitergearbeitet wird, müssen erst alle nutzlosen und dann alle unerreichbaren Symbole eliminiert werden. Wir betrachten ab jetzt nur reduzierte Grammatiken.\nTop-Down-Analyse Wie würden Sie manuell parsen? Hier entsteht ein Tafelbild.\nAlgorithmus: Rekursiver Abstieg Hier ist ein einfacher Algorithmus, der (indeterministisch) einen Ableitungsbaum vom Nonterminal X von oben nach unten aufbaut:\nEingabe: Ein Nichtterminal $X$ und das nächste zu verarbeitende Eingabezeichen $a$.\nRecursive Descent-Algorithmus\nGrenzen des Algorithmus Was ist mit\n$X \\rightarrow a \\alpha \\mid b \\beta$ $X \\rightarrow B\\alpha \\mid C \\beta$ $X \\rightarrow B \\alpha \\mid B \\beta$ $X \\rightarrow B \\alpha \\mid C \\beta$ und $C\\rightarrow B$ $X \\rightarrow X \\beta$ $X \\rightarrow B \\alpha$ und $B \\rightarrow X \\beta$ $X, B, C, D \\in N^{\\ast}; a, b, c, d \\in T^{\\ast}; \\beta$, $\\alpha, \\beta \\in (N \\cup T)^{\\ast}$\nLinksfaktorisierung $X \\rightarrow BC\\ \\vert \\ BD$ Hier entsteht ein Tafelbild.\nAlgorithmus: Linksfaktorisierung Eingabe: Eine Grammatik G = (N, T, P, S)\nAusgabe: Eine äquivalente links-faktorisierte Grammatik $G'$\nAlgorithmus zur Linksfaktorisierung\nLinksrekursion Def.: Eine Grammatik $G=(N, T, P, S)$ heißt linksrekursiv, wenn sie ein Nichtterminal X hat, für das es eine Ableitung $X \\overset{+}{\\Rightarrow} X\\ \\alpha$ für ein $\\alpha \\in (N \\cup T)^{\\ast}$ gibt.\nLinksrekursion gibt es\ndirekt: $X \\rightarrow X \\alpha$\nund\nindirekt: $X \\rightarrow \\ldots \\rightarrow \\ldots \\rightarrow X \\alpha$\nAlgorithmus: Entfernung von direkter Linksrekursion Eingabe: Eine Grammatik G = (N, T, P, S)\nAusgabe: Eine äquivalente Grammatik $G'$ ohne direkte Linksrekursion\nAlgorithmus zur Entfernung direkter Linksrekursion\nAlgorithmus: Entfernung von indirekter Linksrekursion Eingabe: Eine Grammatik G = (N, T, P, S) mit $N= \\lbrace X_1, X_2, \\ldots X_n\\rbrace$ ohne $\\epsilon$-Regeln oder Zyklen der Form $X_1 \\rightarrow X_2, X_2 \\rightarrow X_3, \\ldots X_{m-1} \\rightarrow X_m, X_m \\rightarrow X_1$\nAusgabe: Eine äquivalente Grammatik $G'$ ohne Linksrekursion\nAlgorithmus zur Entfernung indirekter Linksrekursion\nArbeiten mit generierten Parsern: LL(k)-Grammatiken First-Mengen $S \\rightarrow A \\ \\vert \\ B \\ \\vert \\ C$ Welche Produktion nehmen?\nWir brauchen die \"terminalen k-Anfänge\" von Ableitungen von Nichtterminalen, um eindeutig die nächste zu benutzende Produktion festzulegen. $k$ ist dabei die Anzahl der Vorschautoken.\nDef.: Wir definieren $First$ - Mengen einer Grammatik wie folgt:\n$a \\in T^\\ast, |a| \\leq k: {First}_k (a) = \\lbrace a\\rbrace$ $a \\in T^\\ast, |a| \u003e k: {First}_k (a) = \\lbrace v \\in T^\\ast \\mid a = vw, |v| = k\\rbrace$ $\\alpha \\in (N \\cup T)^\\ast \\backslash T^\\ast: {First}_k (\\alpha) = \\lbrace v \\in T^\\ast \\mid \\alpha \\overset{\\ast}{\\Rightarrow} w,\\text{mit}\\ w \\in T^\\ast, First_k(w) = \\lbrace v \\rbrace \\rbrace$ Linksableitungen Def.: Bei einer kontextfreien Grammatik $G$ ist die Linksableitung von $\\alpha \\in (N \\cup T)^{\\ast}$ die Ableitung, die man erhält, wenn in jedem Schritt das am weitesten links stehende Nichtterminal in $\\alpha$ abgeleitet wird.\nMan schreibt $\\alpha \\overset{\\ast}{\\Rightarrow}_l \\beta.$\nFollow-Mengen Manchmal müssen wir wissen, welche terminalen Zeichen hinter einem Nichtterminal stehen können.\nDef. Wir definieren Follow - Mengen einer Grammatik wie folgt:\n$\\forall \\beta \\in (N \\cup T)^*:$ $$Follow_k(\\beta) = \\lbrace w \\in T^\\ast \\mid \\exists \\alpha, \\gamma \\in (N \\cup T)^\\ast\\ \\text{ mit }\\ S \\overset{\\ast}{\\Rightarrow}_l \\alpha \\beta \\gamma\\ \\text{ und }\\ w \\in First_k(\\gamma) \\rbrace$$ LL(k)-Grammatiken Def.: Eine kontextfreie Grammatik G = (N, T, P, S) ist genau dann eine LL(k)-Grammatik, wenn für alle Linksableitungen der Form:\n$S \\overset{\\ast}{\\Rightarrow}_l\\ wA \\gamma\\ {\\Rightarrow}_l\\ w\\alpha\\gamma \\overset{\\ast}{\\Rightarrow}_l wx$ und\n$S \\overset{\\ast}{\\Rightarrow}_l wA \\gamma {\\Rightarrow}_l w\\beta\\gamma \\overset{\\ast}{\\Rightarrow}_l wy$ mit $(w, x, y \\in T^\\ast, \\alpha, \\beta, \\gamma \\in (N \\cup T)^\\ast, A \\in N)$ und $First_k(x) = First_k(y)$ gilt:\n$\\alpha = \\beta$ LL(k)-Grammatiken Das hilft manchmal:\nFür $k = 1$: G ist $LL(1): \\forall A \\rightarrow \\alpha, A \\rightarrow \\beta \\in P, \\alpha \\neq \\beta$ gilt:\n$\\lnot \\exists a \\in T: \\alpha \\overset{\\ast}{\\Rightarrow}_l a\\alpha_1$ und $\\beta \\overset{\\ast}{\\Rightarrow}_l a\\beta_1$ $((\\alpha \\overset{\\ast}{\\Rightarrow}_l \\epsilon) \\Rightarrow (\\lnot (\\beta \\overset{\\ast}{\\Rightarrow}_l \\epsilon)))$ und $((\\beta \\overset{\\ast}{\\Rightarrow}_l \\epsilon) \\Rightarrow (\\lnot (\\alpha\\overset{\\ast}{\\Rightarrow}_l \\epsilon)))$ $((\\beta \\overset{\\ast}{\\Rightarrow}_l \\epsilon)$ und $(\\alpha \\overset{\\ast}{\\Rightarrow}_l a\\alpha_1)) \\Rightarrow a \\notin Follow(A)$ $((\\alpha \\overset{\\ast}{\\Rightarrow}_l \\epsilon)$ und $(\\beta \\overset{\\ast}{\\Rightarrow}_l a\\beta_1)) \\Rightarrow a \\notin Follow(A)$ Die ersten beiden Zeilen bedeuten:\n$\\alpha$ und $\\beta$ können nicht beide $\\epsilon$ ableiten, $First_1(\\alpha) \\cap First_1(\\beta) = \\emptyset$\nDie dritte und vierte Zeile bedeuten:\n$(\\epsilon \\in First_1(\\beta)) \\Rightarrow (First_1(\\alpha) \\cap Follow_1(A) = \\emptyset)$ $(\\epsilon \\in First_1(\\alpha)) \\Rightarrow (First_1(\\beta) \\cap Follow_1(A) = \\emptyset)$ LL(1)-Grammatiken Hier entsteht ein Tafelbild.\nLL(k)-Sprachen Die von LL(k)-Grammatiken erzeugten Sprachen sind eine echte Teilmenge der deterministisch parsbaren Sprachen.\nDie von LL(k)-Grammatiken erzeugten Sprachen sind eine echte Teilmenge der von LL(k+1)-Grammatiken erzeugten Sprachen.\nFür eine kontextfreie Grammatik G ist nicht entscheidbar, ob es eine LL(1) - Grammatik G' gibt mit $L(G) = L(G')$.\nIn der Praxis reichen $LL(1)$ - Grammatiken oft. Hier gibt es effiziente Parsergeneratoren, deren Eingabe eine LL(k)- (meist LL(1)-) Grammatik ist, und die als Ausgabe den Quellcode eines (effizienten) tabellengesteuerten Parsers generieren.\nAlgorithmus: Konstruktion einer LL-Parsertabelle Eingabe: Eine Grammatik G = (N, T, P, S)\nAusgabe: Eine Parsertabelle P\nAlgorithmus zur Generierung einer LL-Parsertabelle\nHier ist $\\perp$ das Endezeichen des Inputs. Statt $First_1(\\alpha)$ und $Follow_1(\\alpha)$ wird oft nur $First(\\alpha)$ und $Follow(\\alpha)$ geschrieben.\nLL-Parsertabellen Hier entsteht ein Tafelbild.\nLL-Parsertabellen Rekursive Programmierung bedeutet, dass das Laufzeitsystem einen Stack benutzt (bei einem Recursive-Descent-Parser, aber auch bei der Parsertabelle). Diesen Stack kann man auch \"selbst programmieren\", d. h. einen PDA implementieren. Dabei wird ebenfalls die oben genannte Tabelle zur Bestimmung der nächsten anzuwendenden Produktion benutzt. Der Stack enthält die zu erwartenden Eingabezeichen, wenn immer eine Linksableitung gebildet wird. Diese Zeichen im Stack werden mit dem Input gematcht.\nAlgorithmus: Tabellengesteuertes LL-Parsen mit einem PDA Eingabe: Eine Grammatik G = (N, T, P, S), eine Parsertabelle P mit $w\\perp$ als initialem Kellerinhalt\nAusgabe: Wenn $w \\in L(G)$, eine Linksableitung von $w$, Fehler sonst\nAlgorithmus zum tabellengesteuerten LL-Parsen\nDer Eingabestring sei $w\\perp$, der initiale Kellerinhalt sei $\\perp$.\nWrap-Up Syntaxanalyse wird mit deterministisch kontextfreien Grammatiken durchgeführt. Eine Teilmenge der dazu gehörigen Sprachen lässt sich top-down parsen. Ein einfacher Recursive-Descent-Parser arbeitet mit Backtracking. Ein effizienter LL(k)-Parser realisiert einen DPDA und kann automatisch aus einer LL(k)-Grammatik generiert werden. Der Parser liefert in der Regel einen abstrakten Syntaxbaum. ",
    "description": "",
    "tags": null,
    "title": "LL-Parser (Theorie)",
    "uri": "/frontend/parsing/ll-parser-theory.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24) \u003e Frontend \u003e Parser",
    "content": "Erinnerung Lexer: Zeichenstrom =\u003e Tokenstrom def nextToken(): while (peek != EOF): # globale Variable, über consume() switch (peek): case ' ': case '\\t': case '\\n': WS(); continue case '[': consume(); return Token(LBRACK, '[') ... default: raise Error(\"invalid character: \"+peek) return Token(EOF_Type, \"\u003cEOF\u003e\") def match(c): # Lookahead: Ein Zeichen consume() if (peek == c): return True else: rollBack(); return False def consume(): peek = buffer[start] start = (start+1) mod 2n if (start mod n == 0): fill(buffer[start:start+n-1]) end = (start+n) mod 2nErinnerung: Der Lexer arbeitet direkt auf dem Zeichenstrom und versucht über längste Matches daraus einen Tokenstrom zu erzeugen. Dabei wird immer das nächste Zeichen angeschaut (Funktion match) und mit consume das aktuelle Zeichen \"verbraucht\" und das nächste Zeichen geladen. Hier kann man über die Doppel-Puffer-Strategie das Einlesen einzelner Zeichen aus einer Datei vermeiden und immer blockweise in den Puffer einlesen.\nGrundidee LL-Parser Grammatik:\nr : X s ;LL-Implementierung:\ndef r(): match(X) s() Für jede Regel in der Grammatik wird eine Methode/Funktion mit dem selben Namen definiert Referenzen auf ein Token T werden durch den Aufruf der Methode match(T) aufgelöst match(T) \"konsumiert\" das aktuelle Token, falls dieses mit T übereinstimmt Anderenfalls löst match() eine Exception aus Referenzen auf Regeln s werden durch Methodenaufrufe s() aufgelöst Erinnerung: In ANTLR werden Parser-Regeln (non-Terminale) mit einem kleinen und Lexer-Regeln (Terminale/Token) mit einem großen Anfangsbuchstaben geschrieben.\nAlternative Subregeln a | b | c kann zu einem switch-Konstrukt aufgelöst werden:\nswitch (lookahead): case predicting_a: a(); break; case predicting_b: b(); break; ... default: raise Error()Dabei ist lookahead eine globale Variable, die das zu betrachtende Token enthält (vergleichbar mit peek beim Lexer).\nDas Prädikat predicting_a soll andeuten, dass man mit dem aktuellen Token eine Vorhersage für die Regel a versucht (hier kommen die FIRST- und FOLLOW-Mengen ins Spiel ...). Wenn das der Fall ist, springt man entsprechend in die Funktion bzw. Methode a().\nOptionale Subregeln: Eins oder keins (T)? wird zu einem if ohne else-Teil:\nif lookahead.predicting_T: match(T)Optionale Subregeln: Mindestens eins (T)+ wird zu einer do-while-Schleife (mind. ein Durchlauf):\nwhile True: match(T) if not lookahead.predicting_T: breakLL(1)-Parser Beispiel: Parsen von Listen, also Sequenzen wie [1,2,3,4]:\nlist : '[' elements ']' ; elements : INT (',' INT)* ; INT : ('0'..'9')+ ;Formal berechnet man die Lookahead-Mengen mit FIRST und FOLLOW, um eine Entscheidung für die nächste Regel zu treffen. Praktisch betrachtet kann man sich fragen, welche(s) Token eine Phrase in der aktuellen Alternative starten können.\nFür LL(1)-Parser betrachtet man immer das aktuelle Token (genau EIN Lookahead-Token), um eine Entscheidung zu treffen.\ndef list(): match(LBRACK); elements(); match(RBRACK); def elements(): match(INT) while lookahead == COMMA: # globale Variable, über consume() match(COMMA); match(INT)Detail: match() und consume() def match(x): if lookahead == x: consume() else: raise Exception() def consume(): lookahead = lexer.nextToken()Quelle: Eigener Code basierend auf einer Idee nach [Parr2010, p. 43]\nDabei setzt man in der Klasse Parser zwei Attribute voraus:\nclass Parser: Lexer lexer Token lookaheadStarten würde man den Parser nach dem Erzeugen einer Instanz (dabei wird ein Lexer mit durchgereicht) über den Aufruf der Start-Regel, also beispielsweise parser.list().\nAnmerkung: Mit dem generierten Parse-Tree bzw. AST beschäftigen wir uns später (=\u003e AST-basierte Interpreter).\nVorrangregeln 1+2*3 == 1+(2*3) != (1+2)*3 Die Eingabe 1+2*3 muss als 1+(2*3) interpretiert werden, da * Vorrang vor + hat.\nDies formuliert man üblicherweise in der Grammatik:\nexpr : expr '+' term | term ; term : term '*' INT | INT ;ANTLR nutzt die Strategie des \"precedence climbing\" und löst nach der Reihenfolge der Alternativen in einer Regel auf. Entsprechend könnte man die obige Grammatik unter Beibehaltung der Vorrangregeln so in ANTLR (v4) formulieren:\nexpr : expr '*' expr | expr '+' expr | INT ;Linksrekursion Normalerweise sind linksrekursive Grammatiken nicht mit einem LL-Parser behandelbar. Man muss die Linksrekursion manuell auflösen und die Grammatik umschreiben.\nBeispiel:\nexpr : expr '*' expr | expr '+' expr | INT ;Diese linksrekursive Grammatik könnte man (unter Beachtung der Vorrangregeln) etwa so umformulieren:\nexpr : addExpr ; addExpr : multExpr ('+' multExpr)* ; multExpr : INT ('*' INT)* ;ANTLR (v4) kann Grammatiken mit direkter Linksrekursion auflösen. Für frühere Versionen von ANTLR muss man die Rekursion manuell beseitigen.\nVergleiche \"ALL(*)\" bzw. \"Adaptive LL(*)\".\nAchtung: Mit indirekter Linksrekursion kann ANTLR (v4) nicht umgehen:\nexpr : expM | ... ; expM : expr '*' expr ;=\u003e Nicht erlaubt!\nAssoziativität Die Eingabe 2^3^4 sollte als 2^(3^4) geparst werden. Analog sollte a=b=c in C als a=(b=c) verstanden werden.\nPer Default werden Operatoren wie + in ANTLR links-assoziativ behandelt, d.h. die Eingabe 1+2+3 wird als (1+2)+3 gelesen. Für rechts-assoziative Operatoren muss man ANTLR dies in der Grammatik mitteilen:\nexpr : expr '^'\u003cassoc=right\u003e expr | INT ;Anmerkung: Laut Doku gilt die Angabe \u003cassoc=right\u003e immer für die jeweilige Alternative und muss seit Version 4.2 an den Alternativen-Operator | geschrieben werden. In der Übergangsphase sei die Annotation an Tokenreferenzen noch zulässig, würde aber ignoriert?!\nLL(k)-Parser expr : ID '++' // x++ | ID '--' // x-- ;Die obige Regel ist für einen LL(1)-Parser nicht deterministisch behandelbar, da die Alternativen mit dem selben Token beginnen (die Lookahead-Mengen überlappen sich). Entweder benötigt man zwei Lookahead-Tokens, also einen LL(2)-Parser, oder man muss die Regel in eine äquivalente LL(1)-Grammatik umschreiben:\nexpr : ID ('++' | '--') ; // x++ oder x--LL(k)-Parser: Implementierung mit Ringpuffer Für einen größeren Lookahead benötigt man einen Puffer für die Token. Für einen Lookahead von $k$ Token (also einen LL(k)-Parser) würde man einen Puffer mit $k$ Plätzen anlegen und diesen wie einen Ringpuffer benutzen. Dabei ist start der Index des aktuellen Lookahead-Tokens. Über die neue Funktion lookahead(1) ist dieses aktuelle Lookahead-Token abrufbar.\nclass Parser: Lexer lexer k = 3 # Lookahead: 3 Token =\u003e LL(3) start = 0 # aktuelle Tokenposition im Ringpuffer Token[k] lookahead # Ringpuffer mit k Plätzen (vorbefüllt via Konstruktor)def match(x): if lookahead(1) == x: consume() else: raise Exception() def consume(): lookahead[start] = lexer.nextToken() start = (start+1) % k def lookahead(i): return lookahead[(start+i-1) % k] # i==1: startQuelle: Eigener Code basierend auf einer Idee nach [Parr2010, p. 47]\nWrap-Up LL(1) und LL(k) mit festem Lookahead Implementierung von Vorrang- und Assoziativitätsregeln Beachtung und Auflösung von Linksrekursion ",
    "description": "",
    "tags": null,
    "title": "LL-Parser selbst implementiert",
    "uri": "/frontend/parsing/ll-parser-impl.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24) \u003e Frontend \u003e Parser",
    "content": "LL-Parser mit Backtracking Problem: Manchmal kennt man den nötigen Lookahead nicht vorher. Beispiel:\nwuppie(); // Vorwärtsdeklaration wuppie() { ...} // Definition Entsprechend sähe die Grammatik aus:\nfunc : fdef | fdecl ; fdef : head '{' body '}' ; fdecl: head ';' ; head : ... ;Hier müsste man erst den gesamten Funktionskopf parsen, bevor man entscheiden kann, ob es sich um eine Deklaration oder eine Definition handelt. Unglücklicherweise gibt es keine Längenbeschränkung bei den Funktionsnamen ...\nMit Hilfe von Backtracking kann man zunächst spekulativ matchen und beim Auftreten eines Fehlers die Spekulation rückgängig machen:\ndef func(): if speculate(fdef): fdef() # Spekuliere auf \"fdef\" elif speculate(fdecl): fdecl() # Spekuliere auf \"fdecl\" else: raise Exception()Die erste Alternative, die passt, gewinnt. Über die Reihenfolge der Spekulationen kann man entsprechend Vorrangregeln implementieren.\nAnmerkung Man könnte die obige Grammatik umformen ...\nfunc : head ('{' body '}' | ';') ; head : ... ;...und bräuchte dann kein spekulatives Parsen mit Backtracking.\nDa wir aber das Parsen mit Backtracking betrachten wollen, blenden wir diese Möglichkeit jetzt einfach aus ;)\nDetails: Spekulatives Matchen def speculate(fn): success = True mark() # markiere aktuelle Position try: fn() # probiere Regel fn() catch: success = False clear() # Rollback return successQuelle: Eigener Code basierend auf einer Idee nach [Parr2010, p. 60]\nDer Funktion speculate wird die zu testende Regel (Funktion) als Parameter übergeben, im obigen Beispiel wären dies fdef bzw. fdecl.\nVor dem spekulativen Matchen muss die aktuelle Position im Tokenstrom markiert werden. Falls der Versuch, die Deklaration zu matchen nicht funktioniert, wird der Regel-Aufruf eine Exception werfen, entsprechend wird die Hilfsvariable gesetzt. Anschließend muss noch mit clear() das aktuelle Token wieder hergestellt werden (wir sind ja nur im Spekulationsmodus, d.h. selbst im Erfolgsfall wird ja die Regel noch \"richtig\" aufgerufen).\nSpekulatives Matchen: Hilfsmethoden I/II class Parser: Lexer lexer markers = [] # Integer-Stack: speichere Tokenpositionen lookahead = [] # Puffer (1 Token vorbefüllt via Konstruktor) start = 0 # aktuelle Tokenposition im lookahead-Puffer def mark(): markers.push(start) def clear(): start = markers.pop()Quelle: Eigener Code basierend auf einer Idee nach [Parr2010, pp. 61/62]\nSpekulatives Matchen: Hilfsmethoden II/II def consume(): ++start if start == lookahead.count() and markers.isEmpty(): start = 0; lookahead.clear() sync(1) def lookahead(i): sync(i) return lookahead.get(start+i-1) def sync(i): n = start + i - lookahead.count() while (n \u003e 0): lookahead.add(lexer.nextToken()); --nQuelle: Eigener Code basierend auf einer Idee nach [Parr2010, pp. 61/62]\nconsume holt wie immer das nächste Token, hier indem der Index start weiter gesetzt wird und ein weiteres Token über sync in den Puffer geladen wird. Falls wir nicht am Spekulieren sind und das Ende des Puffers erreicht haben, nutzen wir die Gelegenheit und setzen den Puffer zurück. (Dies geht nicht, wenn wir spekulieren -- hier müssen wir ja ggf. ein Rollback vornehmen und benötigen also den aktuellen Puffer dann noch.)\nDie Funktion sync stellt sicher, dass ab der Position start noch i unverbrauchte Token im Puffer sind.\nHinweis Die Methode count liefert die Anzahl der aktuell gespeicherten Elemente in lookahead zurück (nicht die Gesamtzahl der Plätze in der Liste -- diese kann größer sein). Mit der Methode add wird ein Element hinten an die Liste angefügt, dabei wird das Token auf den nächsten Index-Platz (count) geschrieben und ggf. die Liste ggf. automatisch um weitere Speicherplätze ergänzt. Über clear werden die Elemente in der Liste gelöscht, aber der Speicherplatz erhalten (d.h. count() liefert den Wert 0, aber ein add müsste nicht erst die Liste mit weiteren Plätzen erweitern, sondern könnte direkt an Index 0 das Token schreiben).\nBacktracking führt zu Problemen Backtracking kann sehr langsam sein (Ausprobieren vieler Alternativen) Der spekulative Match muss ggf. rückgängig gemacht werden Man muss bereits gematchte Strukturen erneut matchen (=\u003e Abhilfe: Packrat-Parsing) Verbesserung Backtracking: Packrat Parser (Memoizing) Bei der Eingabe wuppie(); wird zunächst spekulativ die erste Alternative fdef untersucht und ein head gematcht. Da die Alternative nicht komplett passt (es kommt ein \";\" statt einem \"{\"), muss die Spekulation rückgängig gemacht werden und die zweite Alternative fdecl untersucht werden. Dabei muss man den selben Input erneut auf head matchen! (Und wenn die Spekulation (irgendwann) erfolgreich war, muss noch einmal ein head gematcht werden ...)\nIdee: Wenn head sich merken würde, ob damit ein bestimmter Teil des Tokenstroms bereits behandelt wurde (erfolgreich oder nicht), könnte man das Spekulieren effizienter gestalten. Jede Regel muss also durch eine passende Regel mit Speicherung ergänzt werden.\nDies wird auch als \"Memoization\" bezeichnet und ist eine zentrales Technik des Packrat Parsers (vgl. [Packrat2006]).\nSkizze: Idee des Packrat-Parsing head_memo = {} def head(): if head_memo.get(start) == -1: raise Exception() # kein Match if head_memo.get(start) \u003e= 0: start = head_memo[start]; return True # Vorspulen else: failed = False; start_ = start try: ... # rufe die ursprüngliche head()-Regel auf catch(e): failed = True; raise e finally: head_memo[start_] = (failed ? -1 : start)Quelle: Eigener Code basierend auf einer Idee nach [Parr2010, pp. 65/66]\nWenn bereits untersucht (Eintrag vorhanden): Vorspulen bzw. Exception werfen Sonst (aktuelle Position noch nicht in der Tabelle =\u003e Regel noch nicht an dieser Position getestet): Original-Regel ausführen Exception: Regel hatte keinen Erfolg =\u003e merken und Exception weiter reichen Ergebnis für diese Startposition und diese Regel merken: Falls Regel erfolgreich, dann Start-Position und die aktuelle Position (Stopp-Position) in der Tabelle für diese Regel notieren Falls Regel nicht erfolgreich, zur Start-Position eine ungültige Position setzen Anmerkung consume() Die Funktion consume() muss passend ergänzt werden: Wann immer man den lookahead-Puffer zurücksetzt, werden alle *_memo ungültig und müssen ebenfalls zurückgesetzt werden!\nSemantische Prädikate Problem in Java: enum ab Java5 Schlüsselwort (vorher als Identifier-Name verwendbar)\nprog : (enumDecl | stat)+ ; stat : ... ; enumDecl : ENUM id '{' id (',' id)* '}' ;Wie kann ich eine Grammatik bauen, die sowohl für Java5 und später als auch für die Vorgänger von Java5 funktioniert?\nAngenommen, man hätte eine Hilfsfunktion (\"Prädikat\"), mit denen man aus dem Kontext heraus die Unterscheidung treffen kann, dann würde die Umsetzung der Regel ungefähr so aussehen:\ndef prog(): if lookahead(1) == ENUM and java5: enumDecl() else: stat()Semantische Prädikate in ANTLR Semantische Prädikate in Parser-Regeln @parser::members {public static boolean java5;} prog : ({java5}? enumDecl | stat)+ ; stat : ... ; enumDecl : ENUM id '{' id (',' id)* '}' ;Prädikate in Parser-Regeln aktivieren bzw. deaktivieren alles, was nach der Abfrage des Prädikats gematcht werden könnte.\nSemantische Prädikate in Lexer-Regeln Alternativ für Lexer-Regeln:\nENUM : 'enum' {java5}? ; ID : [a-zA-Z]+ ;Bei Token kommt das Prädikat erst am rechten Ende einer Lexer-Regel vor, da der Lexer keine Vorhersage macht, sondern nach dem längsten Match sucht und die Entscheidung erst trifft, wenn das ganze Token gesehen wurde. Bei Parser-Regeln steht das Prädikat links vor der entsprechenden Alternative, da der Parser mit Hilfe des Lookaheads Vorhersagen trifft, welche Regel/Alternative zutrifft.\nAnmerkung: Hier wurden nur Variablen eingesetzt, es können aber auch Methoden/Funktionen genutzt werden. In Verbindung mit einer Symboltabelle (\"Symboltabellen\") und/oder mit Attributen und Aktionen in der Grammatik (\"Attribute\" und \"Interpreter: Attribute+Aktionen\") hat man hier ein mächtiges Hilfswerkzeug!\nWrap-Up LL(1) und LL(k): Erweiterungen Dynamischer Lookahead: BT-Parser mit Packrat-Ergänzung Semantische Prädikate zum Abschalten von Alternativen ",
    "description": "",
    "tags": null,
    "title": "LL-Parser: Fortgeschrittene Techniken",
    "uri": "/frontend/parsing/ll-advanced.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24) \u003e Frontend \u003e Parser",
    "content": "Hello World grammar Hello; start : stmt* ; stmt : ID '=' expr ';' | expr ';' ; expr : term ('+' term)* ; term : atom ('*' atom)* ; atom : ID | NUM ; ID : [a-z][a-zA-Z]* ; NUM : [0-9]+ ; WS : [ \\t\\n]+ -\u003e skip ; Konsole: Hello (grun, Parse-Tree) Starten des Parsers Grammatik übersetzen und Code generieren: antlr Hello.g4 Java-Code kompilieren: javac *.java Parser ausführen: grun Hello start -tree oder grun Hello start -gui (Grammatik \"Hello\", Startregel \"start\")\nAlternativ mit kleinem Java-Programm:\nimport org.antlr.v4.runtime.CharStreams; import org.antlr.v4.runtime.CommonTokenStream; import org.antlr.v4.runtime.tree.ParseTree; public class Main { public static void main(String[] args) throws Exception { HelloLexer lexer = new HelloLexer(CharStreams.fromStream(System.in)); CommonTokenStream tokens = new CommonTokenStream(lexer); HelloParser parser = new HelloParser(tokens); ParseTree tree = parser.start(); // Start-Regel System.out.println(tree.toStringTree(parser)); } } Startregeln start ist eine Parser-Regel =\u003e Eine Parser-Regel pro Grammatik wird benötigt, damit man den generierten Parser am Ende auch starten kann ... Alle Regeln mit kleinem Anfangsbuchstaben sind Parser-Regeln Alle Regeln mit großem Anfangsbuchstaben sind Lexer-Regeln Formen der Subregeln stmt : ID '=' expr ';' ;Um die Regel stmt anwenden zu können, müssen alle Elemente auf der rechten Seite der Regel erfüllt werden. Dabei müssen die Token wie ID, = und ; matchen und die Subregel expr muss erfüllt werden können. Beachten Sie das abschließende Semikolon am Ende einer ANTLR-Regel!\nstmt : ID '=' expr ';' | expr ';' ;Alternativen werden durch ein | getrennt. Hier muss genau eine Alternative erfüllt werden. Falls nötig, trennt man die Alternativen durch Einschließung in runden Klammern vom Rest der Regel ab: r : a (b | c) d ;.\nexpr : term ('+' term)* ;Der durch den * gekennzeichnete Teil kann beliebig oft vorkommen oder auch fehlen. Bei einem + müsste der Teil mind. einmal vorkommen und bei einem ? entsprechend einmal oder keinmal.\nAuch hier kann man die Operatoren durch ein zusätzliches ? auf non-greedy umschalten (analog zu den Lexer-Regeln).\n(vgl. github.com/antlr/antlr4/blob/master/doc/parser-rules.md)\nReihenfolge in Grammatik definiert Priorität Falls mehr als eine Parser-Regel die selbe Input-Sequenz matcht, löst ANTLR diese Mehrdeutigkeit auf, indem es die erste Alternative nimmt, die an der Entscheidung beteiligt ist.\nstart : stmt ; stmt : expr | ID ; expr : ID | NUM ;Bei der Eingabe \"foo\" würde die Alternative ID in der Regel expr \"gewinnen\", weil sie in der Grammatik vor der Alternative ID in der Regel stmt kommt und damit Vorrang hat.\nParse-Tree Betrachten wir erneut die obige Grammatik.\nDie Eingabe von \"a = 42;\" führt zu folgendem Parse-Tree:\nDiese Eingabe führt zur Erkennung der Token [ID, WS, =, WS, NUM, ;], wobei die WS-Token verworfen werden und der Parser den Tokenstream [ID, =, NUM, ;] erhält.\nDie Startregel hat auf der rechten Seite kein oder mehrere stmt-Regeln. Die stmt-Regel fordert auf der rechten Seite entweder die Token IDund = sowie die Regel expr gefolgt vom Token ;, oder die Regel expr gefolgt vom Token ;. In unserem Beispiel kann für das \"a\" das Token ID produziert werden, das \"=\" matcht ebenfalls. Die \"42\" wird erklärt, indem für expr ein term und dort ein atom aufgerufen wird. Für das atom muss entweder ein Token ID oder NUM als nächstes Token kommen - hier wird die \"42\" wird als Token NUM verarbeitet. Da die weiteren Regelteile in term und expr optional sind, haben wir damit ein expr erfüllt und das nachfolgende ;-Token schließt die erste Alternative der Regel stmt erfolgreich ab.\nIm entstehenden Parse-Tree sind diese Abläufe und grammatikalischen Strukturen direkt erkennbar. Jede erfolgreich durchlaufene Parserregel wird zu einem Knoten im Parse-Tree. Die Token werden als Terminale (Blätter) in den Baum eingehängt.\nAnmerkung: Der Parse-Tree ist das Ergebnis der Parsers-Phase im Compiler und dient damit als Input für die folgenden Compilerstufen. In der Regel benötigt man die oft recht komplexen Strukturen aber später nicht mehr und vereinfacht den Baum zu einem Abstract Syntax Tree (AST). Im Beispiel könnte man den Zweig stmt - expr - term - atom - 42 zu stmt - 42 vereinfachen.\nBetrachten wir nun die Eingabe foo = 2+3*4; bar = 3*4+2;. Diese führt zu folgendem Parse-Tree:\nWie man sehen kann, sind in der Grammatik die üblichen Vorrangregeln für die Operationen + und * berücksichtigt - die Multiplikation wird in beiden Fällen korrekt \"unter\" der Addition im Baum eingehängt.\nTo EOF not to EOF? Startregeln müssen nicht unbedingt den gesamten Input \"konsumieren\". Sie müssen per Default nur eine der Alternativen in der Startregel erfüllen.\nBetrachten wir noch einmal einen leicht modifizierten Ausschnitt aus der obigen Grammatik:\nstart : stmt ;Die Startregel wurde so geändert, dass sie nur noch genau ein Statement akzeptieren soll.\nIn diesem Fall würde die Startregel bei der Eingabe \"aa; bb;\" nur den ersten Teil \"aa;\" konsumieren (als Token ID) und das folgende \"bb;\" ignorieren. Das wäre in diesem Fall aber auch kein Fehler.\nWenn der gesamte Eingabestrom durch die Startregel erklärt werden soll, dann muss das vordefinierte Token EOF am Ende der Startregel eingesetzt werden:\nstart : stmt EOF;Hier würde die Eingabe \"aa; bb;\" zu einem Fehler führen, da nur der Teil \"aa;\" durch die Startregel abgedeckt ist (Token ID), und der Rest \"bb;\" zwar sogar ein gültiges Token wären (ebenfalls ID und ;), aber eben nicht mehr von der Startregel akzeptiert. Durch das EOF soll die Startregel aber den gesamten Input konsumieren und erklären, was hier nicht geht und entsprechend zum Fehler führt.\n(vgl. github.com/antlr/antlr4/blob/master/doc/parser-rules.md)\nExpressions und Vorrang (Operatoren) Betrachten wir noch einmal den Ausschnitt für die Ausdrücke (Expressions) in der obigen Beispielgrammatik:\nexpr : term ('+' term)* ; term : atom ('*' atom)* ; atom : ID ;Diese typische, etwas komplex anmutende Struktur soll sicher stellen, dass die Vorrangregeln für Addition und Multiplikation korrekt beachtet werden, d.h. dass 2+3*4 als 2+(3*4) geparst wird und nicht fälschlicherweise als (2+3)*4 erkannt wird.\nZusätzlich muss bei LL-Parsern Links-Rekursion vermieden werden: Die Parser-Regeln werden in Funktionsaufrufe übersetzt, d.h. bei einer Links-Rekursion würde man die selbe Regel immer wieder aufrufen, ohne ein Token aus dem Token-Strom zu entnehmen.\nANTLR (ab Version 4) kann mit beiden Aspekten automatisch umgehen:\nANTLR kann direkte Linksrekursion automatisch auflösen. Die Regel r : r T U | V ; kann also in ANTLR verarbeitet werden. ANTLR besitzt einen Mechanismus zur Auflösung von Mehrdeutigkeiten. Wie oben geschrieben, wird bei der Anwendbarkeit von mehreren Alternativen die erste Alternative genutzt. Damit lässt sich die typische Struktur für Expression-Grammatiken deutlich lesbarer gestalten:\nexpr : expr '*' expr | expr '+' expr | ID ;Die Regel expr ist links-rekursiv, was normalerweise bei LL-Parsern problematisch ist. ANTLR löst diese Links-Rekursion automatisch auf (vgl. github.com/antlr/antlr4/blob/master/doc/left-recursion.md).\nDa bei Mehrdeutigkeit in der Grammatik, also bei der Anwendbarkeit mehrerer Alternativen stets die erste Alternative genommen wird, lassen sich die Vorrangregeln durch die Reihenfolge der Alternativen in der expr-Regel implementieren: Die Multiplikation hat Vorrang von der Addition, und diese hat wiederum Vorrang von einer einfachen ID.\nDirekte vs. indirekte Links-Rekursion ANTLR kann nur direkte Links-Rekursion auflösen. Regeln wie r : r T U | V ; stellen in ANTLR also kein Problem dar.\nIndirekte Links-Rekursion erkennt ANTLR dagegen nicht:\nr : s T U | V ; s : r W X ;Hier würden sich die Regeln r und s gegenseitig aufrufen und kein Token aus dem Tokenstrom entfernen, so dass der generierte LL-Parser hier in einer Endlosschleife stecken bleiben würde. Mit indirekter Links-Rekursion kann ANTLR nicht umgehen.\nKonflikte in Regeln Wenn mehrere Alternativen einer Regel anwendbar sind, entscheidet sich ANTLR für die erste Alternative.\nWenn sich mehrere Tokenregeln überlappen, \"gewinnt\" auch hier die zuerst definierte Regel.\ndef : 'func' ID '(' ')' block ; FOR : 'for' ; ID : [a-z][a-zA-Z]* ;Hier werden ein implizites Token 'func' sowie die expliziten Token FOR und ID definiert. Dabei sind die Lexeme für 'func' und FOR auch in ID enthalten. Dennoch werden 'func' und FOR erkannt und nicht über ID gematcht, weil sie vor der Regel ID definiert sind.\nTatsächlich sortiert ANTLR die Regeln intern um, so dass alle Parser-Regeln vor den Lexer-Regeln definiert sind. Die impliziten Token werden dabei noch vor den expliziten Token-Regeln angeordnet. Im obigen Beispiel hat also 'func' eine höhere Priorität als FOR, und FOR hat eine höhere Priorität als ID. Aus diesem Grund gibt es die Konvention, die Parser-Regeln in der Grammatik vor den Lexer-Regeln zu definieren - dies entspricht quasi der Anordnung, die ANTLR bei der Verarbeitung sowieso erzeugen würde.\nAus diesem Grund würde auch eine Umsortierung der obigen Grammatik funktionieren:\nFOR : 'for' ; ID : [a-z][a-zA-Z]* ; def : 'func' ID '(' ')' block ;Intern würde ANTLR die Parser-Regel def wieder vor den beiden Lexer-Regeln anordnen, und zwischen den Parser-Regeln und den Lexer-Regeln die impliziten Token (hier 'func').\nKontext-Objekte für Parser-Regeln s : expr {List\u003cEContext\u003e x = $expr.ctx.e();} ; expr : e '*' e ; Jede Regel liefert ein passend zu dieser Regel generiertes Kontext-Objekt zurück. Darüber kann man das/die Kontextobjekt(e) der Sub-Regeln abfragen.\nDie Regel s liefert entsprechend ein SContext-Objekt und die Regel expr liefert ein ExprContext-Objekt zurück.\nIn der Aktion fragt man das Kontextobjekt über ctx ab, in den Listener- und Visitor-Methoden erhält man die Kontextobjekte als Parameter.\nFür einfache Regel-Aufrufe liefert die parameterlose Methode nur ein einziges Kontextobjekt (statt einer Liste) zurück.\nAnmerkung: ANTLR generiert nur dann Felder für die Regel-Elemente im Kontextobjekt, wenn diese in irgendeiner Form referenziert werden. Dies kann beispielsweise durch Benennung (Definition eines Labels, siehe nächste Folie) oder durch Nutzung in einer Aktion (siehe obiges Beispiel) geschehen.\nBenannte Regel-Elemente oder Alternativen stat : 'return' value=e ';' # Return | 'break' ';' # Break ;public static class StatContext extends ParserRuleContext { ... } public static class ReturnContext extends StatContext { public EContext value; public EContext e() { ... } } public static class BreakContext extends StatContext { ... }Mit value=e wird der Aufruf der Regel e mit dem Label value belegt, d.h. man kann mit $e.text oder $value.text auf das text-Attribut von e zugreifen. Falls es in einer Produktion mehrere Aufrufe einer anderen Regel gibt, muss man für den Zugriff auf die Attribute eindeutige Label vergeben.\nAnalog wird für die beiden Alternativen je ein eigener Kontext erzeugt.\nArbeiten mit ANTLR-Listeners ANTLR (generiert auf Wunsch) zur Grammatik passende Listener (Interface und leere Basisimplementierung). Beim Traversieren mit dem Default-ParseTreeWalker wird der Parse-Tree mit Tiefensuche abgelaufen und jeweils beim Eintritt in bzw. beim Austritt aus einen/m Knoten der passende Listener mit dem passenden Kontext-Objekt aufgerufen.\nDamit kann man die Grammatik \"für sich\" halten, d.h. unabhängig von einer konkreten Zielsprache und die Aktionen über die Listener (oder Visitors, s.u.) ausführen.\nexpr : e1=expr '*' e2=expr # MULT | e1=expr '+' e2=expr # ADD | DIGIT # ZAHL ;ANTLR kann zu dieser Grammatik calc.g4 einen passenden Listener (Interface calcListener) generieren (Option -listener beim Aufruf von antlr). Weiterhin generiert ANTLR eine leere Basisimplementierung (Klasse calcBaseListener):\n(Nur \"interessante\" Methoden gezeigt.)\nVon dieser Basisklasse leitet man einen eigenen Listener ab und implementiert die Methoden, die man benötigt.\npublic static class MyListener extends calcBaseListener { public void exitMULT(calcParser.MULTContext ctx) { ... } public void exitADD(calcParser.ADDContext ctx) { ... } public void exitZAHL(calcParser.ZAHLContext ctx) { ... } }Anschließend baut man das alles in eine Traversierung des Parse-Trees ein:\npublic class TestMyListener { public static class MyListener extends calcBaseListener { ... } public static void main(String[] args) throws Exception { calcLexer lexer = new calcLexer(CharStreams.fromStream(System.in)); CommonTokenStream tokens = new CommonTokenStream(lexer); calcParser parser = new calcParser(tokens); ParseTree tree = parser.s(); // Start-Regel ParseTreeWalker walker = new ParseTreeWalker(); MyListener eval = new MyListener(); walker.walk(eval, tree); } } Beispiel: TestMyListener.java und calc.g4 In Syntaxgesteuerte Interpreter werden wir damit einen einfachen syntaxgesteuerten Interpreter aufbauen.\nArbeiten mit dem Visitor-Pattern ANTLR (generiert ebenfalls auf Wunsch) zur Grammatik passende Visitoren (Interface und leere Basisimplementierung).\nHier muss man im Gegensatz zu den Listeners allerdings selbst für eine geeignete Traversierung des Parse-Trees sorgen. Dafür hat man mehr Freiheiten im Vergleich zum Einsatz von Listeners, insbesondere im Hinblick auf Rückgabewerte.\nexpr : e1=expr '*' e2=expr # MULT | e1=expr '+' e2=expr # ADD | DIGIT # ZAHL ;ANTLR kann zu dieser Grammatik einen passenden Visitor (Interface calcVisitor\u003cT\u003e) generieren (Option -visitor beim Aufruf von antlr). Weiterhin generiert ANTLR eine leere Basisimplementierung (Klasse calcBaseVisitor\u003cT\u003e):\n(Nur \"interessante\" Methoden gezeigt.)\nVon dieser Basisklasse leitet man einen eigenen Visitor ab und überschreibt die Methoden, die man benötigt. Wichtig ist, dass man selbst für das \"Besuchen\" der Kindknoten sorgen muss (rekursiver Aufruf der geerbten Methode visit()).\npublic static class MyVisitor extends calcBaseVisitor\u003cInteger\u003e { public Integer visitMULT(calcParser.MULTContext ctx) { return ... } public Integer visitADD(calcParser.ADDContext ctx) { return ... } public Integer visitZAHL(calcParser.ZAHLContext ctx) { return ... } }Anschließend baut man das alles in eine manuelle Traversierung des Parse-Trees ein:\npublic class TestMyVisitor { public static class MyVisitor extends calcBaseVisitor\u003cInteger\u003e { ... } public static void main(String[] args) throws Exception { calcLexer lexer = new calcLexer(CharStreams.fromStream(System.in)); CommonTokenStream tokens = new CommonTokenStream(lexer); calcParser parser = new calcParser(tokens); ParseTree tree = parser.s(); // Start-Regel MyVisitor eval = new MyVisitor(); eval.visit(tree); } } Beispiel: TestMyVisitor.java und calc.g4 In Syntaxgesteuerte Interpreter werden wir damit einen einfachen syntaxgesteuerten Interpreter aufbauen.\nEingebettete Aktionen und Attribute s : expr {System.err.println($expr.v);} ; expr returns [int v] : e1=expr '*' e2=expr {$v = $e1.v * $e2.v;} ;Auch die Parser-Regeln können mit eingebetteten Aktionen ergänzt werden, die in die (für die jeweilige Regel) generierte Methode eingefügt werden und bei erfolgreicher Anwendung der Parser-Regel ausgeführt werden.\nÜber returns [int v] fügt man der Regel expr ein Attribut v (Integer) hinzu, welches man im jeweiligen Kontext abfragen bzw. setzen kann (agiert als Rückgabewert der generierten Methode). Auf diesen Wert kann in den Aktionen mit $v zugegriffen werden.\nIn Attributierte Grammatiken werfen wir einen genaueren Blick auf die attributierten Grammatiken. In Syntaxgesteuerte Interpreter werden wir eingebettete Aktionen und Attribute nutzen, um einen einfachen syntaxgesteuerten Interpreter aufzubauen.\nAnmerkung: Durch den Einsatz von eingebetteten Aktionen und Attributen wird die Grammatik abhängig von der Zielsprache des generierten Lexers/Parsers!\nAusblick Damit haben wir die sprichwörtliche \"Spitze des Eisbergs\" gesehen. Mit ANTLR sind noch viele weitere Dinge möglich. Bitte nutzen Sie aktiv die Dokumentation auf github.com/antlr/antlr4.\nMit Hilfe von semantischen Prädikaten (vgl. LL-Parser: Fortgeschrittene Techniken) können Parser-Regeln aktiviert oder abgeschaltet werden, je nachdem, was vorher gesehen wurde.\nDem Thema Behandlung von Fehlern ist eine eigene Sitzung gewidmet: Error-Recovery.\nWrap-Up Parser mit ANTLR generieren: Parser-Regeln werden mit Kleinbuchstaben geschrieben\nRegeln können Lexer- und Parser-Regeln \"aufrufen\" Regeln können Alternativen haben Bei Mehrdeutigkeit: Vorrang für erste Alternative ANTLR erlaubt direkte Links-Rekursion ANTLR erzeugt Parse-Tree Benannte Alternativen und Regel-Elemente Traversierung des Parse-Tree: Listener oder Visitoren, Zugriff auf Kontextobjekte ",
    "description": "",
    "tags": null,
    "title": "Parser mit ANTLR generieren",
    "uri": "/frontend/parsing/antlr-parsing.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24) \u003e Frontend \u003e Parser",
    "content": "Grenze Lexer und Parser (Faustregeln) Der Lexer verwendet einfache reguläre Ausdrücke, während der Parser mit Lookaheads unterschiedlicher Größe, Backtracking und umfangreicher Error-Recovery arbeitet. Entsprechend sollte man alle Arbeit, die man bereits im Lexer erledigen kann, auch dort erledigen. Oder andersherum: Man sollte dem Parser nicht unnötige Arbeit aufbürden.\n=\u003e Erreiche in jeder Verarbeitungsstufe die maximal mögliche Abstraktionsstufe!\nMatche und verwerfe im Lexer alles, was der Parser nicht braucht.\nWenn bestimmte Dinge später nicht gebraucht werden, sollten sie bereits im Lexer erkannt und aussortiert werden. Der Lexer arbeitet deutlich einfacher und schneller als der Parser ... Und je weniger Token der Parser betrachten muss, um so einfacher und schneller kann er werden.\nMatche gebräuchliche Token (Namen, Schlüsselwörter, Strings, Zahlen) im Lexer.\nDer Lexer hat deutlich weniger Overhead als der Parser. Es lohnt sich deshalb, beispielsweise Ziffern bereits im Lexer zu Zahlen zusammenzusetzen und dem Parser als entsprechendes Token zu präsentieren.\nQuetsche alle lexikalischen Strukturen, die der Parser nicht unterscheiden muss, in einen Token-Typ.\nWenn der Parser bestimmte Strukturen nicht unterscheiden muss, dann macht es wenig Sinn, dennoch unterschiedliche Token an den Parser zu senden.\nBeispiel: Wenn eine Anwendung nicht zwischen Integer- und Gleitkommazahlen unterscheidet, sollte der Lexer dafür nur einen Token-Typ erzeugen und an den Parser senden (etwa NUMBER).\nBeispiel: Wenn der Parser nicht den Inhalt eines XML-Tags \"verstehen\" muss, dann kann man diesen in ein einzelnes Token packen.\nWenn der Parser Texteinheiten unterscheiden muss, erzeuge dafür eigene Token-Typen im Lexer.\nWenn der Parser etwa Elemente einer Telefonnummer verarbeiten muss, sollte der Lexer passende Token für die Teile der Telefonnummer erzeugen und an den Parser schicken.\nDiskussion: Parsen von Adressbüchern und Telefonnummern Typischer Aufbau eines Adressbuch-Eintrags:\nVorname Name: +49.571.8385-268 Zählen der Zeilen des Adressbuchs\nWenn es nur um das Zählen der Zeilen geht, muss der Parser nicht den Aufbau der Zeilen oder sogar den Aufbau von Telefonnummern verstehen. Es reichen einfache Lexer-Regeln (ROW), die quasi die Zeilenumbrüche repräsentieren. Der Rest (OTHER) wird per skip (ANTLR-Syntax) einfach entfernt ...\naddrbk : ROW+; ROW : '\\n'; OTHER : ~'\\n' -\u003e skip ; Liste aller Telefonnummern\nWenn man nun eine Liste aller Telefonnummern erzeugen will, wäre es ausreichend, die Struktur einer Zeile (und damit die Telefonnummern) mit Lexer-Regeln (und -Fragmenten) zu erkennen.\naddrbk : row+; row : SURNAME NAME ':' TELNR; Weitere Verarbeiten der Telefonnummern im Parser (Aktionen)\nWenn man zusätzlich die Telefonnummern noch weiter im Parser verarbeiten will (etwa durch eingebettete Aktionen), dann muss die Regel zum Erkennen der Adressen entsprechend eine Parser-Regel sein:\naddrbk : row+; row : SURNAME NAME ':' telnr; Die weiterführenden Lexer- und Parser-Regeln (telnr, TELNR, SURNAME, NAME) sind hier nicht dargestellt.\nWrap-Up Grenze zw. Lexer und Parser ist gleitend Ziel: möglichst hohe Abstraktion auf jeder Ebene erreichen ",
    "description": "",
    "tags": null,
    "title": "Grenze Lexer und Parser",
    "uri": "/frontend/parsing/finalwords.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24) \u003e Frontend \u003e Parser",
    "content": "Wiederholung Top-Down-Analyse Baumaufbau von oben nach unten die Grammatik muss reduziert sein recursive-descent parser First- und Follow-Mengen bestimmen Wahl der Ableitungen tabellengesteuert nicht mehr rekursiv, sondern mit PDA Motivation LL ist nicht alles Die Menge der LL-Sprachen ist eine echte Teilmenge der deterministisch kontextfreien Sprachen. Wir brauchen ein Verfahren, mit dem man alle deterministisch kontextfreien Sprachen parsen kann.\nBottom-Up-Analyse Von unten nach oben Bei $LL$-Sprachen muss man nach den ersten $k$ Eingabezeichen entscheiden, welche Ableitung ganz oben im Baum als erste durchgeführt wird, also eine, die im Syntaxbaum ganz weit weg ist von den Terminalen, die die Entschiedung bestimmen.Es gibt deterministisch kontextfreie Sprachen, die nicht $LL(k)$ sind für irgendein k.\nBei der Bottom-Up-Analyse geht man den umgekehrten Weg. Der Parse Tree wird von unten nach oben aufgebaut, die Entscheidung, welche Produktion angewandt wird, erfolgt \"näher\" am Terminal. Mit Hilfe der Produktionen und der Vorschautoken werden die Ableitungen \"rückwärts\" angewandt und \"Reduktionen\" genannt.\nFehlermeldungen können näher am Programmtext erfolgen.\nBaumaufbau von unten Hier entsteht ein Tafelbild.\nKann ein Stack helfen? Hier entsteht ein Tafelbild.\nProbleme damit? Hier entsteht ein Tafelbild.\nKonfliktfälle Mehrdeutigkeiten = Konflikte beim Parsen Es gibt Grammatiken, bei denen nicht aus dem Inhalt des Stacks und dem Eingabezeichen entschieden werden kann, wie fortgefahren wird, auch nicht, wenn man, wie auch schon im Fall $LL$, eine feste Zahl $k$ von Vorschautoken berücksichtigt. Diese Grammatiken können mehrdeutig sein.\nFolgen von falschen Entscheidungen:\nfalscher Baum, falsche Bäume kein Baum Mögliche Konflikte Reduce-Reduce-Konflikt: Es sind zwei oder mehr verschiedene Reduktionen möglich Shift-Reduce-Konflikt: Es kann nicht entschieden werden, ob eine Reduktion oder ein Shift durchgeführt werden soll. Shiften bedeutet, das nächste Eingabesymbol miteinbeziehen.\nLR-Parsing Da wollen wir hin Parser-Automat\nSo geht es Der Stack enthält Zustände, keine Terminals oder Nonterminals.\nDer Top-of-Stack ist immer der aktuelle Zustand, am Anfang $I_0$. Im Stack steht $I_0\\ \\bot$.\nVorgehen: Im aktuellen Zustand nachschauen, ob das Eingabezeichen auf einem Pfeil steht.\nja: Shiften, d. h. dem Pfeil folgen und den Zustand am Ende des Pfeils pushen. Dort weiter.\nnein: Reduzieren nach der Regel aus dem aktuellen Zustand mit dem Punkt hinten, d. h. so viele Zustände poppen, wie die Regel Elemente auf der rechten Seite hat. Der Zustand darunter wird aktuell, dem Pfeil mit dem zu reduzierenden Nonterminal der linken Seite der Regel folgen und pushen.\nAm Schluss kann nur noch mit $\\bot$ akzeptiert werden.\nBeispiel Hier entsteht ein Tafelbild.\nDefinitionen Def.: Bei einer kontextfreien Grammatik G ist die Rechtsableitung von $\\alpha \\in (N \\cup T)^{\\ast}$ die Ableitung, die man erhält, wenn das am weitesten rechts stehende Nichtterminal in $\\alpha$ abgeleitet wird. Man schreibt $\\alpha \\overset{\\ast}{\\Rightarrow}_r \\beta$.\nDef.: Eine Rechtssatzform $\\alpha$ einer Grammatik G ist ein Element aus $(N \\cup T)^{\\ast}$ mit $S \\overset{\\ast}{\\Rightarrow}_r \\alpha$.\nDef.: In dem Syntaxbaum von $S \\overset{\\ast}{\\Rightarrow}_r \\alpha\\ A\\ w \\Rightarrow_r \\alpha\\ \\beta\\ w$ einer kontextfreien Grammatik ist $\\beta$ ein Handle von der Produktion $A \\rightarrow \\beta$.\nLR-Parsing Bei der LR-Analyse eines Wortes w wird w von links nach rechts gelesen, dabei wird die Rechtsableitung von w in G von unten nach oben aufgebaut. Man spricht nicht nicht mehr von Ableitungen, sondern von Reduktionen.\nMehrdeutige Grammatiken können nicht LR sein.\nVor der Konstruktion des Automaten wird die Grammatik um eine neues Nonterminal $S'$ und die neue Produktion $S' \\rightarrow S$ erweitert. $S'$ ist dann Startsymbol. Es wird ein Automat erstellt (s.o.) Es wird eine Parse Table aus dem Automaten erstellt, die den Parse-Vorgang steuert, mit Aktionsteil und Sprungteil. Steuerung des Parsens mittels der Parse Table Parser Schema\nArbeitsweise Im Stack stehen nur Zustandsnummern, am Anfang die Nummer des Startzustandes (+ Bottomzeichen, oft auch $\\$$). Es ist nicht nötig, Symbole zu stacken.\nLesen des obersten Stackelements ergibt Zustand q Lesen des nächsten Eingabezeichens ergibt Zeichen a Nachschlagen der Reaktion auf $(q, a)$ in der Parse Table Durchführung der Reaktion Mögliche \"Actions\" ohne Berücksichtigung von Vorschautoken Shift: Schiebe logisch das nächste Eingabesymbol auf den Stack (in Wirklichkeit Zustandsnummern) Reduce: (Identifiziere ein Handle oben auf dem Stack und ersetze es durch das Nichtterminal der dazugehörigen Produktion.) Das ist gleichbedeutend mit: Entferne so viele Zustände vom Stack wie die rechte Seite der zu reduzierenden Regel Elemente hat, und schreibe den Zustand, der im Goto-Teil für $(q, a)$ steht, auf den Stack. Accept: Beende das Parsen erfolgreich Reagiere auf einen Syntaxfehler 0 Vorschautoken = LR(0)-Parsing LR-Parsing ohne Vorschautoken Wichtig: Das Handle, d. h. die rechte Seite einer zu reduzierenden Regel, erscheint oben auf dem Stack, nie weiter unten.\nJe nach Anwendungsfall müssen beim Reduzieren von Handles weitere Aktionen ausgeführt werden: z. B. Syntaxbäume aufgebaut, Werte in Tabellen geschrieben werden, usw. Nicht alle rechten Seiten von Produktionen, die oben auf dem Stack stehen, sind auch Handles, manchmal muss nur geshiftet werden.\nBsp: Steht bei der Beispielgrammatik von Folie 8 oben auf dem Stack ein $T$ mit dem nächsten Eingabezeichen $\\ast$, darf $T$ nicht zu $E$ reduziert werden.\nLösung: Der Parser merkt sich, wo er steht in noch nicht komplett reduzierten Regeln. Dazu benutzt er sogenannte Items oder LR(0)-Items, auch dotted Items oder (kanonische) LR(0)-Elemente genannt.\nItems Def.: Ein Item einer Grammatik G ist eine Produktion von G mit einem Punkt auf der rechten Seite der Regel vor, zwischen oder nach den Elementen.\nBsp.:\nZu der Produktion $A \\rightarrow BC$ gehören die Items:\n$[A\\rightarrow \\cdot B C]$ $[A\\rightarrow B \\cdot C$]\n$[A\\rightarrow B C \\cdot]$ Das zu $A \\rightarrow \\epsilon$ gehörende Item ist $[A \\rightarrow \\cdot]$\nWas bedeuten die Items? Hier entsteht ein Tafelbild.\nBerechnung der Closure_0 von einer Menge I von Items füge $I$ zu $CLOSURE_0 (I)$ hinzu\ngibt es ein Item $[A \\rightarrow \\alpha \\cdot B\\beta]$ aus $CLOSURE_0 (I)$ und eine Produktion $(B \\rightarrow \\gamma)$, füge $[B \\rightarrow \\cdot \\gamma]$ zu $CLOSURE_0 (I)$ hinzu\nBerechnung der GOTO_0-Sprungmarken $GOTO_0(I, X) = CLOSURE_0(\\lbrace[A \\rightarrow \\alpha X \\cdot \\beta] \\mid [A \\rightarrow \\alpha \\cdot X \\beta] \\in I\\rbrace)$ für eine Itemmenge I und $X \\in N \\cup T, A \\in N, \\alpha, \\beta \\in (N \\cup T)^{\\ast}$.\nKonstruktion des $LR(0)$ - Automaten Bilde die Hülle von $S' \\rightarrow S$ und mache sie zum ersten Zustand.\nFür jedes noch nicht betrachtete $\\cdot X, X \\in (N \\cup T)$ in einem Zustand $q$ des Automaten berechne $GOTO_0(q, X)$ und mache $GOTO_0(q, X)$ zu einem neuen Zustand $r$. Verbinde $q$ mit einem Pfeil mit $r$ und schreibe $X$ an den Pfeil. Ist ein zu $r$ identischer Zustand schon vorhanden, wird $p$ mit diesem verbunden und kein neuer erzeugt.\nKonstruktion der Parse Table Erstelle eine leere Tabelle mit den Zuständen als Zeilenüberschriften. Für den Aktionstabellenteil überschreibe die Spalten mit den Terminalen, für den Sprungtabellenteil mit den Nonterminals.\nShift: Für jeden mit einem Terminal beschrifteten Pfeil aus einem Zustand erstelle in der Aktionstabelle die Aktion shift mit der Nummer des Zustands, auf den der Pfeil zeigt. Für Pfeile mit Nonterminals schreibe in die Sprungtabelle nur die Nummer des Folgezustands.\nSchreibe beim Zustand $[S' \\rightarrow S \\cdot]$ ein $accept$ bei dem Symbol $\\bot$.\nFür jedes Item mit $[A \\rightarrow \\beta \\cdot]$ aus allen Zuständen schreibe für alle Terminals $reduce$ und die Nummer der entsprechenden Grammatikregel in die Tabelle.\nUnd wenn in einer Zelle schon ein Eintrag ist? Hier entsteht ein Tafelbild.\nDie Beispielgrammatik G1 (0) $S^{'} \\rightarrow S$\n(1) $S \\rightarrow a A b S c S$\n(2) $S \\rightarrow a A b S$\n(3) $S \\rightarrow d$\n(4) $A \\rightarrow e$\nDer LR(0)-Automat zu G1 LR(0)-Automat\nDie LR(0)-Parsertabelle zu G1 LR(0)-Parsertabelle\nWrap-Up Wrap-Up LR-Analyse baut den Ableitungbaum von unten nach oben auf.\nEs wird ein DFA benutzt zusammen mit einem Stack, der Zustände speichert.\nEine Parse-Tabelle steuert über Aktions- und Sprungbefehle das Verhalten des Parsers.\nDie Tabelle wird mit Items und Closures konstruiert.\n",
    "description": "",
    "tags": null,
    "title": "Syntaxanalyse: LR-Parser (Teil 1)",
    "uri": "/frontend/parsing/lr-parser1.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24) \u003e Frontend \u003e Parser",
    "content": "Wiederholung Ein PDA für $L=\\lbrace ww^{R}\\mid w\\in \\lbrace a,b\\rbrace^{\\ast}\\rbrace$:\nTop-Down-Analyse LL reicht nicht. LR: Aufbau des Ableitungsbaums von unten nach oben. LR(0): Parsen ohne Vorschautoken Ein DFA mit einem Stack wird über eine Tabelle mit Aktions- und Sprungbefehlen gesteuert. Im Stack stehen Zustände des DFAs. Diese Zustände werden mit sog. Dotted Items und deren Closures identifiziert. Motivation Auch LR(0) ist nicht alles Die Menge der LR(0)-Sprachen ist eine echte Teilmenge der deterministisch kontextfreien Sprachen. Wir brauchen ein Verfahren, mit dem man alle deterministisch kontextfreien Sprachen parsen kann.\nBottom-Up-Analyse mit Vorschautoken LR-Parsen mit 1 Vorschautoken Ist eine Grammatik nicht LR(0), kann sie vielleicht mit einem Vorschautoken geparst werden. Hier gibt es drei Verfahren:\nSLR(1)-Parsing (kanonisches) LR(1)-Parsing LALR(1)-Parsing SLR Simple LR(1) = (SLR)-Parsing $A \\rightarrow \\beta$ wird nur reduziert, wenn das Vorschautoken in der $FOLLOW$-Menge von $A$ ist.\n$\\Rightarrow$ Es ändert sich nur die Parse Table:\nBei allen LR(0)-Items in der Tabelle, die einen Punkt am Ende der rechten Seite stehen haben, trage in der Aktionstabelle beim zugehörigen Zustand die Reduktion mittels der zugehörigen Regel bei allen Terminals ein, die in der FOLLOW-Menge des Nonterminals auf der linken Seite der Regel enthalten sind.\nDer SLR-Automat der Grammatik G1: SLR(1)-Automat\nDie SLR-Parsertabelle der Grammatik G1 SLR(1)-Parsertabelle\nZum Vergleich: Die LR(0)-Tabelle von G1 (letzte Vorlesung) LR(0)-Parsertabelle\nKanonische LR(1)-Syntaxanalyse Mehr geht nicht: Kanonische LR(1)-Syntaxanalyse = LR-Analyse Beim SLR-Verfahren wird nach $A \\rightarrow \\beta$ reduziert, wenn das Vorschautoken in $Follow(A)$ liegt. Dabei kann es vorkommen, dass das Vorschautoken ein Element davon ist, aber genau bei dieser Regel kann es nicht dem A folgen. Es wird also falsch reduziert, und es entstehen zu viele Einträge in der Tabelle (Konflikte!).\nJetzt werden nicht Follow-Mengen von Nichtterminalen, sondern LOOKAHEAD-Mengen von Produktionen berechnet.\nDie LR(1)-Items Zu jedem LR(0)-Item (hier auch Kern genannt) wird eine LOOKAHEAD - Menge $L$ hinzugefügt, die angibt, welche Terminals dem Symbol auf der linken Seite folgen können.\nz. B. $[S' \\rightarrow \\cdot S, \\lbrace \\bot \\rbrace ]$\nDie Hülle $CLOSURE_1$ füge $I$ zu $CLOSURE_1 (I)$ hinzu\ngibt es ein LR(1) - Item $[A \\rightarrow \\alpha \\cdot B \\beta,\\ L]$ aus $CLOSURE_1 (I)$ und eine Produktion $(B \\rightarrow \\gamma)$, füge $[B \\rightarrow \\cdot \\gamma, FIRST(\\beta\\ L)]$ zu $CLOSURE_1 (I)$ hinzu ( $\\alpha, \\beta$ dürfen $\\epsilon$ sein).\n$Goto_1$ $GOTO_1(I, X) =$ eine Produktion\n$CLOSURE_1(\\lbrace[A \\rightarrow \\alpha X \\cdot \\beta, \\ L] \\mid [A \\rightarrow \\alpha \\cdot X \\beta,\\ L] \\in I\\rbrace)$ für eine Itemmenge $I$ und $X \\in N \\cup T, A \\in N, \\alpha, \\beta \\in (N \\cup T)^{\\ast}$.\nDer LR(1)-Automat Der Automat wird analog zum LR(0)-Automaten erstellt mit dem Startzustand\n$[S' \\rightarrow \\cdot S, \\lbrace \\bot \\rbrace ]$ Die Tabelle unterscheidet sich nur bei der Reduktion von der LR(0)-Tabelle:\nReduktionsoperationen werden in den Spalten der Terminals eingetragen, die in der LOOKAHEAD-Menge der entsprechenden Regel enthalten sind.\nDie Beispielgrammatik G2 (0) $S^{'} \\rightarrow S$\n(1) $S \\rightarrow NN$\n(2) $N \\rightarrow 0N$\n(3) $N \\rightarrow 1$\nDer LR(1)-Automat der Grammatik G2 LR(1)-Automat\nDie LR(1)-Parsertabelle der Grammatik G2 LR(1)-Parsertabelle\nLookahead-LR = LALR LALR(1) Zusammenfassung aller LR(1)-Zustände, die sich nur in den LOOKAHEAD-Mengen unterscheiden\nParsergeneratoren generieren oft direkt aus einem LR(0)- einen LALR(1)-Zustands- Übergangsgraphen durch Hinzufügen der LOOKAHEAD-Mengen.\nDer LALR-Automat der Grammatik G2 LALR(1)-Automat\nDie LALR-Parsertabelle der Grammatik G2 LALR(1)-Parsertabelle\n$k \\geq 2$ Vorschautoken Hierarachie Zu jeder LR(k)-Sprache gibt es eine LR(1)-Grammatik.\nMehrdeutige Grammatiken Es gibt auch Auswege Mehrdeutige Grammatiken sind oft leichter zu lesen und kleiner als die Grammatiken, die man erhält, wenn man die Mehrdeutigkeit auflöst, sofern möglich. Also die Grammatik mehrdeutig lassen!\nFolgendes kann trotzdem helfen:\nAngabe von Vorrangregeln Angabe von Assoziativität Voreinstellung des Parsergenearators: z. B. Shiften bei Shift-Reduce-Konflikten Voreinstellung des Parsergenearators: z. B. Reduzieren nach der Regel, die in der Grammatik zuerst kommt bei Reduce-Reduce-Konflikten Hierarchie der kontextfreien Sprachen Sprachenhierarchie\nWrap-Up Wrap-Up mit Bottom-Up-Parsing LR(1) kann man alle deterministisch kontextfreien Sprachen parsen\nein Vorschautoken genügt\nLR(0)-, SLR- und LALR- Parsing sind vereinfachte Verfahren für Teilmengen der LR-Sprachen\n",
    "description": "",
    "tags": null,
    "title": "Syntaxanalyse: LR-Parser (Teil 2)",
    "uri": "/frontend/parsing/lr-parser2.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24) \u003e Frontend \u003e Semantische Analyse",
    "content": "Zur semantischen Analyse gehört die Identifikation und Sammlung von Bezeichnern und die Zuordnung zur richtigen Ebene (Scopes). Das Werkzeug hierfür sind die Symboltabellen.\nSemantische Analyse: Symboltabellen Nested Scopes Funktionen Strukturen und Klassen ",
    "description": "",
    "tags": null,
    "title": "Symboltabellen",
    "uri": "/frontend/semantics/symboltables.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24) \u003e Frontend",
    "content": "Auf die lexikalische Analyse und die Syntaxanalyse folgt die semantische Analyse. Nach dem Parsen steht fest, dass ein Programm syntaktisch korrekt ist. Nun muss geprüft werden, ob es auch semantisch korrekt ist.\nSymboltabellen Typen, Type Checking und Attributierte Grammatiken ",
    "description": "",
    "tags": null,
    "title": "Semantische Analyse",
    "uri": "/frontend/semantics.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24) \u003e Frontend \u003e Semantische Analyse \u003e Symboltabellen",
    "content": "Was passiert nach der Syntaxanalyse? int x = 42; int f(int x) { int y = 9; return y+x; } x = f(x); Nach der Syntaxanalyse braucht der Compiler für die darauf folgenden Phasen semantische Analyse, Optimierung und Codegenerierung Informationen über Bezeichner, z.B.\nWelcher Bezeichner ist gemeint? Welchen Typ hat ein Bezeichner? Auf dem Weg zum Interpreter/Compiler müssen die Symbole im AST korrekt zugeordnet werden. Dies geschieht über Symboltabellen. Im Folgenden werden wir verschiedene Aspekte von Symboltabellen betrachten und eine mögliche Implementierung erarbeiten, bevor wir uns (in Interpreter) um die Auswertung (Interpretation) des AST kümmern können.\nLogische Compilierungsphasen Die lexikalische Analyse generiert eine Folge von Token.\nDie Syntaxanalyse generiert einen Parse Tree.\nDie semantische Analyse macht folgendes:\nDer Parse Tree wird in einen abstrakten Syntaxbaum (AST) umgewandelt. Dieser wird häufig mit Attributen annotiert. Dabei sind oft mehrere Baumdurchläufe nötig (z.B. wegen der Abhängigkeiten der Attribute). Nachfolgende Stufen:\nDer AST wird in einen Zwischencode umgewandelt mit Registern und virtuellen Adressen. Der Zwischencode wird optimiert. Aus dem optimierten Zwischencode wird der endgültige Code, aber immer noch mit virtuellen Adressen, generiert. Der generierte Code wird nachoptimiert. Der Linker ersetzt die virtuellen Adressen durch reale Adressen. Abgrenzung der Phasen Diese Phasen sind oft nicht klar unterscheidbar. Schon allein zur Verbesserung der Laufzeit baut der Parser oft schon den abstrakten Syntaxbaum auf, der Lexer trägt schon Bezeichner in Symboltabellen ein, der Parser berechnet beim Baumaufbau schon Attribute, ...\nOft werden gar nicht alle Phasen und alle Zwischendarstellungen benötigt.\nSemantische Analyse und Symboltabellen Syntax und Semantik Syntaxregeln: Formaler Aufbau eines Programms\nSemantik: Bedeutung eines (syntaktisch korrekten) Programms\n=\u003e Keine Codegenerierung für syntaktisch/semantisch inkorrekte Programme!\nZur Erinnerung: Die Syntaxregeln einer Programmiersprache bestimmen den formalen Aufbau eines zu übersetzenden Programms. Die Semantik gibt die Bedeutung eines syntaktisch richtigen Programms an.\nLexikalische und syntaktische Analyse können formalisiert mit regulären Ausdrücken und endlichen Automaten, sowie mit CFG und Parsern durchgeführt werden.\nDie Durchführung der semantischen Analyse ist stark von den Eigenschaften der zu übersetzenden Sprache, sowie der Zielsprache abhängig und kann hier nur beispielhaft für einige Eigenschaften erklärt werden.\nEs darf kein lauffähiges Programm erstellt werden können, dass nicht syntaktisch und semantisch korrekt ist. Ein lauffähiges Programm muss syntaktisch und semantisch korrekt sein!\nAufgaben der semantischen Analyse Identifikation und Sammlung der Bezeichner\nZuordnung zur richtigen Ebene (Scopes)\nTyp-Inferenz\nTypkonsistenz (Ausdrücke, Funktionsaufrufe, ...)\nValidieren der Nutzung von Symbolen\nVermeidung von Mehrfachdefinition Zugriff auf nicht definierte Bezeichner (Lesender) Zugriff auf nicht initialisierte Bezeichner Funktionen werden nicht als Variablen genutzt ... Die semantische Analyse überprüft die Gültigkeit eines syntaktisch korrekten Programms bzgl. statischer semantischer Eigenschaften und liefert die Grundlage für die (Zwischen-) Codeerzeugung und -optimierung. Insbesondere wird hier die Typkonsistenz (in Ausdrücken, von Parametern, ...) überprüft, und implizite Typumwandlungen werden vorgenommen. Oft müssen Typen automatisch bestimmt werden (z.B. bei Polymorphie, Typinferenz). Damit Typen bestimmt oder angepasst werden können, müssen Bezeichner zunächst identifiziert werden, d.h. bei namensgleichen Bezeichnern der richtige Bezug bestimmt werden.\nZu Annotationen/Attributen, Typen und Type-Checks siehe VL Typprüfungen, Attributgrammatiken\n=\u003e Ein wichtiges Hilfsmittel dazu sind Symboltabellen\nIdentifizierung von Objekten Beim Compiliervorgang müssen Namen immer wieder den dazugehörigen Definitionen zugeordnet, ihre Eigenschaften gesammelt und geprüft und darauf zugegriffen werden. Symboltabellen werden im Compiler fast überall gebraucht (siehe Abbildung unter \"Einordnung\").\nWelche Informationen zu einem Bezeichner gespeichert und ermittelt werden, ist dann abhängig von der Klasse des Bezeichners.\nValidieren der Nutzung von Symbolen Hier sind unendlich viele Möglichkeiten denkbar. Dies reicht von den unten aufgeführten Basisprüfungen bis hin zum Prüfen der Typkompatibilität bei arithmetischen Operationen. Dabei müssen für alle Ausdrücke die Ergebnistypen berechnet werden und ggf. automatische Konvertierungen vorgenommen werden, etwa bei 3+4.1 ...\nZugriff auf Variablen: Müssen sichtbar sein Zugriff auf Funktionen: Vorwärtsreferenzen sind OK Variablen werden nicht als Funktionen genutzt Funktionen werden nicht als Variablen genutzt =\u003e Verweis auf VL Typprüfungen, Attributgrammatiken\nDa Funktionen bereits vor dem Bekanntmachen der Definition aufgerufen werden dürfen, bietet sich ein zweimaliger Durchlauf (pass) an: Beim ersten Traversieren des AST werden alle Definitionen in der Symboltabelle gesammelt. Beim zweiten Durchlauf werden dann die Referenzen aufgelöst.\nDas Mittel der Wahl: Tabellen für die Symbole (= Bezeichner) Def.: Symboltabellen sind die zentrale Datenstruktur zur Identifizierung und Verwaltung von bezeichneten Elementen.\nDie Organisation der Symboltabellen ist stark anwendungsabhängig. Je nach Sprachkonzept gibt es eine oder mehrere Symboltabellen, deren Einträge vom Lexer oder Parser angelegt werden. Die jeweiligen Inhalte jedes einzelnen Eintrags kommen aus den verschiedenen Phasen der Compilierung. Symboltabellen werden oft als Hashtables oder auch als Bäume implementiert, manchmal als verkettete Listen. In seltenen Fällen kommt man auch mit einem Stack aus.\nEine Symboltabelle enthält benutzerdefinierte Bezeichner (oder Verweise in eine Hashtable mit allen vorkommenden Namen), manchmal auch die Schlüsselwörter der Programmiersprache. Die einzelnen Felder eines Eintrags variieren stark, abhängig vom Typ des Bezeichners (= Bezeichnerklasse).\nManchmal gibt es für Datentypen eine Extra-Tabelle, ebenso eine für die Werte von Konstanten.\nManchmal werden die Namen selbst in eine (Hash-) Tabelle geschrieben. Die Symboltabelle enthält dann statt der Namen Verweise in diese (Hash-) Tabelle.\nEinfache Verwaltung von Variablen primitiven Typs int x = 0; int i = 0; for (i=0; i\u003c10; i++) { x++; } Bsp.: Die zu übersetzende Sprache hat nur einen (den globalen) Scope und kennt nur Bezeichner für Variablen.\nEine Symboltabelle für alle Bezeichner Jeder Bezeichner ist der Name einer Variablen Symboltabelle wird evtl. mit Einträgen aller Schlüsselwörter initialisiert -- warum? Scanner erkennt Bezeichner und sucht ihn in der Symboltabelle Ist der Bezeichner nicht vorhanden, wird ein (bis auf den Namen leerer) Eintrag angelegt Scanner übergibt dem Parser das erkannte Token und einen Verweis auf den Symboltabelleneintrag Die Symboltabelle könnte hier eine (Hash-) Tabelle oder eine einfache verkettete Liste sein.\nWas kann jetzt weiter passieren? int x = 0; int i = 0; for (i=0; i\u003c10; i++) { x++; } a = 42;In vielen Sprachen muss überprüft werden, ob es ein definierendes Vorkommen des Bezeichners oder ein angewandtes Vorkommen ist.\nDefinitionen und Deklarationen von Bezeichnern Def.: Die Definition eines (bisher nicht existenten) Bezeichners in einem Programm generiert einen neuen Bezeichner und legt für ihn seinem Typ entsprechend Speicherplatz an.\nDef.: Unter der Deklaration eines (bereits existierenden) Bezeichners verstehen wir seine Bekanntmachung, damit er benutzt werden kann. Er ist oft in einem anderen Scope definiert und bekommt dort Speicherplatz zugeteilt.\nInsbesondere werden auch Typen deklariert. Hier gibt es in der Regel gar keine Speicherplatzzuweisung.\nEin Bezeichner kann beliebig oft deklariert werden, während er in einem Programm nur einmal definiert werden kann. Oft wird bei der Deklarationen eines Elements sein Namensraum mit angegeben.\nVorsicht: Die Begriffe werden auch anders verwendet. Z.B. findet sich in der Java-Literatur der Begriff Deklaration anstelle von Definition.\nAnmerkung: Deklarationen beziehen sich auf Definitionen, die woanders in einer Symboltabelle stehen, evtl. in einer anderen Datei, also in diesem Compilerlauf nicht zugänglich sind und erst von Linker aufgelöst werden können. Beim Auftreten einer Deklaration muss die dazugehörige Definition gesucht werden,und wenn vorhanden, im Symboltabelleneintrag für den deklarierten Bezeichner festgehalten werden. Hier ist evtl. ein zweiter Baumdurchlauf nötig, um alle offenen Deklarationen, die sich auf Definitionen in derselben Datei beziehen, aufzulösen.\nWird bei objektorientierten Sprachen ein Objekt definiert, dessen Klassendefinition in einer anderen Datei liegt, kann man die Definition des Objekts gleichzeitig als Deklaration der Klasse auffassen (Java).\nWo werden Verweise in Symboltabellen gebraucht? =\u003e Parse Tree und AST enthalten Verweise auf Symboltabelleneinträge\nIm Parse Tree enthält der Knoten für einen Bezeichner einen Verweis auf den Symboltabelleneintrag. Parser und semantische Analyse (AST) vervollständigen die Einträge. Attribute des AST können Feldern der Symboltabelle entsprechen, bzw. sich aus ihnen berechnen. Für Debugging-Zwecke können die Symboltabellen die ganze Compilierung und das Linken überleben. Grenzen der semantischen Analyse Welche semantischen Eigenschaften einer Sprache kann die semantische Analyse nicht überprüfen?\nWer ist dann dafür verantwortlich? Wie äußert sich das im Fehlerfall? Dinge, die erst durch eine Ausführung/Interpretation eines Programms berechnet werden können.\nBeispielsweise können Werte von Ausdrücken oft erst zur Laufzeit bestimmt werden. Insbesondere kann die semantische Analyse in der Regel nicht feststellen, ob ein Null-Pointer übergeben wird und anschließend dereferenziert wird.\nWrap-Up Semantische Analyse:\nIdentifikation und Sammlung der Bezeichner Zuordnung zur richtigen Ebene (Scopes) Validieren der Nutzung von Symbolen Typ-Inferenz Typkonsistenz (Ausdrücke, Funktionsaufrufe, ...) Symboltabellen: Verwaltung von Symbolen und Typen (Informationen über Bezeichner)\nSymboltabelleneinträge werden an verschiedenen Stellen des Compilers generiert und benutzt\n",
    "description": "",
    "tags": null,
    "title": "Semantische Analyse: Symboltabellen",
    "uri": "/frontend/semantics/symboltables/intro-symbtab.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24) \u003e Frontend \u003e Semantische Analyse \u003e Symboltabellen",
    "content": "Scopes und Name Spaces Def.: Unter dem Gültigkeitsbereich (Sichtbarkeitsbereich, Scope) eines Bezeichners versteht man den Programmabschnitt, in dem der Bezeichner sichtbar und nutzbar ist. Das ist oft der kleinste umgebende Block, außer darin enthaltene Scopes, die ein eigenes Element dieses Namens benutzen.\nScopes sind fast immer hierarchisch angeordnet.\nDef.: Unter einem Namensraum (name space) versteht man die Menge der zu einem Zeitpunkt sichtbaren Bezeichner.\nEs gibt Sprachen, in denen man eigene Namensräume explizit definieren kann (z.B. C++).\nVorsicht: Diese Begriffe werden nicht immer gleich definiert und auch gerne verwechselt.\nSymbole und (nested) Scopes int x = 42; float y; { int x; x = 1; y = 2; { int y = x; } } Aufgaben:\nbind(): Symbole im Scope definieren resolve(): Symbole aus Scope oder Eltern-Scope abrufen Hinzunahme von Scopes Bsp.: Die zu übersetzende Sprache ist scope-basiert und kennt nur Bezeichner für Variablen\nScopes können ineinander verschachtelt sein. Die Spezifikation der zu übersetzenden Sprache legt fest, in welcher Reihenfolge Scopes zu durchsuchen sind, wenn auf einen Bezeichner Bezug genommen wird, der nicht im aktuellen Scope definiert ist.\nInsgesamt bilden die Scopes oft eine Baumstruktur, wobei jeder Knoten einen Scope repräsentiert und seine Kinder die direkt in ihm enthaltenen Scopes sind. Dabei ist es in der Regel so, dass Scopes sich entweder vollständig überlappen oder gar nicht. Wenn ein Bezeichner nicht im aktuellen Scope vorhanden ist, muss er in der Regel in umschließenden Scopes gesucht werden. Hier kann ein Stack aller \"offenen\" Scopes benutzt werden.\nGrundlegendes Vorgehen Das Element, das einen neuen Scope definiert, steht selbst in dem aktuell behandelten Scope. Wenn dieses Element selbst ein Bezeichner ist, gehört dieser in den aktuellen Scope. Nur das, was nur innerhalb des oben genannten Elements oder Bezeichners definiert wird, gehört in den Scope des Elements oder Bezeichners.\nNested Scopes: Symbole und Scopes Implementierung mit hierarchischen (verketteten) Tabellen Pro Scope wird eine Symboltabelle angelegt, dabei enthält jede Symboltabelle zusätzlich einen Verweis auf ihre Vorgängersymboltabelle für den umgebenden Scope. Die globale Symboltabelle wird typischerweise mit allen Schlüsselwörtern initialisiert.\nWenn ein neuer Scope betreten wird, wird eine neue Symboltabelle erzeugt. Scanner: Erkennt Bezeichner und sucht ihn in der Symboltabelle des aktuellen Scopes bzw. trägt ihn dort ein und übergibt dem Parser das erkannte Token und einen Verweis auf den Symboltabelleneintrag (Erinnerung: Der Scanner wird i.d.R. vom Parser aus aufgerufen, d.h. der Parser setzt den aktuellen Scope!) Parser: Wird ein neues Element (ein Bezeichner) definiert, muss bestimmt werden, ob es einen eigenen Scope hat. Wenn ja, wird eine neue Symboltabelle für den Scope angelegt. Sie enthält alle Definitionen von Elementen, die in diesem Scope liegen. Der Bezeichner selbst wird in die aktuelle Symboltabelle eingetragen mit einem Verweis auf die neue Tabelle, die all die Bezeichner beinhaltet, die außerhalb dieses Scopes nicht sichtbar sein sollen. Die Tabellen werden untereinander verzeigert. Wird ein Element deklariert oder benutzt, muss sein Eintrag in allen sichtbaren Scopes in der richtigen Reihenfolge entlang der Verzeigerung gesucht (und je nach Sprachdefinition auch gefunden) werden. Der Parse-Tree enthält im Knoten für den Bezeichner den Verweis in die Symboltabelle Klassenhierarchie für Scopes Für die Scopes wird eine Klasse Scope definiert mit den Methoden bind() (zum Definieren von Symbolen im Scope) und resolve() (zum Abrufen von Symbolen aus dem Scope oder dem umgebenden Scope).\nFür lokale Scopes wird eine Instanz dieser Klasse angelegt, die eine Referenz auf den einschließenden Scope im Attribut enclosingScope hält. Für den globalen Scope ist diese Referenz einfach leer (None).\nKlassen und Interfaces für Symbole Für die Symbole gibt es die Klasse Symbol, wo für jedes Symbol Name und Typ gespeichert wird. Variablensymbole leiten direkt von dieser Klasse ab. Für die eingebauten Typen wird ein \"Marker-Interface\" Type erstellt, um Variablen- und Typ-Symbole unterscheiden zu können.\nQuelle: Eigene Modellierung nach einer Idee in [Parr2010, p. 142]\nAlternative Implementierung über einen Stack Der Parse Tree bzw. der AST enthalten an den Knoten, die jeweils einen ganzen Scope repräsentieren, einen Verweis auf die Symboltabelle dieses Scopes. Die Scopes werden in einem Stack verwaltet. Wird ein Scope betreten beim Baumdurchlauf, wird ein Verweis auf seine Symboltabelle auf den Stack gepackt. Die Suche von Bezeichnern in umliegenden Scopes erfordert ein Durchsuchen des Stacks von oben nach unten. Beim Verlassen eines Scopes beim Baumdurchlauf wird der Scope vom Stack entfernt. Nested Scopes: Definieren und Auflösen von Namen class Scope: Scope enclosingScope # None if global (outermost) scope Symbol\u003cString, Symbol\u003e symbols def resolve(name): # do we know \"name\" here? if symbols[name]: return symbols[name] # if not here, check any enclosing scope try: return enclosingScope.resolve(name) except: return None # not found def bind(symbol): symbols[symbol.name] = symbol symbol.scope = self # track the scope in each symbolQuelle: Eigene Implementierung nach einer Idee in [Parr2010, p. 169]\nAnmerkung: In der Klasse Symbol kann man ein Feld scope vom Typ Scope implementieren. Damit \"weiss\" jedes Symbol, in welchem Scope es definiert ist und man muss sich auf der Suche nach dem Scope eines Symbols ggf. nicht erst durch die Baumstruktur hangeln. Aus technischer Sicht verhindert das Attribut das Aufräumen eines lokalen Scopes durch den Garbage Collector, wenn man den lokalen Scope wieder verlässt: Jeder Scope hat eine Referenz auf den umgebenden (Eltern-) Scope (Feld enclosingScope). Wenn man den aktuellen Scope \"nach oben\" verlässt, würde der eben verlassene lokale Scope bei nächster Gelegenheit aufgeräumt, wenn es keine weiteren Referenzen auf diesen gäbe. Da nun aber die Symbole, die in diesem Scope definiert wurden, auf diesen verweisen, passiert das nicht :)\nNested Scopes: Listener Mit einem passenden Listener kann man damit die nötigen Scopes aufbauen:\nenterStart: erzeuge neuen globalen Scope definiere und pushe die eingebauten Typen exitVarDecl: löse den Typ der Variablen im aktuellen Scope auf definiere ein neues Variablensymbol im aktuellen Scope exitVar: löse die Variable im aktuellen Scope auf enterBlock: erzeuge neuen lokalen Scope, wobei der aktuelle Scope der Elternscope ist ersetze den aktuellen Scope durch den lokalen Scope exitBlock: ersetze den aktuellen Scope durch dessen Elternscope start : stat+ ; stat : block | varDecl | expr ';' ; block : '{' stat* '}' ; varDecl : type ID ('=' expr)? ';' ; expr : var '=' INT ; var : ID ; type : 'float' | 'int' ;Relevanter Ausschnitt aus der Grammatik\nint x = 42; { int y = 9; x = 7; } class MyListener(BaseListener): Scope scope def enterStart(Parser.FileContext ctx): globals = Scope() globals.bind(BuiltIn(\"int\")) globals.bind(BuiltIn(\"float\")) scope = globals def enterBlock(Parser.BlockContext ctx): scope = Scope(scope) def exitBlock(Parser.BlockContext ctx): scope = scope.enclosingScope def exitVarDecl(Parser.VarDeclContext ctx): t = scope.resolve(ctx.type().getText()) var = Variable(ctx.ID().getText(), t) scope.bind(var) def exitVar(Parser.VarContext ctx): name = ctx.ID().getText() var = scope.resolve(name) if var == None: error(\"no such var: \" + name)Anmerkung: Um den Code auf die Folie zu bekommen, ist dies ein Mix aus Java und Python geworden. Sry ;)\nIn der Methode exitVar() wird das Variablensymbol beim Ablaufen des AST lediglich aufgelöst und ein Fehler geworfen, wenn das Variablensymbol (noch) nicht bekannt ist. Hier könnte man weiteres Type-Checking und/oder -Propagation ansetzen.\nSpäter im Interpreter muss an dieser Stelle dann aber auch der Wert der Variablen abgerufen werden ...\nLöschen von Symboltabellen Möglicherweise sind die Symboltabellen nach der Identifizierungsphase der Elemente überflüssig, weil die zusammengetragenen Informationen als Attribute im AST stehen. Die Knoten enthalten dann Verweise auf definierende Knoten von Elementen, nicht mehr auf Einträge in den Symboltabellen. In diesem Fall können die Symboltabellen nach der Identifizierung gelöscht werden, wenn sie nicht z.B. für einen symbolischen Debugger noch gebraucht werden.\nWrap-Up Symboltabellen: Verwaltung von Symbolen und Typen (Informationen über Bezeichner)\nBlöcke: Nested Scopes =\u003e hierarchische Organisation\nBinden von Bezeichner gleichen Namens an ihren jeweiligen Scope =\u003e bind()\nAbrufen von Bezeichnern aus dem aktuellen Scope oder den Elternscopes =\u003e resolve()\n",
    "description": "",
    "tags": null,
    "title": "Nested Scopes",
    "uri": "/frontend/semantics/symboltables/scopes.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24) \u003e Frontend \u003e Semantische Analyse \u003e Symboltabellen",
    "content": "Funktionen und Scopes int x = 42; int y; void f() { int x; x = 1; y = 2; { int y = x; } } void g(int z){} Behandlung von Funktionsdefinitionen Jeder Symboltabelleneintrag braucht ein Feld, das angibt, ob es sich um eine Variable, eine Funktion, ... handelt. Alternativ eine eigene Klasse ableiten ... Der Name der Funktion steht als Bezeichner in der Symboltabelle des Scopes, in dem die Funktion definiert wird. Der Symboltabelleneintrag für den Funktionsnamen enthält Verweise auf die Parameter. Der Symboltabelleneintrag für den Funktionsnamen enthält Angaben über den Rückgabetypen. Jede Funktion wird grundsätzlich wie ein neuer Scope behandelt. Die formalen Parameter werden als Einträge in der Symboltabelle für den Scope der Funktion angelegt and entsprechend als Parameter gekennzeichnet. Behandlung von Funktionsaufrufen Der Name der Funktion steht als Bezeichner in der Symboltabelle des Scopes, in dem die Funktion aufgerufen wird und wird als Aufruf gekennzeichnet. Der Symboltabelleneintrag für den Funktionsnamen enthält Verweise auf die aktuellen Parameter. Die Definition der Funktion wird in den zugänglichen Scopes gesucht (wie oben) und ein Verweis darauf in der Symboltabelle gespeichert. Erweiterung des Klassendiagramms für Funktions-Scopes Quelle: Eigene Modellierung nach einer Idee in [Parr2010, p. 147]\nFunktionen sind Symbole und Scopes class Function(Scope, Symbol): def __init__(name, retType, enclScope): Symbol.__init__(name, retType) # we are \"Symbol\" ... enclosingScope = enclScope # ... and \"Scope\"Funktionen: Listener Den Listener zum Aufbau der Scopes könnte man entsprechend erweitern:\nenterFuncDecl: löse den Typ der Funktion im aktuellen Scope auf lege neues Funktionssymbol an, wobei der aktuelle Scope der Elternscope ist definiere das Funktionssymbol im aktuellen Scope ersetze den aktuellen Scope durch das Funktionssymbol exitFuncDecl: ersetze den aktuellen Scope durch dessen Elternscope exitParam: analog zu exitVarDecl löse den Typ der Variablen im aktuellen Scope auf definiere ein neues Variablensymbol im aktuellen Scope exitCall: analog zu exitVar löse das Funktionssymbol (und die Argumente) im aktuellen Scope auf funcDecl : type ID '(' params? ')' block ; params : param (',' param)* ; param : type ID ; call : ID '(' exprList? ')' ; exprList : expr (',' expr)* ;Relevanter Ausschnitt aus der Grammatik\nint f(int x) { int y = 9; } int x = f(x); def enterFuncDecl(Parser.FuncDeclContext ctx): name = ctx.ID().getText() type = scope.resolve(ctx.type().getText()) func = Function(name, type, scope) scope.bind(func) # change current scope to function scope scope = func def exitFuncDecl(Parser.FuncDeclContext ctx): scope = scope.enclosingScope def exitParam(Parser.ParamContext ctx): t = scope.resolve(ctx.type().getText()) var = Variable(ctx.ID().getText(), t) scope.bind(var) def exitCall(Parser.CallContext ctx): name = ctx.ID().getText() func = scope.resolve(name) if func == None: error(\"no such function: \" + name) if func.type == Variable: error(name + \" is not a function\")Anmerkung: Um den Code auf die Folie zu bekommen, ist dies wieder ein Mix aus Java und Python geworden. Sry ;)\nIm Vergleich zu den einfachen nested scopes kommt hier nur ein weiterer Scope für den Funktionskopf dazu. Dieser spielt eine Doppelrolle: Er ist sowohl ein Symbol (welches im Elternscope bekannt ist) als auch ein eigener (lokaler) Scope für die Funktionsparameter.\nUm später im Interpreter eine Funktion tatsächlich auswerten zu können, muss im Scope der Funktion zusätzlich der AST-Knoten der Funktionsdefinition gespeichert werden (weiteres Feld/Attribut in Function)!\nWrap-Up Symboltabellen: Verwaltung von Symbolen und Typen (Informationen über Bezeichner)\nFunktionen: Nested Scopes =\u003e hierarchische Organisation\nUmgang mit dem Funktionsnamen, den Parametern und dem Funktionskörper\n",
    "description": "",
    "tags": null,
    "title": "Funktionen",
    "uri": "/frontend/semantics/symboltables/functions.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24) \u003e Frontend \u003e Semantische Analyse \u003e Symboltabellen",
    "content": "Strukturen struct A { int x; struct B {int x;}; B b; struct C {int z;}; }; A a; void f() { A a; a.b.x = 42; } Strukturen: Erweiterung der Symbole und Scopes Quelle: Eigene Modellierung nach einer Idee in [Parr2010, p. 162]\nStrukturen stellen wie Funktionen sowohl einen Scope als auch ein Symbol dar.\nZusätzlich stellt eine Struktur (-definition) aber auch einen neuen Typ dar, weshalb Struct auch noch das Interface Type \"implementiert\".\nStrukturen: Auflösen von Namen class Struct(Scope, Symbol, Type): def resolveMember(name): return symbols[name]=\u003e Auflösen von \"a.b\" (im Listener in exitMember()):\na im \"normalen\" Modus mit resolve() über den aktuellen Scope Typ von a ist Struct mit Verweis auf den eigenen Scope b nur innerhalb des Struct-Scopes mit resolveMember() In der Grammatik würde es eine Regel member geben, die auf eine Struktur der Art ID.ID anspricht (d.h. eigentlich den Teil .ID), und entsprechend zu Methoden enterMember() und exitMember() im Listener führt.\nDas Symbol für a hat als type-Attribut eine Referenz auf die Struct, die ja einen eigenen Scope hat (symbols-Map). Darin muss dann b aufgelöst werden.\nKlassen class A { public: int x; void foo() { ; } }; class B : public A { public int y; void foo() { int z = x+y; } }; Klassen: Erweiterung der Symbole und Scopes Quelle: Eigene Modellierung nach einer Idee in [Parr2010, p. 167]\nBei Klassen kommt in den Tabellen ein weiterer Pointer parentClazz auf die Elternklasse hinzu (in der Superklasse ist der Wert None).\nKlassen: Auflösen von Namen class Clazz(Struct): Clazz parentClazz # None if base class def resolve(name): # do we know \"name\" here? if symbols[name]: return symbols[name] # NEW: if not here, check any parent class ... if parentClazz != None: return parentClazz.resolve(name) # ... or enclosing scope if base class try: return enclosingScope.resolve(name) except: return None # not found def resolveMember(name): if symbols[name]: return symbols[name] # NEW: check parent class try: return parentClazz.resolveMember(name) except: return NoneQuelle: Eigene Implementierung nach einer Idee in [Parr2010, p. 172]\nBeim Auflösen von Attributen oder Methoden muss zunächst in der Klasse selbst gesucht werden, anschließend in der Elternklasse.\nBeispiel (mit den obigen Klassen A und B):\nB foo; foo.x = 42;Hier wird analog zu den Structs zuerst foo mit resolve() im lokalen Scope aufgelöst. Der Typ des Symbols foo ist ein Clazz, was zugleich ein Scope ist. In diesem Scope wird nun mit resolveMember() nach dem Symbol x gesucht. Falls es hier nicht gefunden werden kann, wird in der Elternklasse (sofern vorhanden) weiter mitresolveMember() gesucht.\nDie normale Namensauflösung wird ebenfalls erweitert um die Auflösung in der Elternklasse.\nBeispiel:\nint wuppie; class A { public: int x; void foo() { ; } }; class B : public A { public int y; void foo() { int z = x+y+wuppie; } };Hier würde wuppie als Symbol im globalen Scope definiert werden. Beim Verarbeiten von int z = x+y+wuppie; würde mit resolve() nach wuppie gesucht: Zuerst im lokalen Scope unterhalb der Funktion, dann im Funktions-Scope, dann im Klassen-Scope von B. Hier sucht resolve() auch zunächst lokal, geht dann aber die Vererbungshierarchie entlang (sofern wie hier vorhanden). Erst in der Superklasse (wenn der parentClazz-Zeiger None ist), löst resolve() wieder normal auf und sucht um umgebenden Scope. Auf diese Weise kann man wie gezeigt in Klassen (Methoden) auf globale Variablen verweisen ...\nAnmerkung: Durch dieses Vorgehen wird im Prinzip in Methoden aus dem Zugriff auf ein Feld x implizit ein this.x aufgelöst, wobei this die Klasse auflöst und x als Attribut darin.\nWrap-Up Symboltabellen: Verwaltung von Symbolen und Typen (Informationen über Bezeichner)\nStrukturen und Klassen bilden eigenen Scope\nStrukturen/Klassen lösen etwas anders auf: Zugriff auf Attribute und Methoden\n",
    "description": "",
    "tags": null,
    "title": "Strukturen und Klassen",
    "uri": "/frontend/semantics/symboltables/classes.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24) \u003e Frontend \u003e Semantische Analyse",
    "content": "Motivation Ist das alles erlaubt? Operation erlaubt?\nZuweisung erlaubt?\nWelcher Ausdruck hat welchen Typ?\n(Welcher Code muss dafür erzeugt werden?)\na = b a = f(b) a = b + c a = b + o.nummer if (f(a) == f(b)) Taschenrechner: Parsen von Ausdrücken wie 3*5+4 expr : expr '+' term | term ; term : term '*' DIGIT | DIGIT ; DIGIT : [0-9] ; =\u003e Wie den Ausdruck ausrechnen?\nAnmerkung: Heute geht es um die einfachste Form der semantischen Analyse: Anreichern einer Grammatik um Attribute und Aktionen, die während des Parsens oder bei der Traversierung des Parse-Trees ausgewertet werden.\nSemantische Analyse Das haben wir bis jetzt Wir haben den AST vorliegen.\nIdealerweise enthält er bei jedem Bezeichner einen Verweis in sogenannte Symboltabellen (siehe spätere Veranstaltung).\nBeim Parsen können schon einige semantische Eigenschaften des zu übersetzenden Programms überprüft werden, falls erforderlich z. B.:\nWurden alle Variablen / Objekte vor ihrer Verwendung definiert oder deklariert? Wurden keine Elemente mehrfach definiert? Wurden alle Funktionen / Methoden mit der richtigen Anzahl Parameter aufgerufen? (Nicht in allen Fällen schon prüfbar) Haben Arrayzugriffe auch keine zu hohe Dimension? Werden auch keine Namen benutzt, für die es keine Definition / Deklaration gibt? Was fehlt jetzt noch? Es müssen kontextsensitive Analysen durchgeführt werden, allen voran Typanalysen. Damit der \"richtige\" (Zwischen-) Code entsprechend den beteiligten Datentypen erzeugt werden kann, muss mit Hilfe des Typsystems der Sprache (aus der Sprachdefinition) überprüft werden, ob alle Operationen nur mit den korrekten Datentypen benutzt werden. Dazu gehört auch, dass nicht nur Typen von z. B. Variablen, sondern von ganzen Ausdrücken betrachtet, bzw. bestimmt werden. Damit kann dann für die Codeerzeugung festgelegt werden, welcher Operator realisiert werden muss (Überladung).\nAnalyse von Datentypen Typisierung stark oder statisch typisierte Sprachen: Alle oder fast alle Typüberprüfungen finden in der semantischen Analyse statt (C, C++, Java) schwach oder dynamisch typisierte Sprachen: Alle oder fast alle Typüberprüfungen finden zur Laufzeit statt (Python, Lisp, Perl) untypisierte Sprachen: keinerlei Typüberprüfungen (Maschinensprache) Ausdrücke Jetzt muss für jeden Ausdruck im weitesten Sinne sein Typ bestimmt werden.\nAusdrücke können hier sein:\nrechte Seiten von Zuweisungen linke Seiten von Zuweisungen Funktions- und Methodenaufrufe jeder einzelne aktuelle Parameter in Funktions- und Methodenaufrufen Bedingungen in Kontrollstrukturen Typinferenz Def.: Typinferenz ist die Bestimmung des Datentyps jedes Bezeichners und jedes Ausdrucks im Code.\nDer Typ eines Ausdrucks wird mit Hilfe der Typen seiner Unterausdrücke bestimmt.\nDabei kann man ein Kalkül mit sog. Inferenzregeln der Form\n$$\\frac{f:s \\rightarrow t\\ \\ \\ \\ \\ x:s}{f(x) : t}$$ (Wenn f den Typ $s \\rightarrow t$ hat und x den Typ s, dann hat der Ausdruck f(x) den Typ t.)\nbenutzen. So wird dann z. B. auch Überladung aufgelöst und Polymorphie zur Laufzeit.\nStatische Typprüfungen Bsp.: Der + - Operator:\nTyp 1. Operand Typ 2. Operand Ergebnistyp int int int float float float int float float float int float string string string Typkonvertierungen Der Compiler kann implizite Typkonvertierungen vornehmen, um einen Ausdruck zu verifizieren (siehe Sprachdefiniton).\nIn der Regel sind dies Typerweiterungen, z.B. von int nach float.\nManchmal muss zu zwei Typen der kleinste Typ gefunden werden, der beide vorhandenen Typen umschließt.\nExplizite Typkonvertierungen heißen auch Type Casts.\nNicht grundsätzlich statisch mögliche Typprüfungen Bsp.: Der ^ - Operator $(a^b)$:\nTyp 1. Operand Typ 2. Operand Ergebnistyp int int $\\geq$ 0 int int int \u003c 0 float int float float $\\ldots$ $\\ldots$ $\\ldots$ Attributierte Grammatiken Was man damit macht Die Syntaxanalyse kann keine kontextsensitiven Analysen durchführen.\nKontextsensitive Grammatiken benutzen: Laufzeitprobleme, das Parsen von cs-Grammatiken ist PSPACE-complete.\nDer Parsergenerator Bison generiert LALR(1)-Parser, aber auch sog. Generalized LR (GLR) Parser, die bei nichtlösbaren Konflikten in der Grammatik (Reduce/Reduce oder Shift/Reduce) parallel den Input mit jede der Möglichkeiten weiterparsen.\nEin weiterer Ansatz, kontextsensitive Abhängigkeiten zu berücksichtigen, ist der Einsatz von attributierten Grammatiken, nicht nur zur Typanalyse, sondern evtl. auch zur Codegenerierung.\nInformationen weden im Baum weitergegeben.\nSyntax-gesteuerte Übersetzung: Attribute und Aktionen Berechnen der Ausdrücke expr : expr '+' term ; translate expr ; translate term ; handle + ; Attributierte Grammatiken (SDD) auch \"syntax-directed definition\"\nAnreichern einer CFG:\nZuordnung einer Menge von Attributen zu den Symbolen (Terminal- und Nicht-Terminal-Symbole) Zuordnung einer Menge von semantischen Regeln (Evaluationsregeln) zu den Produktionen Definition: Attributierte Grammatik Eine attributierte Grammatik AG = (G,A,R) besteht aus folgenden Komponenten:\nMengen A(X) der Attribute eines Nonterminals X\nG = (N, T, P, S) ist eine cf-Grammatik\nA = $\\bigcup\\limits_{X \\in (T \\cup N)} A(X)$ mit $A(X) \\cap A(Y) \\neq \\emptyset \\Rightarrow X = Y$\nR = $\\bigcup\\limits_{p \\in P} R(p)$ mit $R(p) = \\lbrace X_i.a = f(\\ldots) \\vert p : X_0 \\rightarrow X_1 \\ldots X_n \\in P, X_i.a \\in A(X_i), 0 \\leq i \\leq n\\rbrace$\nAbgeleitete und ererbte Attribute Die in einer Produktion p definierten Attribute sind\nAF(p) = $\\lbrace X_i.a \\ \\vert\\ p : X_0 \\rightarrow X_1 \\ldots X_n \\in P, 0 \\leq i \\leq n, X_i.a = f(\\ldots) \\in R(p)\\rbrace$\nWir betrachten Grammatiken mit zwei disjunkten Teilmengen, den abgeleiteten (synthesized) Attributen AS(X) und den ererbten (inherited) Attributen AI(X):\nAS(X) = $\\lbrace X.a\\ \\vert \\ \\exists p : X \\rightarrow X_1 \\ldots X_n \\in P, X.a \\in AF(p)\\rbrace$\nAI(X) = $\\lbrace X.a\\ \\vert \\ \\exists q : Y \\rightarrow uXv \\in P, X.a\\in AF(q)\\rbrace$\nAbgeleitete Attribute geben Informationen von unten nach oben weiter, geerbte von oben nach unten.\nDie Abhängigkeiten der Attribute lassen sich im sog. Abhängigkeitsgraphen darstellen.\nBeispiel: Attributgrammatiken Produktion Semantische Regel e : e1 '+' t ; e.val = e1.val + t.val e : t ; e.val = t.val t : t1 '*' D ; t.val = t1.val * D.lexval t : D ; t.val = D.lexval Produktion Semantische Regel t : D t' ; t'.inh = D.lexval t.syn = t'.syn t' : '*' D t'1 ; t'1.inh = t'.inh * D.lexval t'.syn = t'1.syn t' : $\\epsilon$ ; t'.syn = t'.inh Wenn ein Nichtterminal mehr als einmal in einer Produktion vorkommt, werden die Vorkommen nummeriert. (t, t1; t', t'1)\nS-Attributgrammatiken und L-Attributgrammatiken S-Attributgrammatiken: Grammatiken mit nur abgeleiteten Attributen, lassen sich während des Parsens mit LR-Parsern bei beim Reduzieren berechnen mittels Tiefensuche mit Postorder-Evaluation:\ndef visit(N): for each child C of N (from left to right): visit(C) eval(N) # evaluate attributes of NL-Attributgrammatiken: Grammatiken, deren gerbte Atribute nur von einem Elternknoten oder einem linken Geschwisterknoten abhängig sind. Sie können während des Parsens mit LL-Parsern berechnet werden. Ein links-nach-rechts-Durchlauf ist ausreichend.\nAlle Kanten im Abhängigkeitsgraphen gehen nur von links nach rechts.\nS-attributierte SDD sind eine Teilmenge von L-attributierten SDD.\nBeispiel: S-Attributgrammatik Produktion Semantische Regel e : e1 '+' t ; e.val = e1.val + t.val e : t ; e.val = t.val t : t1 '*' D ; t.val = t1.val * D.lexval t : D ; t.val = D.lexval Beispiel: Annotierter Syntaxbaum für 5*8+2 Annotierter Parse-Tree\nErzeugung des AST aus dem Parse-Tree für 5*8+2 Produktion Semantische Regel e : e1 '+' t ; e.node = new Node('+', e1.node, t.node) e : t ; e.node = t.node t : t1 '*' D ; t.node = new Node('*', t1.node, new Leaf(D, D.lexval)); t : D ; t.node = new Leaf(D, D.lexval); AST\nBeispiel: L-Attributgrammatik, berechnete u. geerbte Attribute, ohne Links-Rekursion Teil der vorigen SDD zum Parsen und Berechnen von Ausdrücken wie 5*8+2, hier umformuliert ohne Links-Rekursion und mit berechneten und geerbten Attributen:\nProduktion Semantische Regel t : D t' ; t'.inh = D.lexval t.syn = t'.syn t' : '*' D t'1 ; t'1.inh = t'.inh * D.lexval t'.syn = t'1.syn t' : $\\epsilon$ ; t'.syn = t'.inh 5*8 =\u003e\nAnnotierter Parse-Tree mit berechneten und geerbten Attributen (nur Multiplikation)\nVorgriff: Dies ist ein Beispiel für eine \"L-attributierte SDD\".\nBeispiel: Typinferenz für 3+7+9 oder \"hello\"+\"world\" Produktion Semantische Regel e : e1 '+' t ; e.type = f(e1.type, t.type) e : t ; e.type = t.type t : NUM ; t.type = \"int\" t : NAME ; t.type = \"string\" Syntax-gesteuerte Übersetzung (SDT) Erweiterung attributierter Grammatiken Syntax-directed translation scheme:\nZu den Attributen kommen Semantische Aktionen: Code-Fragmente als zusätzliche Knoten im Parse Tree an beliebigen Stellen in einer Produktion, die, wenn möglich, während des Parsens, ansonsten in weiteren Baumdurchläufen ausgeführt werden.\ne : e1 {print e1.val;} '+' {print \"+\";} t {e.val = e1.val + t.val; print(e.val);} ; S-attributierte SDD, LR-Grammatik: Bottom-Up-Parsierbar Die Aktionen werden am Ende jeder Produktion eingefügt (\"postfix SDT\").\nProduktion Semantische Regel e : e1 '+' t ; e.val = e1.val + t.val e : t ; e.val = t.val t : t1 '*' D ; t.val = t1.val * D.lexval t : D ; t.val = D.lexval e : e1 '+' t {e.val = e1.val + t.val; print(e.val);} ; e : t {e.val = t.val;} ; t : t1 '*' D {t.val = t1.val * D.lexval;} ; t : D {t.val = D.lexval;} ; L-attributierte SDD, LL-Grammatik: Top-Down-Parsierbar (1/2) Produktion Semantische Regel t : D t' ; t'.inh = D.lexval t.syn = t'.syn t' : '*' D t'1 ; t'1.inh = t'.inh * D.lexval t'.syn = t'1.syn t' : $\\epsilon$ ; t'.syn = t'.inh t : D {t'.inh = D.lexval;} t' {t.syn = t'.syn;} ; t' : '*' D {t'1.inh = t'.inh * D.lexval;} t'1 {t'.syn = t'1.syn;} ; t' : e {t'.syn = t'.inh;} ; L-attributierte SDD, LL-Grammatik: Top-Down-Parsierbar (2/2) LL-Grammatik: Jede L-attributierte SDD direkt während des Top-Down-Parsens implementierbar/berechenbar\nSDT dazu:\nAktionen, die ein berechnetes Attribut des Kopfes einer Produktion berechnen, an das Ende der Produktion anfügen Aktionen, die geerbte Attribute für ein Nicht-Terminalsymbol $A$ berechnen, direkt vor dem Auftreten von $A$ im Körper der Produktion eingefügen Implementierung im rekursiven Abstieg:\nGeerbte Attribute sind Parameter für die Funktionen für die Nicht-Terminalsymbole berechnete Attribute sind Rückgabewerte dieser Funktionen. T t'(T inh) { match('*'); T t1inh = inh * match(D); return t'(t1inh); }Bison: Attribute und Aktionen Berechnete (synthesized) Attribute expr : expr '+' term { $$ = $1 + $3; } | term ; term : term '*' DIGIT { $$ = $1 * $3; } | DIGIT ; Berechnete Attribute sind der Defaultfall in Bison.\nErinnerung: Keine Typen deklariert:\nBison verwendet per Default int für alle Symbole (Terminalsymbole (Token) und Regeln). Keine Aktionen an den Regeln angegeben:\nBison nutzt die Default-Aktion $$ = $1. Diese Aktionen werden immer dann ausgeführt, wenn die rechte Seite der zugehörigen Regel/Alternative reduziert werden konnte. Geerbte (inherited) Attribute (1/2) functiondecl : returntype fname paramlist ; returntype : REAL { $$ = 1; } | INT { $$ = 2; } ; fname : IDENTIFIER; paramlist : IDENTIFIER { mksymbol($0, $-1, $1); } | paramlist IDENTIFIER { mksymbol($0, $-1, $2); } ; Geerbte (inherited) Attribute (2/2) Hier:\nreturntype und fname haben normale berechnete Attribute\nparamlist: Funktionsaufruf mit den erzeugten Werte für returntype und fname als Parameter $\\Rightarrow$ der Wert von paramlist ist ein \"geerbtes Attribut\".\nZugriff auf die Werte der Symbole auf dem Stack links vom aktuellen Symbol: $0 ist das erste Symbol links vom aktuellen (hier type), $-1 das zweite (hier class) usw. ...\nProbleme mit geerbten Attributen functiondecl : returntype fname paramlist ; functiondecl : STRING paramlist ; /* Autsch! */ ... paramlist : IDENTIFIER { mksymbol($0, $-1, $1); } | paramlist IDENTIFIER { mksymbol($0, $-1, $2); } ; Wenn vor paramlist ein STRING steht, ist $0 der Wert von STRING, nicht fname. Analog für $-1, $\\ldots$\nDies ist eine Quelle für schwer zu findende Bugs!\nTypen für geerbte Attribute functiondecl : returntype fname paramlist ; paramlist : IDENTIFIER { mksymbol($0, $-1, $1); } | paramlist IDENTIFIER { mksymbol($0, $-1, $2); } ; Achtung: Für geerbte Attribute funktioniert die Deklaration von Typen mit %type nicht mehr!\nDas Symbol, auf das man sich mit $0 bezieht, steht nicht in der Produktion, sondern im Stack. Bison kann zur Compilezeit nicht den Typ des referenzierten Symbols bestimmen. Falls oben die Typen von returntype und fname jeweils rval und fval wären, müsste man die Aktion manuell wie folgt anpassen:\nparamlist : IDENTIFIER { mksymbol($\u003cfval\u003e0, $\u003crval\u003e-1, $1); } | paramlist IDENTIFIER { mksymbol($\u003cfval\u003e0, $\u003crval\u003e-1, $2); } ; Bison und Aktionen Regeln ohne Aktion ganz rechts: die Default-Aktion ist $$ = $1; (Vorsicht: Die Typen von $$ und $1 müssen passen!)\nAktionen mitten in einer Regel:\nxxx : A { dosomething(); } B ; wird übersetzt in:\nxxx : A dummy B ; dummy : /* empty */ { dosomething(); } Da nach dem Shiften von A nicht klar ist, ob diese Regel matcht und dosomething ausgeführt werden soll, übersetzt Bison die Regel xxx in zwei Regeln, wobei dosomething() ganz rechts in der Dummy-Regel steht. dummy ist ein normales referenzierbares Symbol.\nBeispiel: xxx : A { $$ = 42; } B C { printf(\"%d\", $2); } ; =\u003e Hier wird \"42\" ausgegeben, da mit $2 auf den Wert der eingebetteten Aktion zugegriffen wird.\n$3: Der Wert von B\n$4: Der Wert von C\nBison: Konflikte durch eingebettete Aktionen xxx : a | b ; a : 'a' 'b' 'a' 'a' ; b : 'a' 'b' 'a' 'b' ; Diese Grammatik ist ohne Konflikte von Bison übersetzbar.\nxxx : a | b ; a : 'a' 'b' { dosomething(); } 'a' 'a' ; b : 'a' 'b' 'a' 'b' ; Nach dem Lesen von \"ab\" gibt es wegen des identischen Vorschauzeichens ('a') einen Shift/Reduce-Konflikt.\nWrap-Up Wrap-Up Die Typinferenz benötigt Informationen aus der Symboltabelle\nEinfache semantische Analyse: Attribute und semantische Regeln (SDD)\nUmsetzung mit SDT: Attribute und eingebettete Aktionen\nReihenfolge der Auswertung u.U. schwierig\nBestimmte SDT-Klassen können direkt beim Parsing abgearbeitet werden:\nS-attributierte SDD, LR-Grammatik: Bottom-Up-Parsierbar L-attributierte SDD, LL-Grammatik: Top-Down-Parsierbar Ansonsten werden die Attribute und eingebetteten Aktionen in den Parse-Tree, bzw. AST, integriert und bei einer (späteren) Traversierung abgearbeitet.\n",
    "description": "",
    "tags": null,
    "title": "Typen, Type Checking und Attributierte Grammatiken",
    "uri": "/frontend/semantics/attribgrammars.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24)",
    "content": "Unter dem \"Backend\" versteht man die hinteren Stufen eines Compilers, die mit der Synthese der Ausgabe beschäftigt sind. Dies sind in der Regel verschiedene Optimierungen und letztlich die Code-Generierung\nOptimierung und Datenflussanalyse Interpreter Generierung von Maschinencode (Skizze) ",
    "description": "",
    "tags": null,
    "title": "Backend",
    "uri": "/backend.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24) \u003e Backend",
    "content": "Motivation Was geschieht hier? 01 { 02 var a; 03 var b = 2; 04 b = a; 05 } Zwischencode ist eine gute Idee Eine Sprache, viele Maschinen vs. viele Sprachen, eine Maschine Hier entsteht ein Tafelbild.\n... und beides zusammen? Hier entsteht ein Tafelbild.\nZwischencode (intermediate code); hier: Drei-Adress-Code registerbasiert Formen: x = y op z, x = op z, x = y temporäre Variablen für Zwischenergebnisse bedingte und unbedingte Sprünge Pointerarithmetik für Indizierung i = 0 while(f[i] \u003e 100) i = i + 1; i = 0 L1: t1 = i * 8 t2 = f + t1 if t2 \u003c= 100 goto L2 t3 = i + 1 i = t3 goto L1 L2: ... Optimierungen Was ist Optimierung in Compilern? Verändern von Quellcode, Zwischencode oder Maschinencode eines Programms mit dem Ziel,\nLaufzeit, Speicherplatz oder Energieverbrauch zu verbessern.\nWas ist machbar? Manche Optimierungen machen den Code nur in bestimmten Fällen schneller, kleiner oder stromsparender.\nDen optimalen Code zu finden, ist oft NP-vollständig oder sogar unentscheidbar.\nHeuristiken kommen zum Einsatz.\nDer Code wird verbessert, nicht in jedem Fall optimiert, manchmal auch verschlechtert.\nDer Einsatz eines Debuggers ist meist nicht mehr möglich.\nAnforderungen an Optimierung sichere Transformationen durchführen\nmöglichst keine nachteiligen Effekte erzeugen\nOptimierung zur Übersetzungszeit vs. Optimierung zur Laufzeit Just-in-time-Compilierung (JIT), z. B. Java:\nFast alle Optimierungsmaßnahmen finden in der virtuellen Maschine zur Laufzeit statt.\nAhead-of-time-Compilierung (AOT), z. B. C:\nDer Compiler erzeugt Maschinencode, die Optimierung findet zur Übersetzungszeit statt.\nBeide haben ihre eigenen Optimierungsmöglichkeiten, es gibt aber auch Methoden, die bei beiden einsetzbar sind.\nWelcher Code wird optimiert? Algebraische Optimierung: Transformationen des Quellcodes\nMaschinenunabhängige Optimierung: Transformationen des Zwischencodes\nMaschinenabhängige Optimierung: Transformationen des Assemblercodes oder Maschinencodes\nViele Transformationen sind auf mehr als einer Ebene möglich. Wir wenden hier die meisten auf den Zwischencode an.\nWelche Arten von Transformationen sind möglich? Eliminierung unnötiger Berechnungen\nErsetzung von teuren Operationen durch kostengünstigere\nBasisblöcke und Flussgraphen Def.: Ein Basisblock ist eine Sequenz maximaler Länge von Anweisungen, die immer hintereinander ausgeführt werden.\nEin Sprungbefehl kann nur der letzte Befehl eines Basisblocks sein.\nDef.: Ein (Kontroll)Flussgraph $G = (V, E)$ ist ein Graph mit\n$V = \\lbrace B_i \\ \\vert \\ B_i \\text{ ist ein Basisblock des zu compilierenden Programms} \\rbrace$,\n$E = \\lbrace (B_i, B_j)\\ \\vert \\text{ es gibt einen Programmlauf, in dem } B_j \\text{ direkt hinter } B_i \\text{ ausgeführt wird} \\rbrace$ Häufig benutzte Strategie: Peephole-Optimierung Ein Fenster mit wenigen Zeilen Inhalt gleitet über den Quellcode, Zwischencode oder den Maschinencode. Der jeweils sichtbare Code wird mit Hilfe verschiedener Verfahren optimiert, wenn möglich.\nPeephole-Optimierung ist zunächst ein lokales Verfahren, kann aber auch auf den gesamten Kontrollflussgraphen erweitert werden.\n=\u003e Anwendung von Graphalgorithmen!\nAlgebraische Optimierung Ersetzen von Teilbäumen im AST durch andere Bäume x = x*2 =\u003e x \u003c\u003c 1 x = x + 0 // k.w. x = x * 1 // k.w. x = x*0 =\u003e x = 0 x = x*8 =\u003e x = x \u003c\u003c 3 Sei $s = 2^a + 2^b$ die Summe zweier Zweierpotenzen:\nx = n*s =\u003e (n \u003c\u003c a) + (n \u003c\u003c b) Diese Umformungen können zusätzlich mittels Peephole-Optimierung in späteren Optimierungsphasen durchgeführt werden.\nMaschinenunabhängige Optimierung Maschinenunabhängige Optimierung lokal (= innerhalb eines Basisblocks), z. B. Peephole-Optimierung\nEinige Strategien sind auch global einsetzbar (ohne die sog. Datenflussanalyse s. u.)\nglobal, braucht nicht-lokale Informationen\nmeist unter Zuhilfenahme der Datenflussanalyse Schleifenoptimierung Lokale Optimierung Constant Folding und Common Subexpression elimination \"Constant Folding\": Auswerten von Konstanten zur Compile-Zeit\nx = 6 * 7 =\u003e x = 42 if 2 \u003e 0 jump L =\u003e jump L \"Common Subexpression Elimination\"\nx = y + z ... a = y + z ersetze mit (falls in ... keine weiteren Zuweisungen an x, y, z erfolgen)\nx = y + z ... a = x Elimination redundanter Berechnungen in einem Basisblock mitels DAGs Hier werden sog. DAGs benötigt:\nEin DAG directed acyclic graph ist ein gerichteter, kreisfreier Graph.\nDAGs werden für Berechnungen in Basisblöcken generiert, um gemeinsame Teilausdrücke zu erkennen.\nBsp.: a = (b + c) * (b + c) / 2\nCopy propagation \"Copy Propagation\"\nx = y + z a = x b = 2*a ersetze mit\nx = y + z a = x b = 2*x Wenn auf a vor seiner nächsten Zuweisung nicht mehr lesend zugegriffen wird, kann a hier entfallen.\nGlobale Optimierung Control Flow und Dead Code Kontrollfluss-Optimierungen\nif debug == 1 goto L1 if debug != 1 goto L2 goto L2 print debug info L1: print debug info L2: ... L2: ... Elimination of unreachable code\ngoto L1 L1: a = b+c ... L1: a = b+c Schleifenoptimierung Loop unrolling:\nfor i = 1 to 3 print(\"1\") print(i) print(\"2\") print(\"3\") Code Hoisting:\nInvarianten vor die Schleife schieben\nx = 0 x = 0 L: a = n*7 a = n*7 x = x + a L: x = x + a if x\u003c42 jump L if x\u003c42 jump L Kombination zweier Verfahren Loop Unrolling (für eine Iteration), danach Common Subexpression Elimination\nwhile (cond) { if (cond) { body body } while (cond) { body } } Datenflussanalyse Die Datenflussanalyse (auf 3-Adress-Code) basiert auf dem Wissen der Verfügbarkeit von Variablen und Ausdrücken am Anfang oder Ende von Basisblöcken, und zwar für alle möglichen Programmläufe.\nMan unterscheidet:\nVorwärtsanalyse (in Richtung der Nachfolger eines Basisblocks)\nRückwärtsanalyse (in Richtung der Vorgänger eines Basisblocks)\nIn beiden Fällen gibt es zwei Varianten:\nany analysis: Es wird die Vereinigung von Informationen benachbarter Block berücksichtigt.\nall analysis: Es wird die Schnittmenge von Informationen benachbarter Block berücksichtigt.\nForward-any-analysis Diese Analyse wird zur Propagation von Konstanten und Variablen benutzt und bildet sukzessive Mengen von Zeilen mit Variablendefinitionen.\n$$out(B_i) = gen(B_i) \\cup (in(B_i) - kill(B_i))$$ $out(B_i)$: alle Zeilennummern von Variablendefinitionen, die am Ende von $B_i$ gültig sind\n$in(B_i)$: alle Zeilennummern von Variablendefinitionen, die am Ende von Vorgängerblöcken von $B_i$ gültig sind\n$gen(B_i)$: alle Zeilennummern von letzten Variablendefinitionen in $B_i$\n$kill(B_i)$: alle Zeilennummern von Variablendefinitionen außerhalb von $B_i$, die in $B_i$ überschrieben werden\nZunächst ist $in(B_1) = \\emptyset$, danach ist $in(B_i) = \\bigcup out(B_j)$ mit $B_j$ ist Vorgänger von $B_i$.\nForward-all-analysis Diese Analyse wird zur Berechnung verfügbarer Ausdrücke der Form $x = y\\ op\\ z$ für die Eliminierung redundanter Berechnungen benutzt und bildet sukzessive Mengen von Ausdrücken.\n$$out(B_i) = gen(B_i) \\cup (in(B_i) - kill(B_i))$$ $out(B_i)$: alle am Ende von $B_i$ verfügbaren Ausdrücke\n$in(B_i)$: alle Ausdrücke, die am Anfang von $B_i$ verfügbar sind\n$gen(B_i)$: alle in $B_i$ berechneten Ausdrücke\n$kill(B_i)$: alle Ausdrücke $x\\ op\\ y$ mit einer Definition von $x$ oder $y$ in $B_i$ und $x\\ op\\ y$ ist nicht in $B_i$\nZunächst ist $gen(B_1) = \\emptyset$, danach ist $in(B_i) = \\bigcap out(B_j)$ mit $B_j$ ist Vorgänger von $B_i$.\nBackward-any-analysis Diese Analyse dient der Ermittlung von lebenden und toten Variablen (für die Registerzuweisung) und bildet sukzessive Mengen von Variablen.\n$$in(B_i) = gen(B_i) \\cup (out(B_i) - kill(B_i))$$ $out(B_i)$: alle Variablen, die am Ende von $B_i$ lebendig sind\n$in(B_i)$: alle Variablen, die am Ende von Vorgängerblöcken von $B_i$ lebendig sind\n$gen(B_i)$: alle Variablen, deren erstes Vorkommen auf der echten Seite einer Zuweisung steht\n$kill(B_i)$: alle Variablen, denen in $B_i$ Werte zugewiesen werden.\nZunächst ist $out(B_n) = \\emptyset$, danach ist $out(B_i) = \\bigcup in(B_j)$ mit $B_j$ ist Nachfolger von $B_i$.\nBackward-all-analysis Diese Analyse wird zur Berechnung von \"very busy\" Ausdrücken der Form $x = y\\ op\\ z$, die auf allen möglichen Wegen im Flussgraphen vom aktuellen Basisblock aus mindestens einmal benutzt werden. Ausdrücke sollten dort berechnet werden, wo sie very busy sind, um den Code kürzer zu machen.\n$$in(B_i) = gen(B_i) \\cup (out(B_i) - kill(B_i))$$ $out(B_i)$: alle Ausdrücke $x\\ op\\ y$, die am Ende von $B_i$ very busy sind\n$in(B_i)$: alle Ausdrücke, die am Anfang von $B_i$ very busy sind\n$gen(B_i)$: alle in $B_i$ benutzen Ausdrücke\n$kill(B_i)$: alle Ausdrücke $x\\ op\\ y$, deren Operanden in $B_i$ nicht redefiniert werden.\nZunächst ist $out(B_n) = \\emptyset$, danach ist $out(B_i) = \\bigcap in(B_j)$ mit $B_j$ ist Nachfolger von $B_i$.\nMaschinenabhängige Optimierung Elimination redundanter Lade-, Speicher- und Sprungoperationen LD a, R0 ST R0, a // k.w. goto L1 goto L2 ... ... L1: goto L2 L1: goto L2 Register Allocation: Liveness Analysis a = b + c d = a + b e = d - 1 a, d, e können auf ein Register abgebildet werden!\nr1 = r2 + r3 r1 = r1 + r2 r1 = r1 - 1 =\u003e a und d sind nach Gebrauch \"tot\"\nBerechnung der minimal benötigten Anzahl von Registern =\u003e Liveness-Graph, Färbungsproblem für Graphen!\nEs wird ein Graph $G = (V, E)$ erzeugt mit\n$V = \\lbrace v \\ \\vert \\ v \\text{ ist eine benötigte Variable} \\rbrace$ und $E = \\lbrace (v_1, v_2)\\ \\vert \\ v_1 \\text{ und } v_2 \\text{ sind zur selben Zeit \"lebendig\"} \\rbrace$\nHeuristisch wird jetzt die minimale Anzahl von Farben für Knoten bestimmt, bei der benachbarte Knoten nicht dieselbe Farbe bekommen.\n=\u003e Das Ergebnis ist die Zahl der benötigten Register.\nUnd wenn man nicht so viele Register zur Verfügung hat? Registerinhalte temporär in den Speicher auslagern (\"Spilling\").\nKandidaten dafür werden mit Heuristiken gefunden, z. B. Register mit vielen Konflikten (= Kanten) oder Register mit selten genutzten Variablen.\nIn Schleifen genutzte Variablen werden eher nicht ausgelagert.\nOptimierung zur Reduzierung des Energieverbrauchs Energieverbrauch verschiedener Maschinenbefehle Maschinenoperationen, die nur auf Registern arbeiten, verbrauchen die wenigste Energie.\nOperationen, die nur lesend auf Speicherzellen zugreifen, verbrauchen ca. ein Drittel mehr Energie.\nOperationen, die Speicherzellen beschreiben, benötigen zwei Drittel mehr Energie als die Operationen ausschließlich auf Register.\nEnergieeinsparung durch laufzeitbezogene Optimierung Kürzere Programmlaufzeiten führen in der Regel auch zu Energieeinsparungen.\ngcc -O1 spart 2% bis 70% (durchschnittlich 20%) Energie\nUmgekehrt: Energiebezogene Optimierung führt in der Regel zu kürzeren Laufzeiten.\nProzessorspannung variieren Viele Prozessoren ermöglichen es, die Betriebsspannung per Maschinenbefehl zu verändern.\nEine höhere Spannung bewirkt eine proportionale Steigerung der Prozessorgeschwindigkeit und des fließenden Stroms, aber einen quadratischen Anstieg des Energieverbrauchs. $(P = U \\times I, U = R \\times I)$\nFolgendes kann man ausnutzen:\nDie Verringerung der Spannung um 20% führt zu einer um 20% geringeren Prozessorgeschwindigkeit, d. h. das Programm braucht 25% mehr Zeit, verbraucht aber 36% $(1-(1-0,2)^2)$ weniger Energie.\n=\u003e Wenn das Programm durch Optimierung um 25% schneller wird und die Prozessorspannung um 20% verringert wird, verändert sich die Laufzeit des Programms nicht, man spart aber 36% Energie.\nWrap-Up Wrap-Up Verschiedene Optimierungsverfahren auf verschiedenen Ebenen, Peephole Datenflussanalyse Senkung des Energieverbrauchs durch Optimierung ",
    "description": "",
    "tags": null,
    "title": "Optimierung und Datenflussanalyse",
    "uri": "/backend/optimization.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24) \u003e Backend",
    "content": "Ein Interpreter erzeugt keinen Code, sondern führt Source-Code (interaktiv) aus. Die einfachste Möglichkeit ist der Einsatz von attributierten Grammatiken, wo der Code bereits beim Parsen ausgeführt wird. Mehr Möglichkeiten hat man dagegen bei der Traversierung des AST, beispielsweise mit dem Visitor-Pattern. (Register- und Stack-basierte Interpreter betrachten wir im Rahmen der Veranstaltung aktuell nicht.)\nSyntaxgesteuerte Interpreter AST-basierte Interpreter: Basics AST-basierte Interpreter: Funktionen und Klassen Garbage Collection ",
    "description": "",
    "tags": null,
    "title": "Interpreter",
    "uri": "/backend/interpretation.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24) \u003e Backend \u003e Interpreter",
    "content": "Überblick Interpreter Beim Interpreter durchläuft der Sourcecode nur das Frontend, also die Analyse. Es wird kein Code erzeugt, stattdessen führt der Interpreter die Anweisungen im AST bzw. IC aus. Dazu muss der Interpreter mit den Eingabedaten beschickt werden.\nEs gibt verschiedene Varianten, beispielsweise:\nSyntaxgesteuerte Interpreter\nEinfachste Variante, wird direkt im Parser mit abgearbeitet Keine Symboltabellen, d.h. auch keine Typprüfung oder Vorwärtsdeklarationen o.ä. (d.h. erlaubt nur vergleichsweise einfache Sprachen) Beispiel: siehe nächste Folie AST-basierte Interpreter\nNutzt den AST und Symboltabellen Beispiel: siehe weiter unten Stack-basierte Interpreter\nSimuliert eine Stack Machine, d.h. hält alle (temporären) Werte auf einem Stack Arbeitet typischerweise auf bereits stark vereinfachtem Zwischencode (IR), etwa Bytecode Register-basierte Interpreter\nSimuliert eine Register Machine, d.h. hält alle (temporären) Werte in virtuellen Prozessor-Registern Arbeitet typischerweise auf bereits stark vereinfachtem Zwischencode (IR), etwa Bytecode Weiterhin kann man Interpreter danach unterscheiden, ob sie interaktiv sind oder nicht. Python kann beispielsweise direkt komplette Dateien verarbeiten oder interaktiv Eingaben abarbeiten. Letztlich kommen dabei aber die oben dargestellten Varianten zum Einsatz.\nSyntaxgesteuerte Interpreter: Attributierte Grammatiken s : expr {System.err.println($expr.v);} ; expr returns [int v] : e1=expr '*' e2=expr {$v = $e1.v * $e2.v;} | e1=expr '+' e2=expr {$v = $e1.v + $e2.v;} | DIGIT {$v = $DIGIT.int;} ; DIGIT : [0-9] ;Die einfachste Form des Interpreters wird direkt beim Parsen ausgeführt und kommt ohne AST aus. Der Nachteil ist, dass der AST dabei nicht vorverarbeitet werden kann, insbesondere entfallen semantische Prüfungen weitgehend.\nÜber returns [int v] fügt man der Regel expr ein Attribut v (Integer) hinzu, welches man im jeweiligen Kontext abfragen bzw. setzen kann (agiert als Rückgabewert der generierten Methode). Auf diesen Wert kann in den Aktionen mit $v zugegriffen werden.\nDa in den Alternativen der Regel expr jeweils zwei \"Aufrufe\" dieser Regel auftauchen, muss man per \"e1=expr\" bzw. \"e2=expr\" eindeutige Namen für die \"Aufrufe\" vergeben, hier e1 und e2.\nEingebettete Aktionen in ANTLR I Erinnerung: ANTLR generiert einen LL-Parser, d.h. es wird zu jeder Regel eine entsprechende Methode generiert.\nAnalog zum Rückgabewert der Regel (Methode) expr() kann auf die Eigenschaften der Token und Sub-Regeln zugegriffen werden: $name.eigenschaft. Dabei gibt es bei Token Eigenschaften wie text (gematchter Text bei Token), type (Typ eines Tokens), int (Integerwert eines Tokens, entspricht Integer.valueOf($Token.text)). Parser-Regeln haben u.a. ein text-Attribut und ein spezielles Kontext-Objekt (ctx).\nDie allgemeine Form lautet:\nrulename[args] returns [retvals] locals [localvars] : ... ; Dabei werden die in \"[...]\" genannten Parameter mit Komma getrennt (Achtung: Abhängig von Zielsprache!).\nBeispiel:\nadd[int x] returns [int r] : '+=' INT {$r = $x + $INT.int;} ;Eingebettete Aktionen in ANTLR II @members { int count = 0; } expr returns [int v] @after {System.out.println(count);} : e1=expr '*' e2=expr {$v = $e1.v * $e2.v; count++;} | e1=expr '+' e2=expr {$v = $e1.v + $e2.v; count++;} | DIGIT {$v = $DIGIT.int;} ; DIGIT : [0-9] ;Mit @members { ... } können im generierten Parser weitere Attribute angelegt werden, die in den Regeln normal genutzt werden können.\nDie mit @after markierte Aktion wird am Ende der Regel list ausgeführt. Analog existiert @init.\nANTLR: Traversierung des AST und Auslesen von Kontext-Objekten Mit dem obigen Beispiel, welches dem Einsatz einer L-attributierten SDD in ANTLR entspricht, können einfache Aufgaben bereits beim Parsen erledigt werden.\nFür den etwas komplexeren Einsatz von attributierten Grammatiken kann man die von ANTLR erzeugten Kontext-Objekte für die einzelnen AST-Knoten nutzen und über den AST mit dem Visitor- oder dem Listener-Pattern iterieren.\nDie Techniken sollen im Folgenden kurz vorgestellt werden.\nANTLR: Kontext-Objekte für Parser-Regeln s : expr {List\u003cEContext\u003e x = $expr.ctx.e();} ; expr : e '*' e ; Jede Regel liefert ein passend zu dieser Regel generiertes Kontext-Objekt zurück. Darüber kann man das/die Kontextobjekt(e) der Sub-Regeln abfragen.\nDie Regel s() liefert entsprechend ein SContext-Objekt und die Regel expr() liefert ein ExprContext-Objekt zurück.\nIn der Aktion fragt man das Kontextobjekt über ctx ab.\nFür einfache Regel-Aufrufe liefert die parameterlose Methode nur ein einziges Kontextobjekt (statt einer Liste) zurück.\nAnmerkung: ANTLR generiert nur dann Felder für die Regel-Elemente im Kontextobjekt, wenn diese in irgendeiner Form referenziert werden. Dies kann beispielsweise durch Benennung (Definition eines Labels, siehe nächste Folie) oder durch Nutzung in einer Aktion (siehe obiges Beispiel) geschehen.\nANTLR: Benannte Regel-Elemente oder Alternativen stat : 'return' value=e ';' # Return | 'break' ';' # Break ;public static class StatContext extends ParserRuleContext { ... } public static class ReturnContext extends StatContext { public EContext value; public EContext e() { ... } } public static class BreakContext extends StatContext { ... }Mit value=e wird der Aufruf der Regel e mit dem Label value belegt, d.h. man kann mit $e.text oder $value.text auf das text-Attribut von e zugreifen. Falls es in einer Produktion mehrere Aufrufe einer anderen Regel gibt, muss man für den Zugriff auf die Attribute eindeutige Label vergeben.\nAnalog wird für die beiden Alternativen je ein eigener Kontext erzeugt.\nANTLR: Arbeiten mit dem Listener-Pattern ANTLR (generiert auf Wunsch) zur Grammatik passende Listener (Interface und leere Basisimplementierung). Beim Traversieren mit dem Default-ParseTreeWalker wird der Parse-Tree mit Tiefensuche abgelaufen und jeweils beim Eintritt in bzw. beim Austritt aus einen/m Knoten der passende Listener mit dem passenden Kontext-Objekt aufgerufen.\nDamit kann man die Grammatik \"für sich\" halten, d.h. unabhängig von einer konkreten Zielsprache und die Aktionen über die Listener (oder Visitors, s.u.) ausführen.\nexpr : e1=expr '*' e2=expr # MULT | e1=expr '+' e2=expr # ADD | DIGIT # ZAHL ;ANTLR kann zu dieser Grammatik einen passenden Listener (Interface calcListener) generieren. Weiterhin generiert ANTLR eine leere Basisimplementierung (Klasse calcBaseListener):\nVon dieser Basisklasse leitet man einen eigenen Listener ab und implementiert die Methoden, die man benötigt.\npublic static class MyListener extends calcBaseListener { Stack\u003cInteger\u003e stack = new Stack\u003cInteger\u003e(); public void exitMULT(calcParser.MULTContext ctx) { int right = stack.pop(); int left = stack.pop(); stack.push(left * right); // {$v = $e1.v * $e2.v;} } public void exitADD(calcParser.ADDContext ctx) { int right = stack.pop(); int left = stack.pop(); stack.push(left + right); // {$v = $e1.v + $e2.v;} } public void exitZAHL(calcParser.ZAHLContext ctx) { stack.push(Integer.valueOf(ctx.DIGIT().getText())); } }Anschließend baut man das alles in eine Traversierung des Parse-Trees ein:\npublic class TestMyListener { public static class MyListener extends calcBaseListener { ... } public static void main(String[] args) throws Exception { calcLexer lexer = new calcLexer(CharStreams.fromStream(System.in)); CommonTokenStream tokens = new CommonTokenStream(lexer); calcParser parser = new calcParser(tokens); ParseTree tree = parser.s(); // Start-Regel System.out.println(tree.toStringTree(parser)); ParseTreeWalker walker = new ParseTreeWalker(); MyListener eval = new MyListener(); walker.walk(eval, tree); System.out.println(eval.stack.pop()); } } Beispiel: TestMyListener.java und calc.g4 ANTLR: Arbeiten mit dem Visitor-Pattern ANTLR (generiert ebenfalls auf Wunsch) zur Grammatik passende Visitoren (Interface und leere Basisimplementierung). Hier muss man allerdings selbst für eine geeignete Traversierung des Parse-Trees sorgen. Dafür hat man mehr Freiheiten im Vergleich zum Listener-Pattern, insbesondere im Hinblick auf Rückgabewerte.\nexpr : e1=expr '*' e2=expr # MULT | e1=expr '+' e2=expr # ADD | DIGIT # ZAHL ;ANTLR kann zu dieser Grammatik einen passenden Visitor (Interface calcVisitor\u003cT\u003e) generieren. Weiterhin generiert ANTLR eine leere Basisimplementierung (Klasse calcBaseVisitor\u003cT\u003e):\nVon dieser Basisklasse leitet man einen eigenen Visitor ab und überschreibt die Methoden, die man benötigt. Wichtig ist, dass man selbst für das \"Besuchen\" der Kindknoten sorgen muss (rekursiver Aufruf der geerbten Methode visit()).\npublic static class MyVisitor extends calcBaseVisitor\u003cInteger\u003e { public Integer visitMULT(calcParser.MULTContext ctx) { return visit(ctx.e1) * visit(ctx.e2); // {$v = $e1.v * $e2.v;} } public Integer visitADD(calcParser.ADDContext ctx) { return visit(ctx.e1) + visit(ctx.e2); // {$v = $e1.v + $e2.v;} } public Integer visitZAHL(calcParser.ZAHLContext ctx) { return Integer.valueOf(ctx.DIGIT().getText()); } }Anschließend baut man das alles in eine manuelle Traversierung des Parse-Trees ein:\npublic class TestMyVisitor { public static class MyVisitor extends calcBaseVisitor\u003cInteger\u003e { ... } public static void main(String[] args) throws Exception { calcLexer lexer = new calcLexer(CharStreams.fromStream(System.in)); CommonTokenStream tokens = new CommonTokenStream(lexer); calcParser parser = new calcParser(tokens); ParseTree tree = parser.s(); // Start-Regel System.out.println(tree.toStringTree(parser)); MyVisitor eval = new MyVisitor(); System.out.println(eval.visit(tree)); } } Beispiel: TestMyVisitor.java und calc.g4 Wrap-Up Interpreter simulieren die Programmausführung\nSyntaxgesteuerter Interpreter (attributierte Grammatiken)\nBeispiel ANTLR: Eingebettete Aktionen, Kontextobjekte, Visitors/Listeners (AST-Traversierung)\n",
    "description": "",
    "tags": null,
    "title": "Syntaxgesteuerte Interpreter",
    "uri": "/backend/interpretation/syntaxdriven.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24) \u003e Backend \u003e Interpreter",
    "content": "Aufgaben im Interpreter Im Allgemeinen reichen einfache syntaxgesteuerte Interpreter nicht aus. Normalerweise simuliert ein Interpreter die Ausführung eines Programms durch den Computer. D.h. der Interpreter muss über die entsprechenden Eigenschaften verfügen: Prozessor, Code-Speicher, Datenspeicher, Stack ...\nint x = 42; int f(int x) { int y = 9; return y+x; } x = f(x); Aufbauen des AST ... =\u003e Lexer+Parser\nAuflösen von Symbolen/Namen ... =\u003e Symboltabellen, Resolving\nType-Checking und -Inference ... =\u003e Semantische Analyse (auf Symboltabellen)\nSpeichern von Daten: Name+Wert vs. Adresse+Wert (Erinnerung: Data-Segment und Stack im virtuellen Speicher)\nAusführen von Anweisungen Text-Segment im virtuellen Speicher; hier über den AST\nAufruf von Funktionen und Methoden Kontextwechsel nötig: Was ist von wo aus sichtbar?\nAST-basierte Interpreter: Visitor-Dispatcher def eval(self, AST t): if t.type == Parser.BLOCK : block(t) elif t.type == Parser.ASSIGN : assign(t) elif t.type == Parser.RETURN : ret(t) elif t.type == Parser.IF : ifstat(t) elif t.type == Parser.CALL : return call(t) elif t.type == Parser.ADD : return add(t) elif t.type == Parser.MUL : return mul(t) elif t.type == Parser.INT : return Integer.parseInt(t.getText()) elif t.type == Parser.ID : return load(t) else : ... # catch unhandled node types return None;Nach dem Aufbau des AST durch Scanner und Parser und der semantischen Analyse anhand der Symboltabellen müssen die Ausdrücke (expressions) und Anweisungen (statements) durch den Interpreter ausgewertet werden. Eine Möglichkeit dazu ist das Traversieren des AST mit dem Visitor-Pattern. Basierend auf dem Typ des aktuell betrachteten AST-Knotens wird entschieden, wie damit umgegangen werden soll. Dies erinnert an den Aufbau der Symboltabellen ...\nDie eval()-Methode bildet das Kernstück des (AST-traversierenden) Interpreters. Hier wird passend zum aktuellen AST-Knoten die passende Methode des Interpreters aufgerufen.\nHinweis: Im obigen Beispiel wird nicht zwischen der Auswertung von Ausdrücken und Anweisungen unterschieden, es wird die selbe Methode eval() genutzt. Allerdings liefern Ausdrücke einen Wert zurück (erkennbar am return im jeweiligen switch/case-Zweig), während Anweisungen keinen Wert liefern.\nIn den folgenden Beispielen wird davon ausgegangen, dass ein komplettes Programm eingelesen, geparst, vorverarbeitet und dann interpretiert wird.\nFür einen interaktiven Interpreter würde man in einer Schleife die Eingaben lesen, parsen und vorverarbeiten und dann interpretieren. Dabei würde jeweils der AST und die Symboltabelle ergänzt, damit die neuen Eingaben auf frühere verarbeitete Eingaben zurückgreifen können. Durch die Form der Schleife \"Einlesen -- Verarbeiten -- Auswerten\" hat sich auch der Name \"Read-Eval-Loop\" bzw. \"Read-Eval-Print-Loop\" (REPL) eingebürgert.\nAuswertung von Literalen und Ausdrücken Typen mappen: Zielsprache =\u003e Implementierungssprache\nDie in der Zielsprache verwendeten (primitiven) Typen müssen auf passende Typen der Sprache, in der der Interpreter selbst implementiert ist, abgebildet werden.\nBeispielsweise könnte man den Typ nil der Zielsprache auf den Typ null des in Java implementierten Interpreters abbilden, oder den Typ number der Zielsprache auf den Typ Double in Java mappen.\nLiterale auswerten:\nINT: [0-9]+ ;elif t.type == Parser.INT : return Integer.parseInt(t.getText())Das ist der einfachste Teil ... Die primitiven Typen der Zielsprache, für die es meist ein eigenes Token gibt, müssen als Datentyp der Interpreter-Programmiersprache ausgewertet werden.\nAusdrücke auswerten:\nadd: e1=expr \"+\" e2=expr ;def add(self, AST t): lhs = eval(t.e1()) rhs = eval(t.e2()) return (double)lhs + (double)rhs # Semantik!Die meisten möglichen Fehlerzustände sind bereits durch den Parser und bei der semantischen Analyse abgefangen worden. Falls zur Laufzeit die Auswertung der beiden Summanden keine Zahl ergibt, würde eine Java-Exception geworfen, die man an geeigneter Stelle fangen und behandeln muss. Der Interpreter soll sich ja nicht mit einem Stack-Trace verabschieden, sondern soll eine Fehlermeldung präsentieren und danach normal weiter machen ...\nKontrollstrukturen ifstat: 'if' expr 'then' s1=stat ('else' s2=stat)? ;def ifstat(self, AST t): if eval(t.expr()): eval(t.s1()) else: if t.s2(): eval(t.s2())Analog können die anderen bekannten Kontrollstrukturen umgesetzt werden, etwa switch/case, while oder for.\nDabei können erste Optimierungen vorgenommen werden: Beispielsweise könnten for-Schleifen im Interpreter in while-Schleifen transformiert werden, wodurch im Interpreter nur ein Schleifenkonstrukt implementiert werden müsste.\nZustände: Auswerten von Anweisungen int x = 42; float y; { int x; x = 1; y = 2; { int y = x; } } Das erinnert nicht nur zufällig an den Aufbau der Symboltabellen :-)\nUnd so lange es nur um Variablen ginge, könnte man die Symboltabellen für das Speichern der Werte nutzen. Allerdings müssen wir noch Funktionen und Strukturen bzw. Klassen realisieren, und spätestens dann kann man die Symboltabelle nicht mehr zum Speichern von Werten einsetzen. Also lohnt es sich, direkt neue Strukturen für das Halten von Variablen und Werten aufzubauen.\nDetail: Felder im Interpreter Eine mögliche Implementierung für einen Interpreter basierend auf einem ANTLR-Visitor ist nachfolgend gezeigt.\nHinweis: Bei der Ableitung des BaseVisitor\u003cT\u003e muss der Typ T festgelegt werden. Dieser fungiert als Rückgabetyp für die Visitor-Methoden. Entsprechend können alle Methoden nur einen gemeinsamen (Ober-) Typ zurückliefern, weshalb man sich an der Stelle oft mit Object behilft und dann manuell den konkreten Typ abfragen und korrekt casten muss.\nclass Interpreter(BaseVisitor\u003cObject\u003e): __init__(self, AST t): BaseVisitor\u003cObject\u003e.__init__(self) self.root = t self.env = Environment()Quelle: Eigener Code basierend auf einer Idee nach Interpreter.java by Bob Nystrom on Github.com (MIT)\nAusführen einer Variablendeklaration varDecl: \"var\" ID (\"=\" expr)? \";\" ;def varDecl(self, AST t): # deklarierte Variable (String) name = t.ID().getText() value = None; # TODO: Typ der Variablen beachten (Defaultwert) if t.expr(): value = eval(t.expr()) self.env.define(name, value) return NoneWenn wir bei der Traversierung des AST mit eval() bei einer Variablendeklaration vorbeikommen, also etwa int x; oder int x = wuppie + fluppie;, dann wird im aktuellen Environment der String \"x\" sowie der Wert (im zweiten Fall) eingetragen.\nAusführen einer Zuweisung assign: ID \"=\" expr;def assign(self, AST t): lhs = t.ID().getText() value = eval(t.expr()) self.env.assign(lhs, value) # Semantik! } class Environment: def assign(self, String n, Object v): if self.values[n]: self.values[n] = v elif self.enclosing: self.enclosing.assign(n, v) else: raise RuntimeError(n, \"undefined variable\")Quelle: Eigener Code basierend auf einer Idee nach Environment.java by Bob Nystrom on Github.com (MIT)\nWenn wir bei der Traversierung des AST mit eval() bei einer Zuweisung vorbeikommen, also etwa x = 7; oder x = wuppie + fluppie;, dann wird zunächst im aktuellen Environment die rechte Seite der Zuweisung ausgewertet (Aufruf von eval()). Anschließend wird der Wert für die Variable im Environment eingetragen: Entweder sie wurde im aktuellen Environment früher bereits definiert, dann wird der neue Wert hier eingetragen. Ansonsten wird entlang der Verschachtelungshierarchie gesucht und entsprechend eingetragen. Falls die Variable nicht gefunden werden kann, wird eine Exception ausgelöst.\nAn dieser Stelle kann man über die Methode assign in der Klasse Environment dafür sorgen, dass nur bereits deklarierte Variablen zugewiesen werden dürfen. Wenn man stattdessen wie etwa in Python das implizite Erzeugen neuer Variablen erlaubten möchte, würde man statt Environment#assign einfach Environment#define nutzen ...\nAnmerkung: Der gezeigte Code funktioniert nur für normale Variablen, nicht für Zugriffe auf Attribute einer Struct oder Klasse!\nBlöcke: Umgang mit verschachtelten Environments block: '{' stat* '}' ;def block(self, AST t): prev = self.env try: self.env = Environment(self.env) for s in t.stat(): eval(s) finally: self.env = prev return None;Quelle: Eigener Code basierend auf einer Idee nach Interpreter.java by Bob Nystrom on Github.com (MIT)\nBeim Interpretieren von Blöcken muss man einfach nur eine weitere Verschachtelungsebene für die Environments anlegen und darin dann die Anweisungen eines Blockes auswerten ...\nWichtig: Egal, was beim Auswerten der Anweisungen in einem Block passiert: Es muss am Ende die ursprüngliche Umgebung wieder hergestellt werden (finally-Block).\nWrap-Up Interpreter simulieren die Programmausführung\nNamen und Symbole auflösen Speicherbereiche simulieren Code ausführen: Read-Eval-Loop Traversierung des AST: eval(AST t) als Visitor-Dispatcher\nScopes mit Environment (analog zu Symboltabellen)\nInterpretation von Blöcken und Variablen (Deklaration, Zuweisung)\n",
    "description": "",
    "tags": null,
    "title": "AST-basierte Interpreter: Basics",
    "uri": "/backend/interpretation/astdriven-part1.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24) \u003e Backend \u003e Interpreter",
    "content": "Funktionen int foo(int a, int b, int c) { print a + b + c; } foo(1, 2, 3); def makeCounter(): var i = 0 def count(): i = i + 1 print i return count; counter = makeCounter() counter() # \"1\" counter() # \"2\" Die Funktionsdeklaration muss im aktuellen Kontext abgelegt werden, dazu wird der AST-Teilbaum der Deklaration benötigt.\nBeim Aufruf muss man das Funktionssymbol im aktuellen Kontext suchen, die Argumente auswerten, einen neuen lokalen Kontext anlegen und darin die Parameter definieren (mit den eben ausgewerteten Werten) und anschließend den AST-Teilbaum des Funktionskörpers im Interpreter mit eval() auswerten ...\nAusführen einer Funktionsdeklaration funcDecl : type ID '(' params? ')' block ; funcCall : ID '(' exprList? ')' ; def funcDecl(self, AST t): fn = Fun(t, self.env) self.env.define(t.ID().getText(), fn) Quelle: Eigener Code basierend auf einer Idee nach LoxFunction.java by Bob Nystrom on Github.com (MIT)\nMan definiert im aktuellen Environment den Funktionsnamen und hält dazu den aktuellen Kontext (aktuelles Environment) sowie den AST-Knoten mit der eigentlichen Funktionsdefinition fest.\nFür Closures ist der aktuelle Kontext wichtig, sobald man die Funktion ausführen muss. In [Parr2010, S.236] wird beispielsweise einfach nur ein neuer Memory-Space (entspricht ungefähr hier einem neuen lokalen Environment) angelegt, in dem die im Funktionskörper definierten Symbole angelegt werden. Die Suche nach Symbolen erfolgt dort nur im Memory-Space (Environment) der Funktion bzw. im globalen Scope (Environment).\nAusführen eines Funktionsaufrufs funcDecl : type ID '(' params? ')' block ; funcCall : ID '(' exprList? ')' ;def funcCall(self, AST t): fn = (Fun)eval(t.ID()) args = [eval(a) for a in t.exprList()] prev = self.env; self.env = Environment(fn.closure) for i in range(args.size()): self.env.define(fn.decl.params()[i].getText(), args[i]) eval(fn.decl.block()) self.env = prevQuelle: Eigener Code basierend auf einer Idee nach LoxFunction.java by Bob Nystrom on Github.com (MIT)\nZunächst wird die ID im aktuellen Kontext ausgewertet. In der obigen Grammatik ist dies tatsächlich nur ein Funktionsname, aber man könnte über diesen Mechanismus auch Ausdrücke erlauben und damit Funktionspointer bzw. Funktionsreferenzen realisieren ... Im Ergebnis hat man das Funktionsobjekt mit dem zugehörigen AST-Knoten und dem Kontext zur Deklarationszeit.\nDie Argumente der Funktion werden nacheinander ebenfalls im aktuellen Kontext ausgewertet.\nUm den Funktionsblock auszuwerten, legt man einen neuen temporären Kontext über dem Closure-Kontext der Funktion an und definiert darin die Parameter der Funktion samt den aktuellen Werten. Dann lässt man den Interpreter über den Visitor-Dispatch den Funktionskörper evaluieren und schaltet wieder auf den Kontext vor der Funktionsauswertung zurück.\nFunktionsaufruf: Rückgabewerte def funcCall(self, AST t): ... eval(fn.decl.block()) ... return None # (Wirkung) class ReturnEx(RuntimeException): __init__(self, v): self.value = v def return(self, AST t): raise ReturnEx(eval(t.expr())) def funcCall(self, AST t): ... erg = None try: eval(fn.decl.block()) except ReturnEx as r: erg = r.value ... return erg; Quelle: Eigener Code basierend auf einer Idee nach Return.java und LoxFunction.java by Bob Nystrom on Github.com (MIT)\nRückgabewerte für den Funktionsaufruf werden innerhalb von block berechnet, wo eine Reihe von Anweisungen interpretiert werden, weshalb block ursprünglich keinen Rückgabewert hat. Im Prinzip könnte man block etwas zurück geben lassen, was durch die möglicherweise tiefe Rekursion relativ umständlich werden kann.\nAn dieser Stelle kann man den Exceptions-Mechanismus missbrauchen und bei der Auswertung eines return mit dem Ergebniswert direkt zum Funktionsaufruf zurück springen. In Methoden, wo man einen neuen lokalen Kontext anlegt und die globale env-Variable temporär damit ersetzt, muss man dann ebenfalls mit try/catch arbeiten und im finally-Block die Umgebung zurücksetzen und die Exception erneut werfen.\nNative Funktionen class Callable: def call(self, Interpreter i, List\u003cObject\u003e a): pass class Fun(Callable): ... class NativePrint(Fun): def call(self, Interpreter i, List\u003cObject\u003e a): for o in a: print a # nur zur Demo, hier sinnvoller Code :-) # Im Interpreter (Initialisierung): self.env.define(\"print\", NativePrint()) def funcCall(self, AST t): ... # prev = self.env; self.env = Environment(fn.closure) # for i in range(args.size()): ... # eval(fn.decl.block()); self.env = prev fn.call(self, args) ...Quelle: Eigener Code basierend auf einer Idee nach LoxCallable.java und LoxFunction.java by Bob Nystrom on Github.com (MIT)\nNormalerweise wird beim Interpretieren eines Funktionsaufrufs der Funktionskörper (repräsentiert durch den entsprechenden AST-Teilbaum) durch einen rekursiven Aufruf von eval ausgewertet.\nFür native Funktionen, die im Interpreter eingebettet sind, klappt das nicht mehr, da hier kein AST vorliegt.\nMan erstellt ein neues Interface Callable mit der Hauptmethode call() und leitet die frühere Klasse Fun davon ab: class Fun(Callable). Die Methode funcCall() des Interpreters ruft nun statt der eval()-Methode die call()-Methode des Funktionsobjekts auf und übergibt den Interpreter (== Zustand) und die Argumente. Die call()-Methode der Klasse Fun muss nun ihrerseits im Normalfall den im Funktionsobjekt referenzierten AST-Teilbaum des Funktionskörpers mit dem Aufruf von eval() interpretieren ...\nFür die nativen Funktionen leitet man einfach eine (anonyme) Klasse ab und speichert sie unter dem gewünschten Namen im globalen Kontext des Interpreters. Die call()-Methode wird dann entsprechend der gewünschten Funktion implementiert, d.h. hier erfolgt kein weiteres Auswerten des AST.\nKlassen und Instanzen I classDef : \"class\" ID \"{\" funcDecl* \"}\" ;def classDef(self, AST t): methods = HashMap\u003cString, Fun\u003e() for m in t.funcDecl(): fn = Fun(m, self.env) methods[m.ID().getText()] = fn clazz = Clazz(methods) self.env.define(t.ID().getText(), clazz)Quelle: Eigener Code basierend auf einer Idee nach Interpreter.java by Bob Nystrom on Github.com (MIT)\nAnmerkung: In dieser Darstellung wird der Einfachheit halber nur auf Methoden eingegangen. Für Attribute müssten ähnliche Konstrukte implementiert werden.\nKlassen und Instanzen II class Clazz(Callable): __init__(self, Map\u003cString, Fun\u003e methods): self.methods = methods def call(self, Interpreter i, List\u003cObject\u003e a): return Instance(self) def findMethod(self, String name): return self.methods[name] class Instance: __init__(self, Clazz clazz): self.clazz = clazz def get(self, String name): method = self.clazz.findMethod(name) if method != None: return method.bind(self) raise RuntimeError(name, \"undefined method\")Quelle: Eigener Code basierend auf einer Idee nach LoxClass.java und LoxInstance.java by Bob Nystrom on Github.com (MIT)\nInstanzen einer Klasse werden durch den funktionsartigen \"Aufruf\" der Klassen angelegt (parameterloser Konstruktor). Eine Instanz hält die Attribute (hier nicht gezeigt) und eine Referenz auf die Klasse, um später an die Methoden heranzukommen.\nZugriff auf Methoden (und Attribute) getExpr : obj \".\" ID ;def getExpr(self, AST t): obj = eval(t.obj()) if isinstance(obj, Instance): return ((Instance)obj).get(t.ID().getText()) raise RuntimeError(t.obj().getText(), \"no object\")Beim Zugriff auf Attribute muss das Objekt im aktuellen Kontext evaluiert werden. Falls es eine Instanz von Instance ist, wird auf das Feld per interner Hash-Map zugriffen; sonst Exception.\nMethoden und this oder self class Fun(Callable): def bind(self, Instance i): e = Environment(self.closure) e.define(\"this\", i) e.define(\"self\", i) return Fun(self.decl, e)Quelle: Eigener Code basierend auf einer Idee nach LoxFunction.java by Bob Nystrom on Github.com (MIT)\nNach dem Interpretieren von Klassendefinitionen sind die Methoden in der Klasse selbst gespeichert, wobei der jeweilige closure auf den Klassenkontext zeigt.\nBeim Auflösen eines Methodenaufrufs wird die gefundene Methode an die Instanz gebunden, d.h. es wird eine neue Funktion angelegt, deren closure auf den Kontext der Instanz zeigt. Zusätzlich wird in diesem Kontext noch die Variable \"this\" definiert, damit man damit auf die Instanz zugreifen kann.\nIn Python wird das in der Methodensignatur sichtbar: Der erste Parameter ist eine Referenz auf die Instanz, auf der diese Methode ausgeführt werden soll ...\nWrap-Up Interpreter simulieren die Programmausführung\nNamen und Symbole auflösen Speicherbereiche simulieren Code ausführen: Read-Eval-Loop Traversierung des AST: eval(AST t) als Visitor-Dispatcher\nScopes mit Environment (analog zu Symboltabellen)\nInterpretation von Funktionen (Deklaration/Aufruf, native Funktionen)\nInterpretation von Klassen und Instanzen\n",
    "description": "",
    "tags": null,
    "title": "AST-basierte Interpreter: Funktionen und Klassen",
    "uri": "/backend/interpretation/astdriven-part2.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24) \u003e Backend \u003e Interpreter",
    "content": "Ist das Code oder kann das weg? x = 42 y = 'wuppie' y = 'fluppie' print(y) def foo(): x = 'wuppie' def bar(): print(x) return f fn = foo() fn() Bei der Erzeugung von Bytecode für eine VM kann man die Konstanten direkt in einem Konstanten-Array sammeln und im Bytecode mit den entsprechenden Indizes arbeiten. Das entspricht dem Vorgehen bei der Maschinencode-Erzeugung, dort sammelt man die Konstanten typischerweise am Ende des Text-Segments.\nBei der Abarbeitung des Bytecodes durch die VM legt diese Objekte für globale und lokale Variablen, Strings sowie für Funktionen etc. an. Der Speicher dafür wird dynamisch auf dem Heap reserviert, und die Adressen beispielsweise im Stack (bei lokalen Variablen) oder in Hashtabellen (Funktionsnamen, globale Variablen) o.ä. gespeichert.\nWenn Objekte nicht mehr benötigt werden, sollten sie entsprechend wieder freigegeben werden, da sonst der Heap der VM voll läuft. Im obigen Beispiel wird der Speicher für wuppie unerreichbar, sobald man die Zuweisung y = 'fluppie' ausführt. Andererseits darf man aber auch nicht zu großzügig Objekt aufräumen: Die lokale Variable x in foo wird in der beim Aufruf erzeugten Funktion bar benötigt (Closure) und muss deshalb von der Lebensdauer wie eine globale Variable behandelt werden.\nErreichbarkeit Quelle: reachable.png by Bob Nystrom on Github.com (MIT)\nErreichbar sind zunächst alle \"Wurzeln\", d.h. alle Objekte, die direkt über den Stack oder die Konstanten-Arrays oder die Hashtabelle mit den globalen Variablen (und Funktionen) erreichbar sind.\nAlle Objekte, die von erreichbaren Objekten aus erreichbar sind, sind ebenfalls erreichbar.\n\"Objekt\" meint dabei im Zuge der Bytecode-Generierung oder während der Bearbeitung durch die VM erstellte Werte/Objekte, die auf dem Heap alloziert wurden und durch die VM aktiv freigegeben werden müssen.\n\"Präzises GC\": Mark-Sweep Garbage Collection Das führt zu einem zweistufigen Algorithmus:\nMark: Starte mit den Wurzeln und traversiere so lange durch die Objektreferenzen, bis alle erreichbaren Objekte besucht wurden. Sweep: Lösche alle anderen Objekte. Quelle: mark-sweep.png by Bob Nystrom on Github.com (MIT)\nDie Strukturen für Objekte und die VM werden ergänzt: Objekte erhalten noch ein Flag für die Markierung.\nZum Auffinden der erreichbaren Objekte wird mit einem Färbungsalgorithmus gearbeitet. Initial sind alle Objekte \"weiß\" (nicht markiert).\nPhase \"Mark\": Wurzeln markieren Im ersten Schritt färbt man alle \"Wurzeln\" \"grau\" ein. Dabei werden alle Objektreferenzen im Stack der VM, in der Hashtabelle für globale Variablen der VM, in der Konstantentabelle des Bytecode-Chunks sowie in den Funktionspointern betrachtet: Über diese Datenstrukturen wird iteriert und alle auf dem Heap der Laufzeitumgebung allozierten Strukturen/Objekte werden markiert, indem ihr Flag gesetzt wird. Zusätzlich werden die Pointer auf diese Objekte in einen \"grayStack\" hinzugefügt. Damit sind alle Wurzeln \"grau\" markiert\".\nPhase \"Mark\": Trace Nachdem alle Wurzeln \"grau\" markiert wurden und auf den grayStack der VM gelegt wurden, müssen nun mögliche Verweise in den Wurzeln verfolgt werden. Dazu entfernt man schrittweise die Objekte vom Stack und betrachtet sie damit als \"schwarz\". (Das Markierungs-Flag bleibt gesetzt, \"schwarz\" sind die \"grau\" markierten Objekte, weil sie nicht mehr auf dem grayStack der VM liegen.) Sofern das aktuell betrachtete Objekt seinerseits wieder Referenzen hat (beispielsweise haben Funktionen wieder einen Bytecode-Chunk mit einem Konstanten-Array), werden diese Referenzen iteriert und alle dabei aufgefundenen Objekte auf den grayStack der VM gelegt und ihr Flag gesetzt.\nDieser Prozess wird so lange durchgeführt, bis der grayStack leer ist. Dann sind alle erreichbaren Objekte markiert.\nPhase \"Sweep\" Jetzt sind alle erreichbaren Objekte markiert. Objekte, deren Flag nicht gesetzt ist, sind nicht mehr erreichbar und können freigegeben werden.\nWenn die Objekte nicht erreichbar sind, wie kommt man dann an diese heran?\nDie Strukturen für Objekte und die VM werden erneut ergänzt: Objekte erhalten noch einen next-Pointer, mit dem alle Objekte in einer verketteten Liste gehalten werden können.\nWann immer für ein Objekt Speicher auf dem Laufzeit-Heap angefordert wird, wird dieses Objekt in eine verkettete Liste aller Objekte der VM eingehängt. Über diese Liste wird nun iteriert und dabei werden alle \"weißen\" (nicht markierten) Objekte ausgehängt und freigegeben.\nZusätzlich müssen alle verbleibenden Objekte für den nächsten GC-Lauf wieder entfärbt werden, d.h. die Markierung muss wieder zurückgesetzt werden.\nHinweise Die Mark-and-Sweep-GC-Variante wird auch \"präzises Garbage Collection\" genannt, da dabei alle nicht mehr benötigten Objekte entfernt werden.\nDa während der Durchführung der GC die Abarbeitung des Programms pausiert wird, hat sich deshalb auch die Bezeichnung stop-the-world GC eingebürgert.\nMetriken: Latenz und Durchsatz Latenz: Längste Zeitdauer, während der das eigentliche Programm (des Users) pausiert, beispielsweise weil gerade eine Garbage Collection läuft\nDurchsatz: Verhältnis aus Zeit für den User-Code zu Zeit für Garbage Collection\nBeispiel: Ein Durchsatz von 90% bedeutet, dass 90% der Rechenzeit für den User zur Verfügung steht und 10% für GC verwendet werden.\nQuelle: latency-throughput.png by Bob Nystrom on Github.com (MIT)\nHeuristik: Self-adjusting Heap GC selten: Hohe Latenz (lange Pausen) GC oft: Geringer Durchsatz Heuristik\nBeobachte den allozierten Speicher der VM Wenn vorher festgelegte (willkürliche) Grenze überschritten: GC Größe des verbliebenen belegten Speichers mal Faktor =\u003e neue Grenze Die Objekte bzw. der Speicherverbrauch, der nach einem GC-Lauf übrig bleibt, ist ein Indikator für den aktuell nötigen Speicher. Deshalb setzt man die neue Schwelle, ab der der nächste GC-Lauf gestartet wird, ungefähr auf diesen Speicherverbrauch mal einem gewissen Faktor (beispielsweise den Wert 2), um nicht sofort wieder einen GC starten zu müssen ...\nGenerational GC Teile Heap in zwei Bereiche: \"Kinderstube\" und \"Erwachsenenbereich\" Neue Objekte werden in der Kinderstube angelegt Häufiges GC in Kinderstube Überlebende Objekte werden nach $N$ Generation in den Erwachsenenbereich verschoben Deutlich selteneres GC im Erwachsenenbereich Die meisten Objekte haben oft eher eine kurze Lebensdauer. Wenn sie aber ein gewisses \"Alter\" erreicht haben, werden sie oft noch weiterhin benötigt.\nMan teilt den Heap in zwei unterschiedlich große Bereiche auf: Die \"Kinderstube\" (Nursery) und den größeren Heap-Bereich für die \"Erwachsenen\". Neue Objekte kommen zunächst in die Kinderstube, und dort wird regelmäßig GC ausgeführt. Bei jedem GC-Lauf wird der Generationen-Zähler der \"überlebenden\" Objekte inkrementiert. Wenn die Objekte eine bestimmte Anzahl an Generationen überlebt haben, werden sie in den Erwachsenenbereich verschoben, wo deutlich seltener eine GC durchgeführt wird.\n\"Konservatives GC\": Boehm GC Man unterscheidet zusätzlich noch zwischen konservativem und präzisem GC:\nKonservatives GC geht eher vorsichtig vor: Wenn ein Speicherbereich möglicherweise noch benötigt werden könnte, wird er nicht angefasst; alles, was auch nur so aussieht wie ein Pointer wird entsprechend behandelt. Präzises GC \"weiss\" dagegen genau, welche Werte Pointer sind und welche nicht und handelt entsprechend. Boehm, Weiser und Demers: \"Boehm GC\" =\u003e Konservativer GC (Variante des Mark-and-Sweep-GC)\nIdee: Nutze die interne Verwaltung des Heaps zum Finden von Objekten Ablauf Mark: Suche alle potentiell zu bereinigenden Objekte: Inspiziere Stack, statische Daten, Prozessor-Register, ... Behandle alle gefundenen Adressen zunächst als \"unsichere Pointer\"\nEs ist noch nicht klar, ob das wirklich gültige Adressen in den Heap sind ... Prüfe alle unsicheren Pointer: Liegt die Adresse tatsächlich im Heap? Zeigt der Pointer auf den Anfang eines Blockes? Ist der Block nicht in der Free-List enthalten? =\u003e Ergebnis: gültige Pointer (\"Root-Pointer\"): markiere diese Objekte als \"erreichbar\" Wiederhole die Schritte (1) bis (3) durch die Untersuchung der gefundenen Objekte Sweep: Iteriere über den Heap (blockweise) und gebe alle belegten Blöcke frei, die nicht als \"erreichbar\" markiert wurden Exkurs Heap-Verwaltung Der Heap ist ein zusammenhängender Speicherbereich, der durch die Allokation und Freigabe von Blöcken in mehrere Blöcke segmentiert wird. Die freien Blöcke werden dabei in eine verkettete Liste \"Free-List\" (im Bild \"freemem\") eingehängt. Diese verkettete Liste wird direkt im Heap abgebildet.\nEs wird dazu eine Verwaltungsstruktur definiert, die neben Informationen wie der Größe des freien Blocks einen Pointer auf den nächsten freien Block aufweist. Jeder Speicherblock im Heap beginnt stets mit dieser Struktur, so dass alle freien Blöcke in die Freispeicherliste eingehängt werden können:\nstruct memblock { size_t size; uint marked; struct memblock *next; }; size kann die Gesamtgröße des Blockes bedeuten oder aber nur die Größe der Nutzdaten, die sich hinter der Verwaltungsstruktur befinden =\u003e letztere Deutung wird in Linux verwendet In Linux hat man eine doppelt verkettete Liste (statt wie hier nur einfach verkettet) Bei malloc durchsucht man diese Liste im Heap, bis man einen passenden Block gefunden hat. Dann setzt man den next-Pointer des Vorgängerblocks auf den Wert des eigenen next-Pointers und hängt damit den gefundenen Block aus der Freispeicherliste aus. Der next-Pointer wird ungültig gemacht, indem man ihn auf einen vordefinierten (und nur zu diesem Zweck genutzten) Wert setzt (alternativ kann man dazu ein weiteres Flag in der Struktur spendieren). Dann bestimmt man per Pointerarithmetik die Adresse des ersten Bytes hinter der Verwaltungsstruktur und liefert diese Adresse als Ergebnis (Pointer auf den Nutzbereich des Blockes) an den Aufrufer zurück. Wenn der gefundene freie Block \"viel zu groß\" ist, kann man den Block auch splitten: Einen Teil gibt man als allozierten Block an den Aufrufer zurück, den anderen Teil (den Rest) hängt man als neuen Block in die Freispeicherliste ein.\nBei einem free bekommt man den Pointer auf das erste Byte der Nutzdaten eines Speicherblockes und muss per Pointerarithmetik den Beginn der Verwaltungsstruktur des Blockes bestimmen. Dann setzt man den next-Pointer des Blockes auf den Wert des Freispeicherlisten-Pointers, und dieser wird auf die Startadresse der Verwaltungsstruktur des Blockes \"umgebogen\". Damit hat man den Block vorn in die Freispeicherliste eingehängt.\nVor- und Nachteile des konservativen GC (+) Keine explizite Kooperation mit der Speicherverwaltung nötig\nDie Speicherverwaltung muss nur eine Bedingung erfüllen: Jedes benutzte Objekt hat einen Pointer auf den Anfang des Blockes (+) Explizite Deallocation (free) ist möglich (+) Kann jederzeit abgebrochen werden\nPraktisch in Verbindung mit opportunistischer GC in interaktiven Applikationen (+) Keine separate Buchführung über alle in der VM erzeugten Objekte nötig (-) Mark-Phase dauert durch die zusätzlichen Tests länger (-) Die Möglichkeit einer Fragmentierung des Speichers ist hoch (-) Fehlinterpretationen können dafür sorgen, dass unsichere Pointer nicht freigegeben werden (-) Bei hoch optimierenden Compilern ist die GC nicht zuverlässig, da die Adressen u.U. nicht mehr auf die benutzen Objekte zeigen Reference Counting Beim Reference Counting erhält jedes Objekt einen Referenz-Zähler.\nBeim Erstellen weiterer Referenzen oder Pointer auf ein Objekt wird der Zähler entsprechend inkrementiert.\nSobald ein Objekt seinen Gültigskeitsbereich (Scope) verlässt, wird versucht, das Objekt freizugeben. Dazu wird der interne Referenz-Zähler dekrementiert. Erst wenn der Zähler dabei den Wert 0 erreicht, bedeutet dies, dass es keine weitere Referenz oder Pointer auf dieses Objekt gibt und das Objekt wird vom Heap entfernt (freigegeben). Wenn der Zähler noch größer Null ist, wird das Objekt nicht weiter verändert.\nDies ist eine einfach Form des GC, die ohne zyklische Sammelphasen auskommt. Allerdings hat diese Form ein Problem mit zyklischen Datenstrukturen.\nAlgorithmus (Skizze) def new(): obj = alloc_memory() obj.set_ref_counter(1) return objnew() wird beim Erstellen eines Objektes aufgerufen.\ndef delete(obj): obj.dec_ref_counter() if obj.get_ref_counter() == 0: for child in children(obj): delete(child) free_object(obj)delete() wird aufgerufen, wenn das Objekt nicht weiter vom Programm verwendet wird, es beispielsweise seinen Scope verlässt. Hierbei wird geprüft, ob noch anderweitig auf dieses Objekt referenziert wird.\nIm Beispiel wird delete(A) ausgeführt: Der Referenz-Zähler (RC) von A wird dekrementiert, und da er den Wert 0 erreicht, werden rekursiv die von A verwiesenen Kinder gelöscht. In B wird dabei ebenfalls der Wert 0 erreicht und delete(D) aufgerufen. Da dort der RC größer 0 bleibt, wird dessen Kind E nicht weiter beachtet. Anschließend werden A und B aus dem Speicher freigegeben.\nProbleme Das größte Problem beim Referenz Counting ist der Umgang mit zyklischen Datenstrukturen, wie verkettete Listen oder einfache Graphen. Es dazu kommen, dass zyklische Datenstrukturen nicht gelöscht und freigegeben werden können und ein Speicherverlust entsteht.\nDas folgende Beispiel erläutert dieses Problem:\nStop-and-Copy Garbage Collection Teile Heap in zwei Bereiche (A und B) Alloziere nur Speicher aus A (bis der Bereich voll ist) Stoppe Programmausführung und kopiere alle erreichbaren Objekte von A nach B Gebe gesamten Speicher in A frei Setze Programmausführung mit vertauschten Rollen von A und B fort Vor- und Nachteile von Stop-and-Copy GC (+) Nur ein Lauf über Daten nötig (+) Automatische Speicherdefragmentierung (+) Aufwand proportional zur Menge der erreichbaren Objekte und nicht zur Größe des Speichers (+) Zyklische Referenzen sind kein Problem (-) Benötigt doppelten Speicherplatz für gegebene Heap-Größe (-) Objekte werden im Speicher bewegt (Update von Referenzen nötig) (-) Programm muss für GC angehalten werden Benchmarking Ziel Vergleich von verschiedenen GC-Algorithmen Wahl des optimalen Algorithmus für konkreten Anwendungsfall Overhead durch GC möglichst gering halten Setup \"Warm up\": einige Iterationen des Benchmarks ohne Messung vorlaufen lassen, um bspw. Just-in-Time Kompilierung und Initialisierung aller Laufzeitkomponenten abzuschließen Anpassung der Heap-Größe an den Benchmark (falls Heap zu bestimmter Rate gefüllt werden soll) Überprüfung und Eliminierung von externen Faktoren, die das Ergebnis beeinflussen können andere Prozesse dynamische Frequenzskalierung der CPU Vermeidung von Lese- und Schreibzugriffen, sofern diese nicht durch Test-Infrastruktur vorgegeben sind Relevante Metriken für Tests Durchsatz (welchen Anteil hat GC an der Laufzeit?) Latenz (welche Verzögerung erzeugt GC?) Szenarien (stark vom getesteten GC-Algorithmus abhängig)\nkonstant gefüllter Heap/leerer Heap Erzeugen und Löschen von stark referenzierten Objekten/temporären Objekten Für Generational GC: Testen von Overhead von Schreib- und Lesebarrieren durch Objektreferenzierung Beispiele für GC-Benchmarks der JVM\nWrap-Up Pflege verkette Liste aller Objekte in der VM\nMark-Sweep-GC:\nMarkiere alle Wurzeln (\"grau\", aus Stack und Hashtabelle) Traversiere ausgehend von den Wurzeln alle Objekte und markiere sie Gehe die verkettete Liste aller Objekte durch und entferne alle nicht markierten Problem: Latenz und Durchsatz =\u003e Idee des \"self-adjusting\" Heaps\nVarianten/Alternativen: Generational GC, Boehm-GC, Reference Counting, Stop-and-Copy GC, ...\n",
    "description": "",
    "tags": null,
    "title": "Garbage Collection",
    "uri": "/backend/interpretation/gc.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24) \u003e Backend",
    "content": "Einordnung Quelle: A Map of the Territory (mountain.png) by Bob Nystrom on Github.com (MIT)\nDie Erzeugung von Maschinencode ist in gewisser Weise ein \"Parallelweg\" zum Erzeugen von Bytecode. Die Schwierigkeit liegt darin, die technischen Besonderheiten der Zielplattform (Register, Maschinenbefehle) gut zu kennen und sinnvoll zu nutzen.\nHäufig nutzt man als Ausgangsbasis den Drei-Adressen-Code als IR, der strukturell dem zu erzeugenden Maschinencode bereits recht ähnlich ist. Oder man macht sich die Sache einfach und generiert LLVM IR und lässt die LLVM-Toolchain übernehmen ;-)\nHier der Vollständigkeit halber ein Ausblick ...\nProzessorarchitektur Quelle: Intel i80286 arch by Appaloosa on Wikimedia.org (CC BY-SA 3.0)\nAm Beispiel der noch übersichtlichen Struktur des Intel i80286 lassen sich verschiedene Grundbausteine eines Prozessors identifizieren.\nZunächst hat man eine Ausführungseinheit (Execution Unit), die sich vor allem aus verschiedenen Registern und der Recheneinheit (ALU) zusammen setzen. Hier kann man Adressen berechnen oder eben auch Dinge wie Addition ...\nÜber die Register wird auch die Adressierung des Speichers vorgenommen. Aus Sicht eines Prozesses greift dieser auf einen zusammenhängenden, linearen Speicher zu (\"Virtueller Speicher\", siehe nächste Folie). Dieser setzt sich in der Realität aus verschiedenen Segmenten zusammen, die auch auf unterschiedliche Speichertypen (RAM, Cache, SSD, ...) verteilt sein können. In der Address Unit werden aus logischen Adressen die konkreten physikalischen Adressen berechnet.\nÜber die Bus Unit erfolgt der physikalische Zugriff auf die konkrete Hardware.\nIn der Instruction Unit wird der nächste Befehl geholt und dekodiert und zur Ausführung gebracht.\nVirtueller Speicher Kernel weist jedem Prozess seinen eigenen virtuellen Speicher zu\nLinearer Adressbereich, beginnend mit Adresse 0 bis zu einer maximalen Adresse Verwaltung durch MMU (Memory Management Unit)\nMMU bildet logische Adressen aus virtuellem Speicher auf den physikalischen Speicher ab (transparent für den Prozess) Segmente des virtuellen Speichers: Text Text Segment (read-only) Programm Code Konstanten, String Literale Bereich initialisierter Daten globale und static Variablen (explizit initialisiert) Bereich uninitialisierter Daten globale und static Variablen (uninitialisiert) =\u003e Wert 0 ACHTUNG: Bereich (un-) initialisierter Daten nicht in Abbildung dargestellt!\nSegmente des virtuellen Speichers: Stack Stackframe je Funktionsaufruf: Lokale Variablen (\"automatische\" Variablen) Argumente und Return-Werte Dynamisch wachsend und schrumpfend Automatische Pflege Nach Funktionsrückkehr wird der Stackpointer (\"Top of Stack\") weiter gesetzt Dadurch \"Bereinigung\": Speicher der lokalen Variablen wird freigegeben Segmente des virtuellen Speichers: Data (Heap) Bereich für dynamischen Speicher (Allokation während der Laufzeit) Dynamisch wachsend und schrumpfend Zugriff und Verwaltung aus laufendem Programm =\u003e Pointer malloc()/calloc()/free() (C) new/delete (C++) typischerweise Pointer Befehlszyklus (von-Neumann-Architektur) op = read_next_op(pc) decode(op) pc += 1 args = nil if operands_needed(op): args = read_operands(pc) pc += 1 execute(op, args)Typischerweise hat man neben dem Stack und dem Heap noch diverse Register auf dem Prozessor, die man wie (schnelle Hardware-) Variablen nutzen kann. Es gibt normalerweise einige spezielle Register:\nProgram Counter (PC): Zeigt auf die Stelle im Textsegment, die gerade ausgeführt wird Stack Pointer (SP): Zeigt auf den nächsten freien Stackeintrag Frame Pointer (FP): Zeigt auf die Rücksprungadresse auf dem Stack Akkumulator: Speichern von Rechenergebnissen Der Prozessor holt sich die Maschinenbefehle, auf die der PC aktuell zeigt und dekodiert sie, d.h. holt sich die Operanden, und führt die Anweisung aus. Danach wird der PC entsprechend erhöht und der Fetch-Decode-Execute-Zyklus startet erneut. (OK, diese Darstellung ist stark vereinfacht und lässt beispielsweise Pipelining außen vor ...)\nAnmerkung: In der obigen Skizze wird der PC für jede Instruktion oder jeden Operanden um 1 erhöht. Dies soll andeuten, dass man die nächste Instruktion lesen möchte. In der Realität muss hier auf die nächste relevante Speicheradresse gezeigt werden, d.h. das Inkrement muss die Breite eines Op-Codes etc. berücksichtigen.\nEin Sprung bzw. Funktionsaufruf kann erreicht werden, in dem der PC auf die Startadresse der Funktion gesetzt wird.\nJe nach Architektur sind die Register, Adressen und Instruktionen 4 Bytes (32 Bit) oder 8 Bytes (64 Bit) \"breit\".\nAufgaben bei der Erzeugung von Maschinen-Code Relativ ähnlich wie bei der Erzeugung von Bytecode, nur muss diesmal die Zielhardware (Register, Maschinenbefehle, ...) beachtet werden:\nÜbersetzen des Zwischencodes in Maschinenbefehle für die jeweilige Zielhardware\nSammeln von Konstanten und Literalen am Ende vom Text-Segment\nAuflösen von Adressen:\nSprünge: relativ (um wie viele Bytes soll gesprungen werden) oder absolut (Adresse, zu der gesprungen werden soll) Strukturen (Arrays, Structs)haben ein Speicherlayout: Zugriff auf Elemente/Felder über Adresse (muss berechnet werden) Zugriffe auf Konstanten oder Literalen: Muss ersetzt werden durch Zugriff auf Text-Segment Zuordnung der Variablen und Daten zu Registern oder Adressen\nAufruf von Funktionen: Anlegen der Stack-Frames (auch Activation Record genannt)\nAufbau des Binärformats und Linking auf der Zielmaschine (auch Betriebssystem) beachten\nÜbersetzen von Zwischencode in Maschinencode Für diese Aufgabe muss man den genauen Befehlssatz für den Zielprozessor kennen. Im einfachsten Fall kann man jede Zeile im Zwischencode mit Hilfe von Tabellen und Pattern Matching direkt in den passenden Maschinencode übertragen. Beispiel vgl. Tabelle 7.1 in [Mogensen2017, S.162].\nJe nach Architektur sind die Register, Adressen und Instruktionen 4 Bytes (32 Bit) oder 8 Bytes (64 Bit) \"breit\".\nDa in einer Instruktion wie ldr r0, x die Adresse von x mit codiert werden muss, hat man hier nur einen eingeschränkten Wertebereich. Üblicherweise ist dies relativ zum PC zu betrachten, d.h. beispielsweise ldr r0, #4[pc] (4 Byte plus PC). Dadurch kann man mit PC-relativer Adressierung dennoch größere Adressbereiche erreichen. Alternativ muss man mit indirekter Adressierung arbeiten und im Textsegment die Adresse der Variablen im Datensegment ablegen: ldr r0, ax, wobei ax eine mit PC-relativer Adressierung erreichbare Adresse im Textsegment ist, wo die Adresse der Variablen x im Datensegment hinterlegt ist. Anschließend kann man dann x laden: ldr ro, [ro].\nÄhnliches gilt für Konstanten: Wenn diese direkt geladen werden sollen, steht quasi nur der \"Rest\" der Bytes vom Opcode zur Verfügung. Deshalb sammelt man die Konstanten am Ende vom Text-Segment und ruft sie von dort ab.\nBeispiel\nIn dem kurzen IR-Snippet soll zum Label \"L\" verweigt werden, wenn ein Wert x kleiner ein anderer Wert v ist.\nL: ... ... if x \u003c v goto L Bei der Erzeugung des Maschinencodes wird das Label \"L\" an einer konkreten Adresse im Text-Segment angesiedelt, beispielsweise bei 1000. Für die If-Abfrage müssen zunächst die beiden Werte in Register geladen werden und die Register subtrahiert werden. Anschließend kann man in dem angenommenen Op-Code bltz (\"branch if less than zero\") zur Adresse 1000 springen, wenn der Wert in Register R0 kleiner als Null ist. Anderenfalls würde einfach hinter der Adresse 1108 weiter gemacht.\n1000: ... ;; L ... 1080: ldr r0, x ;; R0 = x 1088: ldr r1, v ;; R1 = v 1096: sub r0, r0, r1 ;; R0 = R0-R1 1108: bltz r0, 1000 ;; if R0\u003c0 jump to 1000 (L) Anmerkungen: In der Skizze hier stehen x und v für die Adressen, wo die Werte von x und v gespeichert werden. D.h. im Maschinencode steht nicht x, sondern die Adresse von x.\nBeachten Sie auch die unterschiedlichen Adressen: Im Beispiel wurde für ldr r0 ein 4 Byte großer Op-Code angenommen plus 4 Byte für die Adresse x. Für das sub wurden dagegen 3x 4 Byte gebraucht (Op-Code, R0, R1).\nAufruf von Funktionen Es soll Maschinencode für den folgenden Funktionsaufruf erzeugt werden:\nx = f(p1, ..., pn) Dies könnte sich ungefähr in folgenden (fiktiven) Assembler-Code übersetzen lassen:\nFP = SP ;; Framepointer auf aktuellen Stackpointer setzen Stack[SP] = R ;; Rücksprungadresse auf Stack Stack[SP-4] = p1 ;; Parameter p1 auf Stack ... Stack[SP-4*n] = pn ;; Parameter pn auf Stack SP = SP - 4*(n+1) ;; Stackpointer auf nächste freie Stelle Goto f ;; Setze den PC auf die Adresse von f im Textsegment R: x = Stack[SP+4] ;; Hole Rückgabewert SP = SP + 4 ;; Stackpointer auf nächste freie Stelle Funktionsaufruf Ein Funktionsaufruf entspricht einem Sprung an die Stelle im Textsegment, wo der Funktionscode abgelegt ist. Dies erreicht man, in dem man diese Adresse in den PC schreibt. Bei einem return muss man wieder zum ursprünglichen Programmcode zurückspringen, weshalb man diese Adresse auf dem Stack hinterlegt.\nParameter Zusätzlich müssen Parameter für die Funktion auf dem Stack abgelegt werden, damit die Funktion auf diese zugreifen kann. Im Funktionscode greift man dann statt auf die Variablen auf die konkreten Adressen im Stack-Frame zu. Dazu verwendet man den Framepointer bzw. FP, der auf die Adresse des ersten Parameters, d.h. auf die Adresse hinter der Rücksprungadresse, zeigt. Die Parameter sind dann je Funktion über konstante Offsets relativ zum FP erreichbar. Ähnlich wie die Rücksprungadresse muss der FP des aufrufenden Kontexts im Stack-Frame der aufgerufenen Funktion gesichert werden (in der Skizze nicht dargestellt) und am Ende des Aufrufs wieder hergestellt werden.\nLokale Variablen Lokale Variablen einer Funktion werden ebenfalls auf dem Stack angelegt, falls nicht genügend Register zur Verfügung stehen, und relativ zum FP adressiert. Die Größe des dafür verwendeten Speichers wird oft als Framesize oder Rahmengröße bezeichnet. Am Ende eines Funktionsaufrufs wird dieser Speicher freigegeben indem der Stackpointer bzw. SP auf den FP zurückgesetzt wird.\nDie Adressierung von Parametern und Variablen kann auch relativ zum SP erfolgen, so dass kein FP benötigt wird. Der dazu erzeugte Maschinencode kann aber deutlich komplexer sein, da Stack und SP auch für arithmetische Berechnungen verwendet werden und der SP somit für die Dauer eines Funktionsaufrufs nicht zwingend konstant ist.\nRückgabewerte Falls eine Funktion Rückgabewerte hat, werden diese ebenfalls auf dem Stack abgelegt (Überschreiben der ursprünglichen Parameter).\nZusammengefasst gibt es für jeden Funktionsaufruf die in der obigen Skizze dargestellte Struktur (\"Stack Frame\" oder auch \"Activation Record\" genannt):\nFunktionsparameter (falls vorhanden) Rücksprungadresse (d.h. aktueller PC) Lokale Variablen der Funktion (falls vorhanden) Sichern von lokalen Variablen der \"alten\" Funktion Vor Funktionsaufrufen müssen aktuell verwendete Register (lokale Variablen) gesichert werden Register werden in den Speicher (Stack) ausgelagert Sicherung kann durch aufrufende Funktion (Caller-Saves) oder aufgerufene Funktion (Callee-Saves) erfolgen Caller-Safes: nur \"lebende\" Register müssen gesichert werden Callee-Safes: nur tatsächlich verwendete Register müssen gesichert werden Nachteile: unnötiges Sichern von Registern bei beiden Varianten möglich In der Praxis daher meist gemischter Ansatz aus Caller-Saves und Callee-Saves Registern Funktionsaufruf: Prolog f: ... ;; Label f: hier startet die Funktion p1 = Stack[SP+4*n] ;; Zugriff auf p1 (per SP) p1 = Stack[FP-4] ;; Zugriff auf p1 (per FP) ... ;; hier Funktionskram Die Parameter einer Funktion werden vom aufrufenden Kontext auf dem Stack abgelegt. Nach dem Sprung in die Funktion kann über den Stack-Pointer oder den Frame-Pointer darauf zugegriffen werden. Alternativ könnte man die Parameter auch in vorhandene Register laden und entsprechend den SP hochzählen.\nFunktionsaufruf: Epilog SP = FP - 4 ;; Position über Rücksprungadresse auf Stack FP = Stack[FP] ;; Sichere Rücksprungadresse (R) Stack[SP+4] = Ergebnis ;; Ergebnis auf Stack (statt Rücksprung-Adresse) Goto FP ;; Setze den PC auf die Rücksprung-Adresse Beim Rücksprung aus einer Funktion wird der Rückgabewert an die Stelle der Rücksprungadresse geschrieben und der restliche Stack freigegeben.\nDer Rückgabewert wird dann vom Aufrufer an der Stelle Stack[SP+4] abgerufen und freigegeben (siehe Beispiel oben).\nAnmerkung: Das Handling des FP ist im obigen Beispiel nicht konsistent bzw. vollständig. Nach dem Rücksprung in den Aufrufer muss der FP auf die Speicheradresse im Stack zeigen, wo die Rücksprungadresse dessen Aufrufers steht ...\nWrap-Up Skizze zur Erzeugung von Assembler-Code\nRelativ ähnlich wie die Erzeugung von Bytecode Beachtung der Eigenschaften der Zielhardware (Register, Maschinenbefehle, ...) Übersetzen des Zwischencodes in Maschinenbefehle Sammeln von Konstanten und Literalen am Ende vom Text-Segment Auflösen von Adressen Zuordnung der Variablen und Daten zu Registern oder Adressen Aufruf von Funktionen: Op-Codes zum Anlegen der Stack-Frames ",
    "description": "",
    "tags": null,
    "title": "Generierung von Maschinencode (Skizze)",
    "uri": "/backend/machinecode.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24) \u003e Frontend \u003e Parser",
    "content": "Fehler beim Parsen Quelle: Vorlesung \"Einführung in die Programmierung mit Skriptsprachen\" by BC George (CC BY-SA 4.0)\nCompiler ist ein schnelles Mittel zum Finden von (syntaktischen) Fehlern Wichtige Eigenschaften: Reproduzierbare Ergebnisse Aussagekräftige Fehlermeldungen Nach Erkennen eines Fehlers: (vorläufige) Korrektur und Parsen des restlichen Codes =\u003e weitere Fehler anzeigen. Problem: Bis wohin \"gobbeln\", d.h. was als Synchronisationspunkt nehmen? Semikolon? Syntaktisch fehlerhafte Programme dürfen nicht in die Zielsprache übersetzt werden! Typische Fehler beim Parsing grammar VarDef; alt : stmt | stmt2 ; stmt : 'int' ID ';' ; stmt2 : 'int' ID '=' ID ';' ;Anmerkung: Die nachfolgenden Fehler werden am Beispiel der Grammatik VarDef.g4 und ANTLR demonstriert.\nLexikalische Fehler Eingabe: int x1; (Startregel stmt)\nFehlermeldung: token recognition error at: '1'\nDie ist ein Fehler aus dem Lexer, wenn beim Erkennen eines Tokens ein komplett unbekanntes Zeichen auftritt.\nEin extra Token Eingabe: int x y; (Startregel stmt)\nFehlermeldung: extraneous input 'y' expecting ';'\nWenn nur ein Token zu viel ist, dann kann der von ANTLR generierte Parser eine passende Fehlermeldung ausgeben.\nMehrere extra Token Eingabe: int x y z; (Startregel stmt)\nFehlermeldung: mismatched input 'y' expecting ';'\nWenn dagegen mehr als ein Token zu viel ist, dann gibt der von ANTLR generierte Parser eine generische Fehlermeldung aus.\nFehlendes Token Eingabe: int ; (Startregel stmt)\nFehlermeldung: missing ID at ';'\nEin anderer typischer Fehler sind fehlende Token, die kann der Parser analog zu überzähligen Token erkennen und ausgeben.\nFehlendes Token am Entscheidungspunkt Eingabe: int ; (Startregel alt)\nFehlermeldung: no viable alternative at input 'int;'\nHier fehlt ein Token, aber an einer Stelle, wo sich der Parser zwischen zwei Alternativen (Sub-Regeln) entscheiden muss.\nÜberblick Recovery bei Parser-Fehlern Fehler im Lexer (hier nicht weiter betrachtet): Aktuelles Zeichen passt zu keinem Token: Entfernen oder Hinzufügen von Zeichen (plus Rückmeldung an den Parser) Spezielle Token, die typische fehlerhafte Zeichenketten als Token erkennen (mit Weiterverarbeitung im Parser) Fehler im Parser: Token passt nicht: Token entfernen oder ein Dummy-Token erzeugen Panic-Mode: Entferne Token bis zu einem Synchronisationspunkt. Problem: Dabei nicht zu weit zu springen! Spezielle Fehlerproduktionen: Spezielle Regeln in der Grammatik, die typische Fehler matchen. Anmerkung LR-Parser: Ein Syntaxfehler wird entdeckt, wenn die Action-Tabelle für Top-of-Stack und akt. Token leer ist =\u003e Stack und/oder Token modifizieren, aber deutlich schwieriger als bei LL ...\nSkizze: Generierte Parser-Regeln (ANTLR) stmt : 'int' ID ';' ;def stmt(): try: match(\"int\"); match(ID); match(\";\") catch (RecognitionException re): _errHandler.reportError(self) # let's report it _errHandler.recover(self) # Panic-Mode def match(x): if lookahead == x: consume() else: _errHandler.recoverInline(self) # Inline-Mode }Der im Parser registrierte ErrorHandler erzeugt in der Methode reportError() eine geeignete Meldung und gibt sie an den Parser über dessen Methode notifyErrorListeners() weiter.\nDie eigentliche Fehlerbehandlung findet in der Methode recover() bzw. recoverInline() des ErrorHandlers statt.\nInline-Recovery bei Token-Mismatch (Skizze) def recoverInline(parser): # SINGLE TOKEN DELETION if singleTokenDeletion(parser): return getMatchedSymbol(parser) # SINGLE TOKEN INSERTION if singleTokenInsertion(parser): return getMissingSymbol(parser) # that didn't work, throw a new exception throw new InputMismatchException(parser) }Die Klasse InputMismatchException drückt aus, dass das aktuelle Token nicht zur Erwartung des Parsers passt. Deshalb wird diese Exception am Ende von recoverInline() geworfen. Die Klasse RecognitionException, die in den Parserregeln wie stmt gefangen wird, ist die gemeinsame Oberklasse aller Parser-Exceptions.\nListe der wichtigsten Exceptions (nach github.com/antlr/antlr4/blob/master/doc/parser-rules.md):\nException Beschreibung RecognitionException Basisklasse für alle Parser-Exceptions NoViableAltException Parser konnte sich nicht für (mind.) einen Pfad entscheiden angesichts des Tokenstroms LexerNoViableAltException Lexer-Pendant zu NoViableAltException InputMismatchException Das aktuelle Token ist nicht das, was der Parser erwartet Panic Mode: Sync-and-Return (Skizze) def rule(): try: ... rule-body ... catch (RecognitionException re): _errHandler.reportError(self) # let's report it _errHandler.recover(self) # Panic-Mode }=\u003e Entferne solange Token, bis aktuelles Token im \"Resynchronization Set\"\nANTLR: Einsatz des \"Resynchronization Set\" Following Set: Menge der Token, die direkt auf eine Regel-Referenz folgen, ohne dass die aktuelle Regel/Alternative verlassen wird Resynchronization Set: Vereinigung der Following Sets für alle Regeln im aktuellen Aufruf-Stack Quelle: nach [Parr2014, pp. 161-163]\nstmt : 'if' expr ':' stmt // Following Set für \"expr\": {':'} | 'while' '(' expr ')' stmt ; // Following Set für \"expr\": {')'} expr : term '+' INT ; // Following Set für \"term\": {'+'} Eingabe: if : Aufruf-Stack nach Bearbeitung von if: [stmt, expr, term] Resynchronization Set: {'+', ':'} Hinweis: FOLLOW $\\ne$ Following FOLLOW ist die Menge aller Token, die auf eine Regel folgen können\nFOLLOW(term) = {'+'} FOLLOW(expr) = {':', ')'} Following ist dagegen abhängig vom aktuellen Kontext!\nStack: [stmt, expr, term] =\u003e Resynchronization Set: {'+', ':'} Beispiele Resynchronisation im Panic Mode (ANTLR) Hinweis: Die Regel term ist in obigem Beispiel nicht weiter detailliert. Hier wird angenommen, dass das aktuelle Token ':' nicht passt.\nEingabe: if : In Regel term: Token ':' passt nicht consume(), bis aktuelles Token in Resynchronization Set: {'+', ':'} (d.h. hier bleibt ':' das aktuelle Token) Rückkehr zu Regel expr In Regel expr: Token ':' passt nicht consume(), bis aktuelles Token in Resynchronization Set: {':'} (d.h. hier bleibt ':' das aktuelle Token) Rückkehr zu Regel stmt In Regel stmt: Token ':' passt jetzt Abschluss des Parsing (mit Fehlermeldung) Eingabe: if x + 42 ))): In Regel stmt: Token ')' passt nicht consume(), bis aktuelles Token in Resynchronization Set: {':'} (d.h. hier werden alle ')' entfernt) In Regel stmt: Token ':' passt jetzt Abschluss des Parsing (mit Fehlermeldung) ANTLR: Anmerkungen Fehlerbehandlung in Sub-Regeln Bei Sub-Regeln (d.h. eine Regel enthält Alternativen) oder Schleifenkonstrukten (d.h. eine Regel enthält (...)* oder (...)+) geht ANTLR etwas anders vor.\nStart einer Sub-Regel/Alternative: Versuch einer single token deletion\n# am Anfang einer Alternative oder Schleife _errHandler.sync(self) Schleifenkonstrukte: (...)* oder (...)+\nVersuche, in der Schleife zu bleiben! Im Fehlerfall consume() bis\nWeitere Iteration der Schleife erkannt Token, welches der Schleife folgt, erkannt Token im Resynchronization Set des aktuellen Aufruf-Stacks Anmerkung: Im Prinzip entspricht dies dem Panic Mode, der Unterschied liegt darin, bis wohin der Parser nach der Recovery in einer Funktion/Methode (Regel) zurückspringt. D.h. wenn es verschiedene Möglichkeiten gibt, haben diese die obige Priorisierung.\nFail-Save\nUm Endlos-Schleifen durch die Schritte (1) bzw. (2) zu vermeiden, löst der Parser beim zweiten Versuch, die selbe Parser-Stelle und Input-Position zu bearbeiten (also bei bereits aktivem Fehler), einen \"Fail-Safe\" aus. Der Parser konsumiert dann ein Token und fährt dann mit der Recovery fort.\nZu Details zur Fehlerbehandlung durch ANTLR vergleiche [Parr2014, S. 170 ff.].\nANTLR: Ändern der Fehlerbehandlungs-Strategie Ändern der Fehlerbehandlungs-Strategie (global) Sie überschreiben die Klasse DefaultErrorStrategy und müssen die oben gezeigten Methoden recover() und recoverInline()aufrufen. Die eigene Fehlerbehandlung setzen Sie über die Methode setErrorHandler des Parsers.\nÄndern der Fehlerbehandlungs-Strategie (lokal) r : ... ; catch[RecognitionException e] { throw e; }Es lassen sich auch andere bzw. mehrere Exceptions fangen. Der catch-Block ersetzt den Default-catch-Block der generierten Methode. Das bedeutet, dass sich der geänderte Modus nur für die eine Regel auswirkt.\nÄndern der Fehler-Meldungen Für einen eigenen Listener leitet man sinnvollerweise von BaseErrorListener ab und überschreibt die leere Implementierung von syntaxError().\nDamit die Fehlermeldungen nicht mehrfach ausgegeben werden, entfernt man zunächst alle Listener und fügt dann den eigenen hinzu, bevor man den Parser startet.\nPanic Mode in Bison (Error Recovery) stmt : 'int' ID ';' { printf(\"%s\\n\", $2); } | error '\\n' { yyerror(); yyerrok; } ;Bison kennt ein spezielles Fehler-Token error. Dieses Token wird genutzt, um einen Synchronisationspunkt in der Grammatik zu definieren, von dem aus man höchstwahrscheinlich weiter parsen kann.\nParsen mit error-Token Der Parser wird mit diesen Produktionen generiert wie mit normalen Token auch. Im Fehlerfall werden so lange Symbole vom Stack entfernt, bis eine Regel der Form $A \\to \\operatorname{error} \\alpha$ anwendbar ist. Dann wird das Token error auf den Stack geschoben und so lange Eingabe-Token gelesen und verworfen, bis eines gefunden wird, welches auf das error-Token folgen kann. Dies nennt Bison \"Resynchronisation\". Anschließend wird im Recovery-Modus normal fortgefahren, bis drei weitere Token auf den Stack geschoben wurden und damit der Recovery-Modus verlassen wird. Falls bereits vorher weitere Fehler auftreten, werden diese nicht separat gemeldet.\nAnwendung im obigen Beispiel Im obigen Beispiel ist die Regel stmt : error '\\n' enthalten. Im Fehlerfall werden die Symbole vom Stack entfernt, bis ein Zustand erreicht ist, der eine Shift-Aktion auf das Token error hat. Das Error-Token wird auf den Stack geschoben und alle Eingabetoken bis zum nächsten '\\n' gelesen und direkt entfernt. Mit dem Erreichen des Zeilenumbruchs wird die zugeordnete Aktion ausgeführt. Diese gibt den Fehler auf der Konsole aus und führt mit dem Makro yyerrok einen Reset des Parsers aus (d.h. er verlässt den Recovery-Modus vor dem Shiften der per Default drei gültigen Token). Anschließend ist der Bison-Parser wieder im normalen Modus. Die fehlerhaften Symbole/Token wurden aus dem Eingabestrom entfernt.\nWo kommen die error-Token am besten hin? Die \"schwarze Kunst\" ist, die Error-Token an geeigneten Stellen unterzubringen, d.h. vorherzusehen, wo der Parser am sinnvollsten wieder aufsetzen kann. Häufig sind dies beispielsweise das ein Statement beendende Semikolon oder die einen Block beendende schließende geschweifte Klammer. Beispielsweise könnte man für die Sprache C bei der Definition von Statements mehrere Synchronisationspunkte einbauen:\nstmt : ... | error ';' /* Synchronisation für 'return' */ | error '}' /* Synchronisation nach Block */ | error '\\n' /* Synchronisation nach Zeilenumbruch */ ;Bison und C und Speichermanagement im Fehlerfall Wenn Bison im Recovery-Modus ist, werden Symbole und ihre Werte vom Stack entfernt. Falls diese Werte (vgl. %union) Pointer mit dynamisch alloziertem Speicher sind, muss Bison diesen Speicher freigeben.\nDazu kann man sich über die Direktive %destructor { code } symbols oder %destructor { code } \u003ctypes\u003e Code definieren, der dann für die jeweiligen Symbole oder Typen ausgeführt wird.\nDie Typangabe \u003c*\u003e dient dabei als Catch-All für Symbole, für die ein Typ definiert wurde, aber kein Destruktor.\nBeispiel:\n%union { char* str; } %token \u003cstr\u003e ID %destructor { free($$); } \u003cstr\u003eFür weitere Details vergleiche [Levine2009, Kap. 8].\nFehlerproduktionen Häufig vorkommende Fehler kann man bereits in der Grammatik berücksichtigen. Dadurch kommt es nicht zu einem Parser-Error mit Recovery-Mechanismus, sondern der Fehler wird über eine entsprechende Alternative in der Grammatik korrigiert.\nEs bietet sich an, in diesem Fall eine entsprechende Ausgabe zu tätigen. Dies wird in der folgenden Grammatik über eingebettete Aktionen erledigt.\nANTLR stmt : 'int' ID ';' : 'int' ID {notifyErrorListeners(\"Missing ';'\");} : 'int' ID ';' ';' {notifyErrorListeners(\"Too many ';'\");} ;Der aus der Grammatik generierte Parser leitet von der Basisklasse Parser ab. Dort wird eine Methode notifyErrorListeners() implementiert, die man mit Hilfe von in die Grammatik eingebetteten Aktionen aufrufen kann (Vorgriff auf Syntaxgesteuerte Interpreter). Letztlich steht im generierten Parser in der generierten Methode stmt() an der passenden Stelle ein Aufruf notifyErrorListeners(Too many ';'\"); ...\nFlex und Bison stmt : 'int' ID ';' { $$ = $2; } : 'int' ID { yyerror(\"unterminated id\"); $$ = $2; } ; %% void yyerror(char *s, ...) { va_list ap; va_start(ap, s); fprintf(stderr, \"%d: error: \", yylineno); vfprintf(stderr, s, ap); fprintf(stderr, \"\\n\"); }Analog zu ANTLR ist es auch in Flex/Bison üblich, für typische Szenarien \"nicht ganz korrekte\" Eingaben zu akzeptieren. Dazu definiert man zusätzliche Lexer- oder Parser-Regeln, die diese Eingaben als das, was gemeint war akzeptieren und eine zusätzliche Warnung ausgeben.\nDabei definiert man sich typischerweise die Funktion yyerror(). Über yytext hat man Zugriff auf den Eingabetext des aktuellen Tokens, und mit yylineno hat man Zugriff auf die aktuelle Eingabezeile (yylineno wird automatisch bei jedem \\n inkrementiert). Wenn man weitere Informationen benötigt, muss man mit dem Bison-Feature \"Locations\" arbeiten. Dies ist ein spezieller Datentyp YYLTYPE.\nFür weitere Details vergleiche [Levine2009, Kap. 8].\nAnmerkung: Nicht eindeutige Grammatiken stat: expr ';' | ID '+' ID ';' ; expr: ID '+' ID | INT ;=\u003e Was passiert bei der Eingabe: a+b ??! Welche Regel/Alternative soll jetzt matchen, d.h. welcher AST soll am Ende erzeugt werden?!\nANTLR Nicht eindeutige Grammatiken führen nicht zu einer Fehlermeldung, da nicht der Nutzer mit seiner Eingabe Schuld ist, sondern das Problem in der Grammatik selbst steckt.\nWährend des Debuggings von Grammatiken lohnt es sich aber, diese Warnungen zu aktivieren. Dies kann entweder mit der Option \"-diagnostics\" beim Aufruf des grun-Tools geschehen oder über das Setzen des DiagnosticErrorListener aus der ANTLR-Runtime als ErrorListener.\nBison Bison meldet nicht eindeutige Grammatiken beim Erzeugen des Parsers (vgl. Shift/Reduce- und Reduce/Reduce-Konflikte) und entscheidet sich jeweils für eine Operation (wobei Shift bevorzugt wird). Dies kann man im über die Option -v erzeugten \u003cname\u003e.output-File überprüfen.\nWrap-Up Fehler bei match(): single token deletion oder single token insertion\nPanic Mode: sync-and-return bis Token in Resynchronization Set (ANTLR) oder error-Token shiftbar (Bison)\nANTLR: Sonderbehandlung bei Start von Sub-Regeln und in Schleifen ANTLR: Fail-Save zur Vermeidung von Endlosschleifen Fehler-Alternativen in Grammatik einbauen\n",
    "description": "",
    "tags": null,
    "title": "Error-Recovery",
    "uri": "/frontend/parsing/recovery.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24)",
    "content": "",
    "description": "",
    "tags": null,
    "title": "Categories",
    "uri": "/categories.html"
  },
  {
    "breadcrumb": "MIF 1.10: Compilerbau / MIF 1.5: Concepts of Programming Languages (Winter 2023/24)",
    "content": "",
    "description": "",
    "tags": null,
    "title": "Tags",
    "uri": "/tags.html"
  }
]
